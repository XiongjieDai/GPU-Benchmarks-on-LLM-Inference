{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742b7abf-cd44-4f5e-a91a-34cb975c06c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============GPU================\n",
      "Mon Jul 17 04:26:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:82:00.0 Off |                  Off |\n",
      "|  0%   30C    P8    19W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "============CUDA version================\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "============CPU================\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "model name\t: AMD EPYC 7352 24-Core Processor\n",
      "============Memory================\n",
      "MemTotal:       263848528 kB\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"============GPU================\")\n",
    "!nvidia-smi\n",
    "\n",
    "# CUDA version\n",
    "print(\"============CUDA version================\")\n",
    "!nvcc --version\n",
    "\n",
    "# CPU\n",
    "print(\"============CPU================\")\n",
    "!cat /proc/cpuinfo | grep model\\ name\n",
    "\n",
    "# Memory\n",
    "print(\"============Memory================\")\n",
    "!cat /proc/meminfo | grep MemTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afca2f3f-6215-4d45-b2bf-46a10db08485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca95f66-e949-4596-a441-59d3e4d24009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13B  30B  65B  7B  ggml-vocab.bin  tokenizer.model  tokenizer_checklist.chk\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3208b531-248f-4942-839d-75498710f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
      "I LDFLAGS:  \n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "rm -vf *.o *.so main quantize quantize-stats perplexity embedding benchmark-matmult save-load-state server simple vdot train-text-from-scratch embd-input-test build-info.h\n",
      "removed 'common.o'\n",
      "removed 'ggml-cuda.o'\n",
      "removed 'ggml.o'\n",
      "removed 'k_quants.o'\n",
      "removed 'llama.o'\n",
      "removed 'libembdinput.so'\n",
      "removed 'main'\n",
      "removed 'quantize'\n",
      "removed 'quantize-stats'\n",
      "removed 'perplexity'\n",
      "removed 'embedding'\n",
      "removed 'server'\n",
      "removed 'simple'\n",
      "removed 'vdot'\n",
      "removed 'train-text-from-scratch'\n",
      "removed 'embd-input-test'\n",
      "removed 'build-info.h'\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:  Linux\n",
      "I UNAME_P:  x86_64\n",
      "I UNAME_M:  x86_64\n",
      "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n",
      "I LDFLAGS:  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "I CC:       cc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "I CXX:      g++ (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0\n",
      "\n",
      "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c llama.cpp -o llama.o\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/common.cpp -o common.o\n",
      "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n",
      "nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n",
      "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‚Äò\u001b[01m\u001b[Kvoid llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float, float)\u001b[m\u001b[K‚Äô:\n",
      "\u001b[01m\u001b[Kllama.cpp:2208:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koperation on ‚Äò\u001b[01m\u001b[Kt_start_sample_us\u001b[m\u001b[K‚Äô may be undefined [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsequence-point\u0007-Wsequence-point\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2208 |     int64_t t_start_sample_us = \u001b[01;35m\u001b[Kt_start_sample_us = ggml_time_us()\u001b[m\u001b[K;\n",
      "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‚Äò\u001b[01m\u001b[Kvoid ggml_cuda_op_mul(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K‚Äô:\n",
      "\u001b[01m\u001b[Kggml-cuda.cu:2637:102:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‚Äò\u001b[01m\u001b[Ki1\u001b[m\u001b[K‚Äô [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
      " 2637 |     float * src0_ddf_i, float * src1_ddf_i, float * dst_ddf_i, int64_t i02, int64_t i01_low, int6\u001b[01;35m\u001b[K4_t i0\u001b[m\u001b[K1_high, int i1,\n",
      "      |                                                                                                  \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/main/main.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o main -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o quantize-stats -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o perplexity -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embedding -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o -o vdot -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o ggml-cuda.o -o train-text-from-scratch -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o simple -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o server -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o libembdinput.so -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n",
      "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o -o embd-input-test -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L. -lembdinput\n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!make clean && LLAMA_CUBLAS=1 make -j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2197dafa-6825-4b02-a5a6-f2361227bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568042\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to live it. As a young man, I dreamed of being an engineer and designing bridges that would stand for hundreds of years. As I grew older, I realized there were more important things in life than engineering bridges.\n",
      "I‚Äôve worked hard to find my place in this world and have been rewarded with a rich and fulfilling life. I was born near the southern tip of India; when I reached age 12, my parents moved us to America so that I could get an education. We settled in Florida where I went to high school, college, and medical school. My wife and I had two children‚Äîa son and a daughter‚Äîand we‚Äôve been very happily married for more than 40 years.\n",
      "I always wanted to be a doctor, but when it came time to make the decision about what kind of doctor to become, my father suggested that family practice might offer the most rewarding career. That was certainly right on the money!\n",
      "I started my own family practice in the small community of Gainesville, Florida. I really enjoyed practicing medicine and helping people get back on their feet again after getting sick or injured. At the time, though, I felt that I could do more for people if I knew their complete medical history‚Äîwhich is why I decided to go into internal medicine.\n",
      "I‚Äôve been happily practicing in Jacksonville since 1986. One of my favorite things about internal medicine is that it allows me to help a lot of patients at one time, rather than just focusing on one individual patient.\n",
      "A very important part of helping people is educating them and working with them to prevent illness or injury from occurring in the first place. That‚Äôs why I became certified in hypertension management and now provide my patients with personalized education about how they can keep their blood pressure under control.\n",
      "As a teacher, I enjoy lecturing to medical students at Mayo Clinic and other institutions of higher learning. I also feel privileged to be able to give back by serving as the director of internal medicine for the University of Florida College of Medicine Jacksonville.\n",
      "I‚Äôm passionate about my family; we have a very close-knit group. We are all healthy, so we consider ourselves to be very lucky! I also have a great love for golf and other sports like tennis. My wife is a retired nurse who enjoys sewing and gardening.\n",
      "llama_print_timings:        load time =  5727.05 ms\n",
      "llama_print_timings:      sample time =   410.36 ms /   512 runs   (    0.80 ms per token,  1247.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   243.56 ms /   265 tokens (    0.92 ms per token,  1088.03 tokens per second)\n",
      "llama_print_timings:        eval time =  3685.45 ms /   510 runs   (    7.23 ms per token,   138.38 tokens per second)\n",
      "llama_print_timings:       total time =  4467.09 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84a1f92-dc4e-488f-ba76-c1bef4f63350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568054\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to love it, and to experience as much as you can while you're alive. When I was 17 years old I got my first job in an office environment (I worked for a real estate agent) and I loved the fact that I had a routine and could go out into the world for work!\n",
      "In 2015, I had two major life changes: I got married to a wonderful man who is now my husband, and we moved to Brisbane. Since then I have been working as a real estate agent in both Queensland and New South Wales.\n",
      "I am passionate about the Real Estate Industry because of how it can change your life! I love seeing my clients become proud owners and living their dreams. This is why I strive to always provide excellent service, which has helped me achieve many awards including being a recipient of the REAA‚Äôs 2015 Rising Star Award.\n",
      "I also love the sense of community that can be found in Real Estate agencies. They are very close-knit and supportive environments where you feel like you belong, which is why I have made some life-long friends with other agents in my office.\n",
      "If you want a real estate agent who will listen to your needs, guide you through each step of the buying or selling process and give you confidence that they are working hard for you - then look no further. Contact me today!\n",
      "\"We couldn't have asked for a better agent than Renee. She was very understanding and accommodating to our requests. She also went above and beyond her duties, making sure everything was ready before we moved in (we were away at the time). We would definitely recommend Renee as an agent!\"\n",
      "\"We could not be happier with the service provided by Renee. From her initial approach, through the negotiation process and right up to settlement day, she has been professional, reliable and a pleasure to deal with. Highly recommended.\"\n",
      "\"We have never had such a positive experience with an agent before and we would highly recommend Renee to anyone looking for a great agent!\"\n",
      "\"Renee was fantastic throughout the whole process of selling our house. Her knowledge regarding property law, sales trends etc. was invaluable and she managed to get us over $100K more than we were expecting! She was also very easy to deal with and never made us feel press\n",
      "llama_print_timings:        load time =  1054.04 ms\n",
      "llama_print_timings:      sample time =   401.04 ms /   512 runs   (    0.78 ms per token,  1276.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   243.41 ms /   265 tokens (    0.92 ms per token,  1088.68 tokens per second)\n",
      "llama_print_timings:        eval time =  3685.23 ms /   510 runs   (    7.23 ms per token,   138.39 tokens per second)\n",
      "llama_print_timings:       total time =  4456.81 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de65425-328d-43f8-a61d-1072375899f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568061\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1640.39 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 4892 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your calling and live it. There‚Äôs a lot more to say on that topic, but for now let me just point you in the right direction:\n",
      "Career Opportunities at Google\n",
      "Ideas Worth Spreading: TED Talks\n",
      "This entry was posted on Sunday, March 7th, 2010 at 16:54 and is filed under Philosophy.\tYou can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site.\n",
      "3 Responses to The Meaning of Life\n",
      "Sunday, March 7th, 2010 at 19:56\n",
      "I‚Äôm in complete agreement with you, though I doubt that many people will be able to ‚Äúlive it‚Äù.\n",
      "Monday, March 8th, 2010 at 13:47\n",
      "That is a very good point, and a big part of the reason why I posted this. There are lots of things which most people would agree with (being honest, etc.) but they still don‚Äôt know what to do about them in their lives.\n",
      "It‚Äôs like when I tell my students that they need to study hard at school. Most will say ‚ÄúYeah, I know‚Äù and then get back on facebook or whatever.\n",
      "Wednesday, March 10th, 2010 at 18:35\n",
      "To be honest, I have always been afraid to find my calling because I thought that if I did, it would define me as a person and make the rest of my life irrelevant.\n",
      "But recently, I‚Äôve discovered this quote by Jim Carrey (of all people): ‚ÄúBecome addicted to continuous learning. Reading is the essential thing‚Äù and since then I have read more than I ever could in my life! This is something that has really changed me and I am excited about it every single day!\n",
      "I think that if we become interested in something, then our whole lives will change ‚Äì for better or worse. And so, I‚Äôm just going to do whatever interests me the most.\n",
      "Thanks for your reply, and thanks again for inspiring me! üôÇ\n",
      "That‚Äôs a good point. It would be very easy to get stuck in a rut if you find something that you like doing but don\n",
      "llama_print_timings:        load time =  1063.90 ms\n",
      "llama_print_timings:      sample time =   400.70 ms /   512 runs   (    0.78 ms per token,  1277.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   245.94 ms /   265 tokens (    0.93 ms per token,  1077.49 tokens per second)\n",
      "llama_print_timings:        eval time =  3700.47 ms /   510 runs   (    7.26 ms per token,   137.82 tokens per second)\n",
      "llama_print_timings:       total time =  4474.52 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f4a68d-5a59-46a0-b09a-e44caeaa237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568068\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be found in living.\n",
      "And for my part, I plan to live mine as fully and joyfully as I can,\n",
      "in order to give the world what I have to give.\n",
      "~ John Lennon (1940‚Äì1980) English musician, singer-songwriter,\n",
      "and peace activist, half of the duo \"Lennon/McCartney\".\n",
      "Quote from: http://en.wikipedia.org/wiki/John_Lennon\n",
      "The following is a partial list of some of the more notable people and organizations who have supported The Peace Alliance over the years in one way or another:\n",
      "Ross Perot (American politician and businessman)\n",
      "Jesse Ventura (American actor, author, wrestler, and former Minnesota governor.)\n",
      "Ed Asner (actor and social activist)\n",
      "The Rev. Jesse Jackson (activist and Baptist minister).\n",
      "Medea Benjamin (activist, co-founder of CODEPINK: Women for Peace)\n",
      "Howard Zinn (historian, author, playwright, social critic, and political scientist.)\n",
      "Michael Franti (musician, poet, human rights activist)\n",
      "The Dalai Lama (spiritual leader of Tibetan Buddhism)\n",
      "Ram Dass (American spiritual teacher and the author of Be Here Now)\n",
      "Deepak Chopra (physician, author, and public speaker)\n",
      "Richard Gere (actor and social activist)\n",
      "David Crosby (musician and songwriter)\n",
      "Bill McKibben (environmentalist, author, educator, and journalist.)\n",
      "Lawrence Lessig (American academic, attorney, and political activist.)\n",
      "Al Sharpton (American civil rights activist, Baptist minister, and radio host).\n",
      "Bernie Sanders (politician, U.S. Senator, and Democratic Party presidential candidate)\n",
      "Jim Hightower (American author, public speaker, radio commentator, and columnist.)\n",
      "Danny Glover (actor and humanitarian)\n",
      "Lisa Simpson (character from The Simpsons television series)\n",
      "The following is a partial list of some of the more notable people and organizations who have supported the National Peace Alliance:\n",
      "Hillary Clinton (politician, First Lady of US, Former U.S. Senator, Secretary of State, Democratic president\n",
      "llama_print_timings:        load time =  9810.92 ms\n",
      "llama_print_timings:      sample time =   389.04 ms /   512 runs   (    0.76 ms per token,  1316.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   435.48 ms /   265 tokens (    1.64 ms per token,   608.53 tokens per second)\n",
      "llama_print_timings:        eval time =  6150.68 ms /   510 runs   (   12.06 ms per token,    82.92 tokens per second)\n",
      "llama_print_timings:       total time =  7101.98 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c793c8ff-4ee2-4b4b-a557-0a68596558c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568088\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy and healthy.\n",
      "I'm not a big fan of reality television, but I do enjoy watching it sometimes with friends.\n",
      "I think my greatest achievement so far is being a high school graduate and being able to go to college in the fall!\n",
      "My most embarrassing moment was when I was around seven years old and I got on stage for my dance recital and forgot all of the moves.\n",
      "One thing I'm good at is remembering people's names.\n",
      "The last book I read was a romance novel, The Marriage Bargain by Jennifer Probst. It was a great book!\n",
      "I love to watch movies with my friends. My favorite movie of all time is Mean Girls.\n",
      "One thing I cannot do without would be my cell phone because that's how I keep in contact with family and friends.\n",
      "My favorite food is Italian, especially chicken parmigiana.\n",
      "If money were no object I would love to travel around the world!\n",
      "I think the most important quality to have in a relationship is trust. You must be able to trust each other completely.\n",
      "What's one thing that surprises people about you? That I am very outgoing and always smiling!\n",
      "I enjoy going to concerts or music festivals, especially since my boyfriend plays guitar.\n",
      "My favorite TV show of all time is Friends because it was so easy to watch and funny.\n",
      "One place I would love to visit someday is Italy because I am of Italian descent and I love their culture!\n",
      "My most embarrassing moment happened when I was on stage for a dance recital and forgot the moves, just like I did in my youth.\n",
      "I think the greatest invention ever made was the iPhone. It makes life so much easier.\n",
      "My favorite movie is Mean Girls because it's such an easy watch, not to mention funny!\n",
      "One thing that I cannot live without is my family. They are always there for me and I love them so much.\n",
      "I think the most important quality in a relationship is trust. If you can't trust your significant other then what's the point?\n",
      "What surprises people about me is how outgoing I am! I feel like some people might think that because I am so small, I'm shy and quiet... but my friends know better.\n",
      "I love to eat Italian food with my family.\n",
      "If I could go\n",
      "llama_print_timings:        load time =  1622.99 ms\n",
      "llama_print_timings:      sample time =   409.12 ms /   512 runs   (    0.80 ms per token,  1251.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   434.94 ms /   265 tokens (    1.64 ms per token,   609.29 tokens per second)\n",
      "llama_print_timings:        eval time =  6151.04 ms /   510 runs   (   12.06 ms per token,    82.91 tokens per second)\n",
      "llama_print_timings:       total time =  7121.96 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72a014f0-852a-4705-b109-3345efebe3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568098\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1923.99 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 8905 MB\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. The purpose of life is to give it away.‚Äù\n",
      "‚Äì Pablo Picasso, 1983\n",
      "The year was 2005 and I had just started my first job as a trainee with an investment bank based in Mumbai. The days were long; the pressure was high, and so was the pay! Every day before leaving home, I made it a point to look into the mirror and give myself a pep talk: ‚ÄúI‚Äôm the best there is. I can do this.‚Äù And every night, my husband, who works in the same field, would share his experiences with me. He said that he had seen some very bright and talented people lose their confidence during training as they were constantly compared to those ahead of them. They were told time and again how far behind they were. Some even went so low mentally that they ended up quitting the job in a few months! So, I was determined not to let that happen to me.\n",
      "The first two days were the worst as I felt like I didn‚Äôt have what it took to excel at this level. But then again, I was told by my senior colleagues, ‚ÄúYou will be fine once you start working on the deals.‚Äù So, I put all my efforts into learning and completing the tasks assigned to me and soon enough, there came a time when I started feeling good about what I had done in a day. And then it happened ‚Äì two weeks after I had joined work; I was given the responsibility of leading a research assignment for our CEO! And that was just the beginning‚Ä¶\n",
      "I never thought I would become so confident that I could even teach someone else how to excel, but my experiences have taught me that it can be done. All you need is the desire and willingness to learn and improve. So here are some of the things I learnt from my journey:\n",
      "1) Don‚Äôt let anyone dictate your dreams or aspirations ‚Äì Ever! You are a unique human being with unique potential. Let no one tell you what is in store for you, because they can never know what‚Äôs inside your heart and soul. And if they try to do that ‚Äì run away from them as fast as you can!\n",
      "2) Believe in yourself ‚Äì No matter how talented or gifted someone else might be; they have no power over your abilities unless you allow it. It is not about whether you are the most\n",
      "llama_print_timings:        load time =  1598.53 ms\n",
      "llama_print_timings:      sample time =   406.96 ms /   512 runs   (    0.79 ms per token,  1258.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   434.97 ms /   265 tokens (    1.64 ms per token,   609.24 tokens per second)\n",
      "llama_print_timings:        eval time =  6154.86 ms /   510 runs   (   12.07 ms per token,    82.86 tokens per second)\n",
      "llama_print_timings:       total time =  7123.96 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728b0adb-547f-466c-9eeb-ce2a48b5cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568109\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to experience it.\n",
      "I believe we are all born with a purpose that we must find for ourselves.\n",
      "I believe the only true failure in life is not living.\n",
      "I believe we are all human and therefore will fail at some point in our lives, but that is okay because we always have a chance to try again.\n",
      "I believe in being yourself. I believe in being real. Because real is always more beautiful than fake.\n",
      "I believe in the importance of self-love.\n",
      "I believe it is never too late for anything.\n",
      "I believe in honesty and integrity.\n",
      "I believe in living life with a sense of adventure.\n",
      "I believe you can get what you want by working hard, not giving up, and being nice to people.\n",
      "I believe our minds are capable of incredible things if we only allow them to be free.\n",
      "I believe that when your soul is happy, your mind will follow.\n",
      "I believe in the importance of finding peace in our lives.\n",
      "I believe that everyone deserves a chance because you never know what they have been through or what they are going through.\n",
      "I believe that the most important thing we can do is to love one another.\n",
      "I believe there are no mistakes, only lessons to be learned.\n",
      "I believe that the more we grow in life, the more we realize how little we know and how much there still is to learn.\n",
      "I believe our bodies are beautiful, and that the most important thing about them is how they feel, not what they look like.\n",
      "I believe in dreams. I believe in setting goals and working hard to achieve them.\n",
      "I believe in karma and paying it forward.\n",
      "I believe that everyone has a story, something that makes them unique, interesting, and beautiful in their own way.\n",
      "I believe that we are all connected. We are one human race. We need each other to live our lives and make this world a better place for future generations.\n",
      "I believe there is so much beauty in the world if you just open your eyes to see it.\n",
      "I believe that the more you give of yourself, the more you will receive back.\n",
      "I believe that everyone deserves a second chance; even when we don‚Äôt deserve one ourselves.\n",
      "I believe in the importance of making time for things and people you love, because life is short and too often we forget to live it to its fullest.\n",
      "I believe that no matter how broken or imperfect someone\n",
      "llama_print_timings:        load time = 23582.07 ms\n",
      "llama_print_timings:      sample time =   403.38 ms /   512 runs   (    0.79 ms per token,  1269.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1029.20 ms /   265 tokens (    3.88 ms per token,   257.48 tokens per second)\n",
      "llama_print_timings:        eval time = 13973.01 ms /   510 runs   (   27.40 ms per token,    36.50 tokens per second)\n",
      "llama_print_timings:       total time = 15532.94 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cd7b061-19bd-40e2-9262-fb3054632466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568151\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to find your gift. To find your purpose‚Äîwhich Parker Palmer describes as \"the place where your greatest passion meets the world's greatest need\"‚Äîand then to give yourself to it.\n",
      "Giving, in my book, means using your gifts and talents for something larger than yourself. It may involve taking a risk or moving out of your comfort zone; it definitely requires discipline and focus. But once you find that place where your passion meets the world's need, something amazing happens: You stop being obsessed with yourself!\n",
      "This is a lesson I have been learning my whole life‚Äîand one I am still trying to master. I was raised in a family of givers. My father, who died when I was 15, had a heart as big as the world and believed that if you live right and do right by others, everything will work itself out.\n",
      "My mother is a great role model for generosity. She has always found ways to give back: to her family, to her church and community, to the poor. I have been blessed with a lot of good things in life, but none of them can compare with the greatest wealth of all‚Äîwhich is the act of giving itself.\n",
      "When I was growing up, we never had much money. Mom worked hard to support us as a hospital dietitian and sometimes took on extra work as a private chef. Dad was a psychiatrist in the Air Force; later he started his own practice. One of my most vivid childhood memories is of watching my dad make house calls to patients who couldn't afford his regular rates. He didn't do it for money, but because he thought it was the right thing to do.\n",
      "The first time I gave a speech in public‚Äîat a school assembly when I was 14 years old‚ÄîI talked about my dad and how he had inspired me. The title of my talk was \"To Live Is to Give.\" It wasn't original; Dad always used to say that. But as I spoke those words into the microphone, I realized they were true.\n",
      "Living is giving. If you aren't giving, you aren't living. Giving means being generous with your time and energy, your love and compassion‚Äîand yes, your money. It means thinking about how you can help others, not just yourself. It means taking responsibility for the people around you,\n",
      "llama_print_timings:        load time =  3190.23 ms\n",
      "llama_print_timings:      sample time =   393.91 ms /   512 runs   (    0.77 ms per token,  1299.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1029.46 ms /   265 tokens (    3.88 ms per token,   257.42 tokens per second)\n",
      "llama_print_timings:        eval time = 13996.26 ms /   510 runs   (   27.44 ms per token,    36.44 tokens per second)\n",
      "llama_print_timings:       total time = 15546.99 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "049ca020-506c-4016-9a6b-bb68d0433687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568172\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/30B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size =    0.14 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2215.41 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (768 kB + n_ctx x 208 B) = 436 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 63/63 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 20951 MB\n",
      "llama_new_context_with_model: kv self size  =  780.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to experience as much as possible, and to learn from it.\n",
      "In order to do that we need time. Time to be able to do things we like, like traveling, going out with friends, being a part of our community, and time to do things we don‚Äôt like but still have to do, like work or take care of the kids.\n",
      "We all have a certain amount of time on this Earth. The difference is that some people have more of it than others.\n",
      "Some live for 100 years, and some die at birth. Some get sick from old age, while others die in their sleep. Some need to work hard the whole life only to make ends meet, and some get paid millions to play sports or make movies.\n",
      "The difference between these two groups of people is money. Money gives us time.\n",
      "If you don‚Äôt have enough of it you will be forced to do more work than you would like in order to survive. You will spend more time at your job and less time doing the things you love, or with people you care about.\n",
      "This is not how life should be.\n",
      "Money gives us time and therefore money is important. We need it to survive, but we also need it to be able to experience as much of this life as possible.\n",
      "It is impossible to have enough money in your life to buy all the time you would ever want. But it is not impossible to have enough money to allow you to do things that matter to you more often than most people around you are doing them.\n",
      "People like Bill Gates, Richard Branson or Elon Musk didn‚Äôt just get lucky and become rich by accident. They knew what they wanted from life, had a vision and worked hard towards it.\n",
      "They knew that money is important, but they also knew that the goal wasn‚Äôt to collect as much of it as possible. The goal was to be free to do whatever it is that they wanted. And to have enough time for everything they want to experience in life.\n",
      "It is not always easy to find your way in this world and find out what you really want from life, but I believe it is worth searching for it. And once you find it, go after it with all of your heart.\n",
      "You won‚Äôt get another chance at this life. So make the most out of it.\n",
      "Pingback: The meaning of life ‚Äì What makes you happy? | Welcome\n",
      "llama_print_timings:        load time =  3203.45 ms\n",
      "llama_print_timings:      sample time =   412.86 ms /   512 runs   (    0.81 ms per token,  1240.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1029.73 ms /   265 tokens (    3.89 ms per token,   257.35 tokens per second)\n",
      "llama_print_timings:        eval time = 14002.54 ms /   510 runs   (   27.46 ms per token,    36.42 tokens per second)\n",
      "llama_print_timings:       total time = 15572.27 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/30B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad89ce92-0c6a-4b91-a6d2-6ee592786eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568194\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81d36643-14dd-4e54-9282-bb5fc99e2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568240\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807a58b4-9bb4-4e68-83a8-7a27bfde065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568246\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/65B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 8192\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 64\n",
      "llama_model_load_internal: n_layer    = 80\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 22016\n",
      "llama_model_load_internal: model size = 65B\n",
      "llama_model_load_internal: ggml ctx size =    0.19 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 3278.82 MB (+ 5120.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (1536 kB + n_ctx x 416 B) = 872 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 80 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 83/83 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 40943 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/65B/ggml-model-q4_0.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df2c18-e5b0-480f-ab4a-690e2abf27f1",
   "metadata": {},
   "source": [
    "### f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae0f9eb-5a55-4214-8d30-d6ff7b7ba728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568252\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to discover your purpose, live it and inspire others.\n",
      "I am a self-driven person who has a passion for life and always looking forward to see what‚Äôs next in my life. I have a positive attitude toward life which makes me enjoy every day of mine. Having an open mind makes me appreciate the little things in life, and it creates more opportunities for me to learn from different people and experiences.\n",
      "I love to travel and discover new places. It is amazing to see how people live their lives outside Singapore. I try my best to bring back those learnings into my workplace so that I can help others grow through the same experiences. I enjoy watching movies, cooking, reading and playing basketball as well.\n",
      "I have been working in different industries such as logistics, sales, customer service and education. Throughout these years of experience, I learnt that it is crucial to build strong relationships with customers/students and colleagues. Listening to people‚Äôs needs and providing them the most suitable solution are what makes me feel satisfied about my work.\n",
      "I am proud that after 10 years in education industry, NIE has given me a chance to be part of the team again as an educational consultant. I hope to serve more students in their search for the right courses and schools abroad. It is rewarding to see them pursue higher education and achieving success in life.\n",
      "I am thankful to have this opportunity to work with NIE! The team here is very supportive, friendly, helpful as well as patient. I feel encouraged and motivated to do my best every day. Looking forward for more exciting challenges ahead.\n",
      "Achieved 90% attendance in school during 2015-2017.\n",
      "Congratulations to our P6 students who have received the A* for their PSLE! We are so proud of you and we know that you will continue to work hard and strive towards your goal in life! Good luck as you take on your next journey!\n",
      "Holiday Camps 2019 ‚Äì New Theme Announced: LEGO¬Æ, Engineering & Robotics Camp!\n",
      "The holiday camps has been a great success since its launch last year. We received many positive feedbacks from parents and students. Our camp themes are always carefully selected to suit the interests of our students. This year, we have come up with an interesting\n",
      "llama_print_timings:        load time = 17036.54 ms\n",
      "llama_print_timings:      sample time =   389.44 ms /   512 runs   (    0.76 ms per token,  1314.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   259.20 ms /   265 tokens (    0.98 ms per token,  1022.39 tokens per second)\n",
      "llama_print_timings:        eval time =  8587.21 ms /   510 runs   (   16.84 ms per token,    59.39 tokens per second)\n",
      "llama_print_timings:       total time =  9362.80 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "171afbbb-f105-4580-ab1d-0545f0f95922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568281\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to become a living soul, while having a heart so big that God can use us in the world.\n",
      "I was born on October 15th, 1978 in St. Louis Missouri USA, where I grew up and attended high school. After graduating from Webster University with an undergraduate degree in Psychology, I traveled to Israel for a few months and then moved back to the US to attend graduate school at California Coast University.\n",
      "During my time in grad school I worked as a counselor for at-risk children. It was then that I realized that what I really wanted to do is help people heal through the power of love, kindness and compassion. This led me into the field of holistic healthcare where I have spent the last 10 years helping people heal their bodies and minds from physical illnesses like cancer or diabetes all the way up to emotional issues such as addiction or depression.\n",
      "Although my life has been full of challenging moments, it‚Äôs also filled with amazing experiences that have helped shaped who I am today. These include traveling to 25 countries around the world, living in a Buddhist monastery for almost a year and serving as an ambassador for peace in Israel.\n",
      "As a result, I developed a love of people from many different cultures and backgrounds. I believe that we can all learn something from each other if we just take the time to listen.\n",
      "I am also passionate about helping people create healthy relationships with food through my work as an accredited holistic nutritionist in California. It‚Äôs a privilege to help others heal their bodies and minds so they can live a life filled with purpose, meaning and love.\n",
      "Today I spend my time between the US, Israel and Spain where I have a private practice and I also facilitate workshops in holistic healthcare. When I‚Äôm not working, you will find me traveling around the world to new destinations or simply hanging out with friends and family.\n",
      "You can read more about my story here.\n",
      "If you want to get in touch with me, please use the form below. I look forward to hearing from you!\n",
      "I‚Äôd love to hear your feedback on this site or anything else that is on your mind. Feel free to reach out using the following form and I will be sure to respond\n",
      "llama_print_timings:        load time =  2202.68 ms\n",
      "llama_print_timings:      sample time =   384.31 ms /   512 runs   (    0.75 ms per token,  1332.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   259.87 ms /   265 tokens (    0.98 ms per token,  1019.76 tokens per second)\n",
      "llama_print_timings:        eval time =  8577.42 ms /   510 runs   (   16.82 ms per token,    59.46 tokens per second)\n",
      "llama_print_timings:       total time =  9348.64 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be6552a6-85f0-4059-9ba2-11b7499e262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568294\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/7B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 1820.08 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 288 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 35/35 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13918 MB\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "\n",
      "system_info: n_threads = 1 / 48 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 512, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m I believe the meaning of life is\u001b[0m to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back?\n",
      "We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son.\n",
      "I believe the meaning of life is to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back? We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son.\n",
      "I believe the meaning of life is to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back? We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son. We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son.\n",
      "I believe the meaning of life is to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back? We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son. I believe the meaning of life is to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back? We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son.\n",
      "I believe the meaning of life is to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back? We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such a wonderful husband and a beautiful son. I believe the meaning of life is to be happy, so why not live a life that you want? And why not do it with someone whom you love and who loves you back? We are all searching for happiness. I found my happiness in my family. I am grateful every day that I have such\n",
      "llama_print_timings:        load time =  2270.35 ms\n",
      "llama_print_timings:      sample time =   400.21 ms /   512 runs   (    0.78 ms per token,  1279.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   259.85 ms /   265 tokens (    0.98 ms per token,  1019.81 tokens per second)\n",
      "llama_print_timings:        eval time =  8581.63 ms /   510 runs   (   16.83 ms per token,    59.43 tokens per second)\n",
      "llama_print_timings:       total time =  9368.81 ms\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/7B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94746285-cab0-4477-b8d4-99d1c22544ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 845 (672dda1)\n",
      "main: seed  = 1689568308\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9\n",
      "llama.cpp: loading model from ./models/13B/ggml-model-f16.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (pre #1405)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 1 (mostly F16)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  = 2148.60 MB (+ 1608.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 360 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 26483 MB\n",
      "CUDA error 2 at ggml-cuda.cu:3606: out of memory\n"
     ]
    }
   ],
   "source": [
    "!./main --color  -ngl 10000 -t 1 --temp 0.7 --repeat_penalty 1.1 -n 512 --ignore-eos -m ./models/13B/ggml-model-f16.bin  -p \"I believe the meaning of life is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d1eee-2c59-4354-8422-e67877299371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
