{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0ab3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                              ggml-vocab-falcon.gguf\n",
      "\u001b[1m\u001b[36m13B-v2\u001b[m\u001b[m                           ggml-vocab-gpt-neox.gguf\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                              ggml-vocab-llama.gguf\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                              ggml-vocab-mpt.gguf\n",
      "\u001b[1m\u001b[36m70B-v2\u001b[m\u001b[m                           ggml-vocab-refact.gguf\n",
      "\u001b[30m\u001b[43m7B\u001b[m\u001b[m                               ggml-vocab-stablelm-3b-4e1t.gguf\n",
      "\u001b[1m\u001b[36m7B-v2\u001b[m\u001b[m                            ggml-vocab-starcoder.gguf\n",
      "ggml-vocab-aquila.gguf           \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "ggml-vocab-baichuan.gguf         \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6699f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the GGUF read/write example\n",
    "# !make gguf\n",
    "\n",
    "# # write a dummy GGUF model to test.gguf\n",
    "# !./gguf test.gguf w\n",
    "\n",
    "# # read the dummy GGUF model\n",
    "# !./gguf test.gguf r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca0e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install Python dependencies\n",
    "# !python3 -m pip install -r requirements.txt\n",
    "\n",
    "# # convert the model to ggml FP16 format (edit your params.json file if the \"vocab_size\" mismatch)\n",
    "# !python3 convert.py models/7B/\n",
    "# !python3 convert.py models/13B/\n",
    "# !python3 convert.py models/30B/\n",
    "# !python3 convert.py models/65B/\n",
    "\n",
    "\n",
    "# # quantize the model to 4-bits (using q4_0 method)\n",
    "# !./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/13B/ggml-model-f16.gguf ./models/13B/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/30B/ggml-model-f16.gguf ./models/30B/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/65B/ggml-model-f16.gguf ./models/65B/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Darwin\n",
      "I UNAME_P:   arm\n",
      "I UNAME_M:   arm64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \n",
      "I NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \"\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "I CC:        Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "I CXX:       Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope\n",
      "build-info.o\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-backend.o\n",
      "ggml-metal.o\n",
      "ggml-quants.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "llama.o\n",
      "sampling.o\n",
      "train.o\n",
      "tests/test-c.o\n",
      "benchmark-matmult\n",
      "common/build-info.cpp\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "vdot\n",
      "q8dot\n",
      "train-text-from-scratch\n",
      "convert-llama2c-to-ggml\n",
      "simple\n",
      "batched\n",
      "batched-bench\n",
      "save-load-state\n",
      "server\n",
      "gguf\n",
      "llama-bench\n",
      "libllava.a\n",
      "llava-cli\n",
      "baby-llama\n",
      "beam-search\n",
      "speculative\n",
      "infill\n",
      "tokenize\n",
      "parallel\n",
      "finetune\n",
      "export-lora\n",
      "lookahead\n",
      "metal\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Darwin\n",
      "I UNAME_P:   arm\n",
      "I UNAME_M:   arm64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \n",
      "I NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \"\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "I CC:        Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "I CXX:       Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread    -c ggml.c -o ggml.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c llama.cpp -o llama.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/common.cpp -o common.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/sampling.cpp -o sampling.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/console.cpp -o console.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread  -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread     -c ggml-quants.c -o ggml-quants.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/train.cpp -o train.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread  -c tests/test-c.c -o tests/test-c.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/build-info.cpp -o build-info.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  pocs/vdot/vdot.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  pocs/vdot/q8dot.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/export-lora/export-lora.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/metal/metal.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o metal -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -framework Accelerate -framework Foundation -framework Metal -framework MetalKit   -Wno-cast-qual\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/gguf/gguf.cpp ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -framework Accelerate -framework Foundation -framework Metal -framework MetalKit  -Wno-cast-qual\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c68bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975312\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.97 MiB\n",
      "llm_load_tensors: mem required  = 3647.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.98 MiB, ( 3648.61 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 3904.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 3975.16 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, who had been so wise in some respects, and had hitherto borne herself so well, was now for a time given over to a species of insanity. A certain madness ran through the whole country, from north to south; from east to west; from the top of the Exchequer-stairs (which then were twenty-four steps) to the bottom of Cheapside: among the rich, the poor, and the middle class of people. And for a wonder, there was not one who had his reason, but he went mad, except John Wilkes, whose insanity never did forsake him. \n",
      " It was at this season of universal lunacy that I lighted on the premises of the Osgood Schroffs: and learned to my sorrow, that their patient Selina was not after all to be the bride of my bosom. Poor fellow! it is not in human nature, that I should now have any interest left in the concerns of Mr. John Dashwood; but when I hear of his engagement to Miss Lucilla LynnI must confess to a secret pang. I would almost give worldsaye and my life into the bargainthat it had been to me! \n",
      " But to proceed. In process of time, Mrs. Dashwood, who was indefatigable both in visiting and writing letters, succeeded not only in discovering all her husband's weaknesses, but likewise in making some allowance for them: so that his conduct no longer appeared so very bad to her; it lost even the air of insolence which she at first beheld in it. The acuteness of her own feelings made her still more sensible of its excessive folly; and yet, with all this, as it had been her fate to love a man who was utterly unworthy of such affection, so it appeared, by the event, that she had chosen sensibly: for she never loved again, and she certainly could not have done better. She now found herself in the possession of all those domestic comforts which she had left behind her at Norland; and in a situation to dispose of them to the greatest advantage: for Sir John and Lady Middleton lived but a few miles from Barton: and while she was with them, it would have been highly improbableeven for the sake of an offer as generous as that which had been made to her by Mrs. Dashwoodfor her to leave the comfortable home which she found ready to welcome her. In short, she soon grew so fond of Devonshire and its society (not forgetting in particular Sir John Middleton's society) that after remaining about a year with them, and finding herself absolutely unable to quit them before the close of the season, she resolved to make one of their number during the ensuing winter: and accordingly, as soon as it was known that her return was intended for Christmas, the invitation was repeated.\n",
      "When this period arrived, thereforewhen Sir John Middleton had been domesticated by two or three months' residence in Barton cottage; when every thing about Mrs. Dashwood and her daughters seemed to promise a continuance of comfort; and when it was found that Harry and Eleanor were only kept from them because he was at school, and she still living with the Middletonswhen this period arrived, I say, every body expected the return of Mr. Dashwood and his daughter by a few days. He cameand in what an ill-humour was he come!Mrs. Dashwood would not believe it possible;for he came with the determination of breaking to her, at once, that he considered her home as lost for ever. She entreated him to sit down and relate the matter to her before he took any stepbut his first inquiry was after Eleanor. She had just been carried upstairs by Mrs. Dashwood. He ordered a glass of wine; but it was not taken till afterwards; for, when he sat down, he could say nothing else than thisWell, ma'am, there is no help for me: I am lost to you for ever!\n",
      "Mrs. Dashwood had never seen him look so gloomy before. She instantly knew that his mind was made up in some way or other on the subject of their removal. Eleanor soon returned; and, though she looked paler than usual, her air expressed nothing but surprise at a question which he addressed to her about her sister-in-law's health. What are we all coming to? cried Elinor in alarm.Sit down, said Mrs. John Dashwood:he took the chair she offered him, and without any farther preface began thus:\n",
      "Mrs\n",
      "llama_print_timings:        load time =     904.26 ms\n",
      "llama_print_timings:      sample time =      93.86 ms /  1024 runs   (    0.09 ms per token, 10909.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     405.71 ms /   494 tokens (    0.82 ms per token,  1217.62 tokens per second)\n",
      "llama_print_timings:        eval time =   11160.37 ms /  1023 runs   (   10.91 ms per token,    91.66 tokens per second)\n",
      "llama_print_timings:       total time =   11799.02 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2325fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975325\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.97 MiB\n",
      "llm_load_tensors: mem required  = 3647.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.98 MiB, ( 3648.61 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 3904.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 3975.16 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England had been through this stage of prosperity in 1763, but did not arrive at this last till about the end of Queen Charlottes eleventh year (1809); which was signalised by the resurrection of Southcottism, and an abortive revival of Jacobinism.\n",
      "A few words as to that strange delusion called the Shaking Palsy, or St Vitus Dance; a disease incomprehensible to physicians, but not to those who have felt it. When I had the honour (as I may say) of attending at Mr. Pickwicks house-warming, my attention was arrested by observing, every now and then, an old lady gently wriggling herself about on a sofa; and after some consideration, I discovered that her motions were in perfect keeping with the musicnow quickening into briskness, as the tempo became faster: when suddenly she stopped short, with an exclamation of terror, as if she thought the orchestra had gone mad. On questioning, however, she said she felt a little faint; and that was all. Mr. Weller senior having mentioned, in his reply to a question put by Sam, that the old woman were always doing it, I inferred that the affliction of which we were speaking was that of shaking palsy; and accordingly read up a report on it, in my physicians book. Mr. Pickwicks account tallied exactly with this statement. He told me that the old woman had been seized with the Shaking Palsy, during a party at Mrs. Gamps, two or three years back. How long does it last? inquired Sam.\n",
      "Its indefinite, replied Mr. Pickwick; they are incapable of saying when they will get better.\n",
      "What do the Doctors say about it, Sir? inquired Sam.\n",
      "Oh! Nothing at all, answered Mr. Pickwick: Ive heard that some people believe it is caused by bad air; and that other people say it arises from a cold taken when one was very young.\n",
      "Here again Mr. Weller senior smiled, and said something in a whisper to his son: but I could not hear what it was; perhaps Sam himself might have been unable to do so, for he at once remarked upon the subject of which they were speakingSurely, says Sam, the Doctor knows that?\n",
      "Mr. Pickwick nodded assent; and then added\n",
      "You can judge for yourself whether or no there is anything in this notion about the air; but you must take my word that it seems to me as if there really was something in it.\n",
      "Sam did not venture upon any remark of his own, but turned to Mr. Pickwick with a countenance which seemed to say plainly and distinctlyI hope you wont do anything rash about this matter. He was right; for just at that moment Mrs. Gamp walked in.\n",
      "She had not been gone ten minutes before Mr. Tupman, who happened to be within hearing distance when Sam made the last remark, remarked to me, very significantlyI hope he wont do anything rash about it! and, at the same time, turned his back upon the fireplace in which we were all sitting; for which reason Mr. Winkle was not so soon apprised of what had occurred as Mr. Tupman thought he would like to have been.\n",
      "It wont do, Tupman, said that gentleman, it really wont do! We must talk this over, and I will go and see the Doctor before dinner.\n",
      "Sam, who was sitting opposite to me, made a wry face as soon as Mr. Winkle was out of ear-shot; and then turned his countenance towards Mrs. Gamp, who was now seated in a chair, and apparently waiting for somebody or somethingfor the fire seemed to be her object in lifeand the moment she saw Sam turn round she exclaimed\n",
      "Oh! Mr. Snobby, sir! I do hope youll excuse my not rising.\n",
      "Certainly, maam, said Sam, looking with his head on one side at Mrs. Gamp, as if he were quite willing to be a little civil in her favourcertainly, maam.\n",
      "This was said with an air of such extreme condescension, that the ladys feelings underwent a sudden revulsion, and she smiled most graciously on Sam. He appeared to like it exceedingly; for after giving a glance round the room as if he thought that nobody else was there but his goddess\n",
      "llama_print_timings:        load time =     788.82 ms\n",
      "llama_print_timings:      sample time =      96.32 ms /  1024 runs   (    0.09 ms per token, 10631.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     406.86 ms /   494 tokens (    0.82 ms per token,  1214.17 tokens per second)\n",
      "llama_print_timings:        eval time =   11165.73 ms /  1023 runs   (   10.91 ms per token,    91.62 tokens per second)\n",
      "llama_print_timings:       total time =   11824.04 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975338\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.97 MiB\n",
      "llm_load_tensors: mem required  = 3647.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.98 MiB, ( 3648.61 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 3904.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 3975.16 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England had been overturned by the same process; but was now stopping short in her career, to take breath before resuming it. The two countries were at peace: notwithstanding that war (not made on either's account) was then actually raging in Europe: which might have been conducted by their respective monarchs, with a mutual indulgence of pride and self-esteem, according to the ancient and orthodox method of nations desiring each other's destruction. The King of France had lately made his Majesty a visit in England: the preliminaries whereof had consisted in a proclamation at Boulogne, offering an asylum to all the regicides and regicide authors now living. That monarch (whose crown was not of wood, like Oliver's) might have come to a bad end, if his subjects had obeyed the edict: though he got off with the loss of two millions sterling for the cost of that royal entertainment; which, being paid, England could afford to forgive him anything. \n",
      " CHAPTER THE FIFTH : Containing The Adventures Of Mr. Pickwick And His Comrades In Their Way Thither\n",
      " Chapter I - A Night Alarm at the George and Vulture\n",
      "IT IS NOT UNCOMMON in country districts for an innkeeper who is not on a footing of intimacy with his guests, to send round his ostler or barmaid into their apartments as soon as he goes to rest, with a candle in his hand, to see that all is right and safe: the ostler generally proceeds thus far in his mission. In London it is quite different: the waiter carries a couple of candles; and the landlady, who knows her guests better, walks herself into their room or bed-chamber with them. The chambermaid is generally too sleepy to stir herself for anything but tips, which are considered as mere perquisites by all except such unfortunate people as have got them without doing any favour; and even then they are disposed to take the course of least resistance, and let a little cash slip through their fingers. \n",
      "Chapter II - Mr. Pickwick Meets With Some Adventures On His Way To Rochester\n",
      " MR. WELLER had determined to start at a quarter before seven o'clock; and, accordingly, was engaged in getting ready for his journey by six. He took up his valise, and put on his great-coat and hat: he buttoned the latter as far down over his face as it would go, in order that nobody might know who he was: he turned out the gas at the street door, in order to save his candle; and he walked slowly down stairs. Mr. Pickwick did not come down so fast; partly from a consciousness of being a little under-dressed for a quarter before seven on such an occasion, and partly because he was rather fatigued by having got up early that morning in order to catch the coach: indeed, it may be necessary for me to explain here, that Mr. Pickwick's first intention had been to go on to London; but that his friend Sam Weller had so strongly objected to this, and had argued so strenuously in favour of an earlier departure from the metropolis (that being the only point at issue between them), that Mr. Pickwick had yielded to his authority, and resolved upon going back again by way of Rochester. \n",
      "Chapter III - The Two Travellers Reach Chatham\n",
      "MR. WELLER having got into a fly for his own convenience, went slowly down to the inn yard at Chatham; but Mr. Pickwick having been overtaken in his rapid progress by the coachman, was obliged to wait some little time for him. Mr. Tracy Tupman and Sam Weller had alighted before, and were making preparations to sit up all night over a bottle of sherry, at which the latter was very much disappointed: he said they ought to have stopped on the way for that purpose; and it was not without some difficulty that he could be prevailed upon to take a glass of brandy, in lieu thereof. By this time Mr. Pickwick had arrived.\n",
      "\"You'd better come along with us,\" said Mr. Weller, \"it ain't half so dull; and we can stop anywheres.\"\n",
      "Mr. Weller, being a little disappointed at not finding more people in the inn yard, gave an extra pull to his forefinger, by way of reminding Sam that he was not to lose sight of his friend: Mr. Pickwick, who had no knowledge whatever on this subject (having forgotten all about it during the night), put up his umbre\n",
      "llama_print_timings:        load time =     814.69 ms\n",
      "llama_print_timings:      sample time =      83.06 ms /  1024 runs   (    0.08 ms per token, 12328.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =     406.48 ms /   494 tokens (    0.82 ms per token,  1215.32 tokens per second)\n",
      "llama_print_timings:        eval time =   11111.81 ms /  1023 runs   (   10.86 ms per token,    92.06 tokens per second)\n",
      "llama_print_timings:       total time =   11730.10 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975351\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.12 MiB\n",
      "llm_load_tensors: mem required  = 12853.12 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.12 MiB, (12853.75 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, (13109.78 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, (13180.30 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, having begun to make money and spend it too, went down hill more slowly; but with a greater scramble. A little money got, turned round upon itself in the shape of interest for money borrowed: was lent out again for double the amount, which grew fourfold by repayment of the original sum with interest; then trebled by further loans made on the same security; and so on, till all the little begotten wealth had turned into an enormous sum. This latter being inadequate to the demand for it in Paris or Amsterdam, where they trade most with England, was again borrowed at compound interest: but as that could not be made out of money which produced no more, and the necessity for having more produced a want of means to produce it, the funds were raised by the sale of titles, honours, and places, in a manner to strike dumb all parties concerned. \n",
      " It is a good thing at least that men are so made as to prefer money to honour; otherwise the national debt would never have been paid off. The way to pay it, indeed, was not quite what the Chancellor of the Exchequer and the Treasurer of the Navy, in their wisdom and with all their knowledge of mankind, prescribed: but still the national debt has been paid; which is more than the debts of private gentlemen can say for themselves. \n",
      " This is not the time nor place for an Essay on Government; but that great evil of every state must be admitted by all wise men  in England especially by those who do their duty in public places, or take the trouble to know how it ought to be done, and why they cannot have it so. \n",
      " It is no less true than important to add, that nothing can possibly go on rightly while such a system continues; because, though many honest men are employed about the public business, it must necessarily be a greater number of rogues. These last are kept in employment by the rest: and when they are once fairly started upon the race of deceiving one another and defrauding the people, nothing will stop them till they have stripped their masters also. \n",
      " The Chancellor of the Exchequer has lately made a speech to this purpose; and though some gentlemen think it was not so good as a new pair of breeches, yet others consider that there was more in what he said than is usually understood by those who understand nothing at all about such matters. \n",
      " It must be allowed also, that many very able men, both public servants and private persons, are indefatigable in their endeavours to get the right things done; but they must do it in spite of everything  as a man must wade through ankle deep water, with a leg of mutton upon his shoulders. The thing itself is so difficult that nothing less than a miracle can accomplish it; and it would be easier to turn the wind from west to east, or get a man to take off his hat before God Almighty. \n",
      " One thing we must bear in mind  and never forget it, whatever happens; I mean that all honest men are willing to serve the public as they can, according to their station and opportunity. The fault does not lie with them: but in a system which forces those who wish to do well into the ranks of those whose object is self-aggrandisement and personal enrichment  not of the country. \n",
      " I have written this to the best of my knowledge and ability; because it seems to me that some one ought to write such things as are worth reading: for we cannot go on, always talking about ourselves, or how we have lost our shoes in walking abroad; or how much better a thing would have been done by somebody else. If I could be persuaded that my countrymen were at all in earnest and capable of doing anything  which I doubt very much; I should feel encouraged to write more frequently upon this subject. \n",
      " _Letter from Charles Lamb_ (17751834)\n",
      "The first half-century of the nineteenth century was marked by a profound conservatism in both political and cultural life. The Napoleonic wars had brought an end to the glorious age of revolution, the French Revolutionary wars having ended with Napoleon's defeat at Waterloo in 1815. Yet although France had been beaten on the battlefield, the Napoleonic Code was adopted as a model by many other European countries and Napoleon had established a new vision for Europe that had outlasted his brief reign. As well as encouraging greater cultural unity with the formation of the Latin Union  an attempt to revive Roman imperialism  Napoleon also led the way in a number of technological innovations, including steam power, and also promoted artistic and intellectual developments\n",
      "llama_print_timings:        load time =    2914.35 ms\n",
      "llama_print_timings:      sample time =      89.04 ms /  1024 runs   (    0.09 ms per token, 11500.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     358.28 ms /   494 tokens (    0.73 ms per token,  1378.80 tokens per second)\n",
      "llama_print_timings:        eval time =   25104.56 ms /  1023 runs   (   24.54 ms per token,    40.75 tokens per second)\n",
      "llama_print_timings:       total time =   25698.13 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975380\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.12 MiB\n",
      "llm_load_tensors: mem required  = 12853.12 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.12 MiB, (12853.75 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, (13109.78 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, (13180.30 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England had been long practising the same wholesome expedient; by which means she has contrived to pay for all manner of foreign luxuries and vices. Whenever that excellent mode of payment shall have impoverished her of everything, including all her present power over France, (which is at present merely a paper superiority) then it will be time enough to renounce the practice; or rather to return to the original, if by any means the country can be brought about again. \n",
      " But to go on with our story. England, though always in favour of the French revolutionists and against the Emperor (whom she had lately been at war with), was as anxious as they could desire that a restoration of monarchy should take place, if possible; which it fortunately did; though under circumstances too unfavourable to the Queen for her own wishes. The royal family were, indeed, restored to their ancient palace in Paris: but by their flight to Varennes they lost France and all the possessions of that kingdom as a prey for ever: while those who had rescued them from the hands of the rapacious mob (whom the Emperor had not left time to satiate) were thrown into prison, there to perish of starvation and neglect. The Emperor, however, who now began to be afraid of the insurrection he had fomented, was anxious enough to secure their release; which was effected by a general pardon, at his own expense, before Paris became inaccessible through the bloodshed and filth that covered it in the midst of revolutionary fervour. \n",
      " At the close of the year 1806 (the year of Bonaparte's greatest triumph), all France was at peace with itself, and every Frenchman under the empire; except the King and his family, who, from a mistaken notion that he could not do better, had come over to Paris as guests in their own capital. But they found that their position was too delicate for them to be suffered long to remain there. The people of France began to get tired of an established monarchy: the court, though well disposed towards it (and perhaps rather inclined than not to have had one), saw with alarm that nothing but the most unremitting vigilance could preserve them from the wrath and indignation of their countrymen. It was no longer enough for Bonaparte, now become Emperor, to show himself at court every morning: he must go out at night-time, and ride through the streets till nearly daybreak, accompanied by his staff only. Thus it happened that this first visit paid by our sovereigns to Paris was attended with much embarrassment; and after having remained there four months, they found it necessary to return to their capital. \n",
      " The Emperor Bonaparte now felt that the time had come for a new change in his policy: he had shown France what she could be made by the strength of one man's arm and mind: but, unfortunately for him, the whole country was not able yet to enjoy the blessings of this empire. There were some who hated him as their worst enemy; others, even more numerous, who regarded him with hatred and suspicion as a usurper of power which had belonged to them; there were still many, at least, who feared his ambition, and believed he might one day turn that empire into absolute monarchy. All these parties were in arms against France, even those most hostile to the Emperor's pretensions; for they dreaded above all else the unbridled passions of a man in such circumstances. \n",
      " The French people had begun to doubt his wisdom, and his capacity for governing them: in this country it is true that his presence was generally considered as a good omen: but there were many who feared for their country's peace; while others again looked upon the Emperor with an air of respectful adulation. There was a considerable number who hated him on account of his foreign origin, and those in his own family circle who had been most prominent in all that he had done to subjugate France: there were many also who would not have shed any tears at seeing the Emperor and his wife in their grave-clothes. \n",
      " These different parties began to find themselves in a very critical position; and even those whom Bonaparte had won over by his generosity, or the promises he had given them of support during the troubles that were about to come upon us, had already begun to feel that their interests could only be promoted under other circumstances than the Emperor's reign. \n",
      " The French people at large believed they should soon hear some new plans proposed by the government; and they awaited with anxiety its announcement of such a change as might preserve France from those wars which it saw about to\n",
      "llama_print_timings:        load time =    2500.96 ms\n",
      "llama_print_timings:      sample time =      88.40 ms /  1024 runs   (    0.09 ms per token, 11583.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     359.06 ms /   494 tokens (    0.73 ms per token,  1375.81 tokens per second)\n",
      "llama_print_timings:        eval time =   25126.31 ms /  1023 runs   (   24.56 ms per token,    40.71 tokens per second)\n",
      "llama_print_timings:       total time =   25713.34 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975408\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.12 MiB\n",
      "llm_load_tensors: mem required  = 12853.12 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.12 MiB, (12853.75 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, (13109.78 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, (13180.30 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was in a similar line, but had got as far as having a debt of three hundred million francs  an enormous sum to carry about ones pouches at that time; France having the advantage of carrying her own interest in a small portable bundle. The French King had taken a fancy to make his son-in-law, Mr. Wilkes, Earl of Pembroke and Montgomery, with a peerage of twenty thousand pounds  no more, no less  and the English Parliament was disposed, not only to accept that favour with gratitude, but also to confer upon His Most Christian Majesty an equivalent favour in recompense thereof. The French monarch had likewise taken a fancy for some other matters: which being rejected by the British Government, a certain body of Frenchmen  a certain number of them, at all events  thought they might as well take the hint and go to work for themselves, and accordingly did so. \n",
      " When things are put fair and square before human nature, it has always been found in the long-run that the wisest, kindest, best, most self-denying of the species will not carry out any such transactions as these  though carried through with all the delicacy of manner possible to a man who thinks himself a great deal cleverer than his fellow-men. A few such men have now and again appeared upon the earth; but in general it is found that human nature prefers the reverse of what they do, which is usually to take out their own pockets and fill up somebody elses  taking care at the same time never to make either the one or the other too full. \n",
      " Now, it happened just about this very period of the French Revolution that a certain number of men had taken it into their heads that they wanted a reform in some things: but, being a little short on the matter of any really new ideas upon which to found their demand for reform (for these things have been going on ever since man began to act and think as man), they went out one fine morning into the street before the palace, wherein dwelt a certain gentleman named Louis XVI., King of France and Navarre  who was not exactly their most cordial friend upon earth. This unfortunate monarch, it appears, had been reading in his closet with his queen for some time past, when they suddenly broke off to send away their children  as all fathers and mothers do from time to time: and while the Queen of France was getting ready to go to take them back again, the King was sitting in a little chair before the fire, smoking his pipe. \n",
      " Now, this pipe  it being then a new thing for him to smoke at all, or to smoke as other people did  had somehow got into an extraordinary state of lightness, and kept hopping about on its holder without any sort of stability whatever. It was also beginning to be a little too warm for comfort. The King looked up; the Queen came back; the pipe jumped; the King coughed: it fell upon his foot, and knocked off one of his boots. His Majesty now began to be a little disquieted: he looked about him, and saw nobody there but a mob of ragged people who had not been invited  he put on his other boot; and the ragged people, not understanding what he was doing, set up such an infernal screaming, yelling, howling, bawling, roaring, rumbling, that the King grew alarmed, and jumped for the door. \n",
      " Now the King was not used to a revolutionary rabble of any sort: he never had been before, in his life. He began to run for it; but finding himself suddenly met by a mob, he thought it best to stand still again; and stood with his hands in his pockets. This did not help the matter much  no more than standing on one leg would have done. The rabble were upon him now: he was knocked down, trampled on, and beaten. But what is this? They are letting him up again! His Majesty got up, wiping his face: they have picked out another pair of boots for him, too! What shall I do with them? thought the King; they were so new, he could not bear to put them on  yet it would be dreadful to go barefoot in the presence of this mob. The mob now came and offered him its hand: it was as bloody as if it had been cut up for his dinner. He looked at its fingers: he did not like the taste; but, considering it was a sort of salute, he took it, saying with a smile, `How do you do?' They then demanded he should shake hands all round with every man in the rabble  and there were\n",
      "llama_print_timings:        load time =    2538.21 ms\n",
      "llama_print_timings:      sample time =      99.38 ms /  1024 runs   (    0.10 ms per token, 10303.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     360.20 ms /   494 tokens (    0.73 ms per token,  1371.45 tokens per second)\n",
      "llama_print_timings:        eval time =   25211.40 ms /  1023 runs   (   24.64 ms per token,    40.58 tokens per second)\n",
      "llama_print_timings:       total time =   25827.86 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975437\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: mem required  = 7024.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.03 MiB, ( 7024.66 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, ( 7424.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, ( 7499.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the leadership of an eminent frenchifyer of the time, she was most sedulously trying to out-Herod, Herod; and was even labelling her rulers murder of his family, the Crassus affair.\n",
      "A branch of the family de France had lately played at Napoleonics in a chronicle published under the title of the Life of the Emperor Napoleon. This book is now in its second edition: in which the publishers have committed the grave error of not printing it, from the beginning, to form the letter O. The account they give us of an original manuscript, of which we hear no more afterward (if it ever existed at all), would be as incoherent and contradictory a narrative as any that was ever offered to the public: the evidence is that no human creature could have written it. It must be viewed, however, as part and parcel of the harvest that France is now reaping from the seed sown in the Napoleonic epoch; part of a long series of illusions which began with the Grand Army and ended with the Army of Italy. \n",
      "The story begins at the end: at Waterloo: then goes back to Hougomont, where the French were fighting, by way of establishing their claim to have been there. Thence it pursues them to Paris, where they arrived after having driven Blucher into a pass between two hills in Germany; and from this pass proceeds, at a venture, to the burning of Moscow, in Russia: thence, with a vast fleet and army, back again to France. Here there is a break in the threada very wide one, tooof two years and a half; when suddenly we are shown them again, and Napoleon has been beaten by Blucher at Leipsic. From this point it follows him rapidly downhill, through Germany, Switzerland, France, Spain, Portugal, to Waterloo.\n",
      "From this summary of the book it will easily be perceived that the author is no friend of his hero; who, he takes great pains to show us, is a selfish, ambitious, tyrannical, unprincipled, vindictive, unpatriotic man. To him he attributes the most atrocious crimes: the massacres and devastations of whole provinces; the imprisonment of mothers in dungeons, and even in cavesto say nothing of their children tooin order that they might not perish by cold in those wretched hovels to which they were doomed by his orders. It is to him, he says, that we are indebted for the French Revolution; that, from the day he took command of the army on the 15th of July, 1789though there was no war then between France and Austriait commenced; and that, from the moment it did commence, his life has been one long succession of bloody wars. It is to him also that we are indebted for the invasion of Russia, which he undertook in 1812, merely because the Czar refused to be made drunk with French wine; and which brought such a crash upon his fortunes as has seldom been known since the fall of Napoleon. The author would have us believe that no other man ever existed who could compare with him for meanness or baseness. He accuses him of having caused his brother Jerome, Prince Regent of Westphaliawho, by the way, had behaved so ill to the King and Queen of Bavaria as to be disowned by them as a son, and whose wife was a divorced princessto murder an old woman of ninety, because she refused to lend him her snuff box. He also accuses him of having caused his own brother Joseph to put out the eyes of General Hoche, which, however, was not done till after Napoleon had given up all power in France. Of one thing we may rest assured, namely, that whatever meanness or baseness Napoleon ever committed, he never did it on behalf of an old woman of ninety.\n",
      "Having read Mr. Hazlitts account of the Emperor, as also his Life of John Milton, and a large number of other works which have been written since he died by his numerous admirers, I may fairly say that he is to me the most remarkable man who has ever lived, whether on this side of the grave or over it. His memory seems never to die out. To read what every one says about him reminds one of what Lord Bacon said when asked to write a memoir of Queen Elizabeth; namely, That it would be needless for he had only to turn his eyes to any one who spoke on the subject.\n",
      "llama_print_timings:        load time =    1649.25 ms\n",
      "llama_print_timings:      sample time =      88.28 ms /  1024 runs   (    0.09 ms per token, 11599.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     746.41 ms /   494 tokens (    1.51 ms per token,   661.83 tokens per second)\n",
      "llama_print_timings:        eval time =   18468.62 ms /  1023 runs   (   18.05 ms per token,    55.39 tokens per second)\n",
      "llama_print_timings:       total time =   19438.39 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975459\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: mem required  = 7024.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.03 MiB, ( 7024.66 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, ( 7424.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, ( 7499.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never plundered nor murdered, to punish him for having strangled a dog. It is not forgotten that in the early age of Queen Elizabeth, some Englishmen were hanged for pretending to be wizards: but no record exists of their having disturbed the government by so impressing it with a dread of their diabolical potency as that, in the fulness of success, it should be thus wholly engrossed. \n",
      "Mysterious are the ways of society; and on this mystic principle, do social improvements arise. From the Dunkirk Memorial to the New Poor-law, the world has yet to learn what the most effectual of these mysterious ways may be. The King of Prussia, with a profounder insight in such lore than any English minister could be expected to have, has lately let loose the German hordes, to subjugate Europe in double harvest-time; and it is well known that they, in their turn, have borrowed from England a phrase for the name of their movement.\n",
      "Such are examples of what is called Civilization. It was not until Charles II had been dead for some years that his subjects began to appreciate the merits of his reign; since which time an increasing number have so appreciated them as to admit that he might, without impeachment of any political virtue, have reigned on to this hour. In the course of a century or so, it is very possible that his memory may come to be popularly associated with some great social good, whose beneficent working no one will stop to consider how or why it has been effected. It is quite certain that he did nothing towards bringing about such a consummation: and indeed the wisdom of an English King might have been expected to be far above all consideration of any such matter. But stillwho shall say what may come to pass, in the natural order of things?\n",
      "In the meantime, however, it is not necessary that we should anticipate him too far. We have now to deal with Charles II only as a young man, and there are some considerations of a purely private nature which cannot be disregarded without injustice either towards the subject or towards the public. It would be an injury, for instance, if we were to assert that Charles II, at this early date, was in love with any particular woman; since all his biographers are agreed, upon evidence so strong and positive as is rarely met with, that he was not. He had been born into the world with a natural gift of charming manners, which became more and more engaging every year: but it did not occur to him that any of the women whom he found it necessary to live with for reasons of State could ever be of any further importance to himself than as mere amusement. If this should seem hard to believe, we must ask the reader to bear in mind how many such women there were at that time; and that each successive one had been chosen for some special reason, which might have its advantages or disadvantages, but was certainly no indication of a tendency towards sentiment on his part.\n",
      "This may seem at first sight to be an unfair account of so gallant a prince: but it is made more fair by the circumstance that the young King himself never denied any portion of it. When he had been married eight years, and the question was still asked why he had never fallen in love with his wife, the Duchess of Cleveland, he only replied that he supposed every man had a right to look out for as much happiness as he could find without looking very far; and he did not see why any one should think it more strange if a man went to market than if he went a-hunting. In which view of the case, perhaps, the reader may be inclined to agree with him: or else, if he wishes to be unprejudiced, he will be as good as his word, and look on in silence.\n",
      "Certain it is that when the question was put to a great number of persons, from one end of the kingdom to the other, not so much as one man or woman ever thought of denying, or even disputing, what the King had said: but everybody laughed heartily, and agreed with him. This being settled so clearly and satisfactorily, nobody can say that anything has been left undecided.\n",
      "But while this question was being asked in England, there were still many others being asked about the King on the Continent; and to these no one could ever find an answer. He was known in France as the Grand Monarque; but when he went to Paris (which had happened twice) to be\n",
      "llama_print_timings:        load time =    1454.88 ms\n",
      "llama_print_timings:      sample time =      86.63 ms /  1024 runs   (    0.08 ms per token, 11820.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.93 ms /   494 tokens (    1.53 ms per token,   655.23 tokens per second)\n",
      "llama_print_timings:        eval time =   18791.40 ms /  1023 runs   (   18.37 ms per token,    54.44 tokens per second)\n",
      "llama_print_timings:       total time =   19791.79 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975480\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: mem required  = 7024.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.03 MiB, ( 7024.66 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, ( 7424.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, ( 7499.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never plundered nor murdered, to punish him for having strangled a garden snake; burned a railway traveller alive, because he had the audacity to save himself from being run over by jumping off his carriage; and, of course, she had the goodness to ruin herself, for the benefit of her creditors. \n",
      " England, on the other hand, imitated the example of Mr. Micawber, and waiting till she had got her affairs into 'a clear' state, went to sleep in an arm-chair. She had some difficulty, indeedespecially in her capacity of landlady to the Lord Mayorin keeping awake through Christmas-time; and then slept the sleep of the just until after the festival of the Goddess of Liberty.\n",
      "For the particular purpose now under contemplation by Mrs. Skewton and her son, a good many reasons might have been advanced. To name no more than two: First, that the weather was as fine as heart could wish; secondly, that a certain very remarkable murder, or rather double-murder, had taken place not many miles from London; which afforded Mr. Carker in his official capacity of town reporter, an opportunity of displaying before all his admiring constituents the extent and variety of his professional resources.\n",
      "A gentleman named Caldwell, possessed of great wealth and influence, had been found shot to death by some one who, after the commission of this rash act, had decamped with his wife: a very beautiful woman; a lady whom everybody was disposed to esteem highly for her good qualities. Mr. Carker's account of the whole affair filled two long columns in the newspaper, and the public read it with avidity.\n",
      "The deceased gentleman had been killedso the report statedby some one who had come into his presence on a pretence of being desirous to consult him as to some property or other; whom Mr. Caldwell had probably admitted into his room, and that personage having afterwards murdered him, had absconded with Mrs. Caldwell. The lady's bonnet and shawl were found on the lawn of an old house in a neighbouring park, and there was reason to suppose she must have gone into one of the offices (there being three or four occupied by some groom or other), which had not yet been explored; but it could only be supposed that she had fallen downstairs, for nothing further had transpired. The most singular circumstance attending this case was its absolute want of any clue whatever as to the motives of such an outrage on society.\n",
      "Notwithstanding Mr. Carker's assiduities in search of one, not a trace of evidence could be discovered. But there was a strong impression that a large reward would lead to further discovery; and the public were so earnestly invited to exert themselves for this purpose that many hundreds of pounds were subscribed, to be paid on any information leading to the apprehension of Mrs. Caldwell's assassin, dead or alive.\n",
      "The effect of this appeal upon a certain section of the population was not unproductive; though the reward, even now, remains undistinguished. As Mr. Carker in his report said: 'It may be that the man who perpetrated this dreadful deed is still within reach, and will yet come to light by the evidence of some person connected with him.'\n",
      "There was a man who knew where he could lay hands on such a one at any time; but who would have nothing to do with him now. He was in prisona good long sentence hanging over his headand what had he got to look forward to if he should take the bull by the horns, and make himself free? He might find a man who could lay down his life for any friend of his; but that wasn't the way. No; not in that quarter.\n",
      "His friend was locked up, as aforesaid: an unfortunate creature whose mind had been undermined by the drinking propensities of a long course of hardship. He had come to London and sought a better condition, but his evil geniusin the guise of one of his own kindhad found him out; he was corrupted in no time; and as years went on, his evil deeds increased. At length his character was so bad that he could scarcely find any companion except among thieves. He had now reached a pass at which there seemed no prospect of amendment: he might be reformed, but not by\n",
      "llama_print_timings:        load time =    1494.91 ms\n",
      "llama_print_timings:      sample time =      96.79 ms /  1024 runs   (    0.09 ms per token, 10579.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     749.21 ms /   494 tokens (    1.52 ms per token,   659.36 tokens per second)\n",
      "llama_print_timings:        eval time =   22272.72 ms /  1023 runs   (   21.77 ms per token,    45.93 tokens per second)\n",
      "llama_print_timings:       total time =   23278.50 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975505\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.71 MiB\n",
      "llm_load_tensors: mem required  = 24826.71 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.72 MiB, (24827.34 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, (25227.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, (25302.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that these were dark times, though we have endeavoured to make them as clear as we could by our version of the New Testament. But they were the best of times for England; and possibly, the worst of times for France. \n",
      "CHAPTER I  \n",
      "THE OLD PIERROT AND HIS SON\n",
      "It was a dark night in Paris in June, fifteen hundred and eighty-five; so dark that the lamps could not be lighted; there were no lamps. They were not wanted where men held deadly councils in dark streets and dark rooms with little daylight or fresh air to assist them, but plenty of wine.\n",
      "There was a man, well known as The Seigneur de Mollevautte, standing at the window of his lodging looking into the narrow street below; he was thinking of something very sad which had happened to him lately in his own country, Brittanya thing that seemed to him at first an utter impossibility, a thing which he would have said was not in the nature of things: but it was true.\n",
      "He stood there gazing down into the black streets, thinking and brooding; he could hardly realise how his beautiful young wife could be so cruelly wronged; and yet he had come home to Brittany full of trouble, having learned that a young lady had been born in Paris as the lawful daughter of himself and his late dear wife; but when he got back home, he found the truth of the matter was quite different. His wife had made her own will long before she died, declaring that she would marry no one but some honest peasant whom she might love with all her heart: she did so love, as it happened; and her husband knew nothing of it until he saw his pretty young wife in the village church with her husband standing by her side. And he was the first to congratulate them when they were married at the church door.\n",
      "And now here he was in Paris againand where is my son? This was a question that troubled him very much; for when he was abroad, his wife had died and left him with a fine little baby boy of six months olda beautiful child who might have inherited all her mothers good looks. Where is he now? said the poor man again and again to himself.\n",
      "The day was just beginning to dawn when this sad man saw a young girl going past, looking very tired. She had come into Paris by train late at night; and as she walked down that dark street in the gray light of morning, her dress hung loosely about herher hands were red with cold, her head drooped wearily, and she was hungry: so she went up to the door of an old church that stood near there and knocked.\n",
      "The door was opened by a man who looked very sad and grave, and she asked him if he could give her some shelter until morning light; but when they went in and the man had looked at her face more closely, he started back with a cry of surprise and said to her, \"God save me, how you are changed! It is my own wife that I see before me.\"\n",
      "Then this sad father rushed forward and kissed his little boy over and over again, and when the good people who lived near came in to church, they saw what was happening, and they ran to get the old priest. But before he could come the girl had fallen down dead: she died with her head on the bosom of this father who had thought he never should see his child alive againand so that young wifes life was over as quickly as a flower is faded in the night.\n",
      "The little baby boy grew up to be a good and handsome man; but his father lived to old age, always saddened by this strange story of his lost wife, until one morning he died too. And now you see that we must all die sometime. It may be sudden or slow, but no matter how we get out of the world its all the same in the end; and if there is a God, He will surely give us good care when our time comes to leave this troubled life.\n",
      "Source: Charles Perrault, Histoires ou Contes du Temps Pass (1697), \"Le Petit Chaperon Rouge,\" pp. 38-41; translated by Dent and D\n",
      "llama_print_timings:        load time =    6184.60 ms\n",
      "llama_print_timings:      sample time =     221.67 ms /  1024 runs   (    0.22 ms per token,  4619.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     742.70 ms /   494 tokens (    1.50 ms per token,   665.14 tokens per second)\n",
      "llama_print_timings:        eval time =   47657.39 ms /  1023 runs   (   46.59 ms per token,    21.47 tokens per second)\n",
      "llama_print_timings:       total time =   48887.54 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975561\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.71 MiB\n",
      "llm_load_tensors: mem required  = 24826.71 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.72 MiB, (24827.34 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, (25227.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, (25302.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "\n",
      "There was another student in the Temple Gardens, who was likewise working early on this fine morning, and who had, therefore, an opportunity of observing the state of the air as well as his predecessor had; but he, poor youth, was not half so happily situated. He had a wife and child who were starving, and whom it behoved him to keep alive with all speed. For, they also were in the hands of Fate: she could not make an end of them in one place, while his eyes were upon them, without making another of them too many for him here; so she had taken them away from him.\n",
      "\n",
      "His name was Gaspard, and he belonged to a class of workmen who are never idle. He earned eight or nine shillings a-week by letter-carryingthat is to say, conveying letters through the postwhich required considerable labour, patience, discretion, and promptitude: he earned something more by copying verses on linen, for which purpose it is only necessary to know how to write: he had acquired that knowledge so early in life, that he had now forgotten who taught him. He worked out a sort of rude philosophy from the fragments of his reading; and he had taken into his service an unfortunate girl, by whose assistance, when her strength allowed it, he earned something more in washing linen: she had been brought to the hospital where she lay with an abortion, and had no friends nor means of living but such as he provided. He made a good wife of her, for she was very grateful, and was so much attached to him that she seemed to have no will but his; he never asked any promise from her but to do well, and she never failed him in that respect. She had borne him a daughter who was delicate, and had not survived the age of six months, and they had resolved that they would not attempt to rear another child until they were more established in their fortuneswhich would probably happen some day, as long as there were any letters in the post to carry.\n",
      "\n",
      "His lodging consisted of a room so small, that it might easily have been mistaken for one corner of an outhouse, though he paid three francs fifty-cents per week for it, and another two francs fifty centimes for light; he had bought all its furniturea bedstead and a stool.\n",
      "His table was his knee. A piece of wood, shaped like the blade of a fan, served him as a desk; this was covered with a sheet of paper which answered every purpose, because he could tear off a page from it when he had made an end of that particular jobthat is to say, if any one had given him a job. At other times, and especially in summer, he went to a public garden where there were seats under the trees, and worked among them on his knee. His paper was not very white; it might have been the paper which was sold at a farthing the sheet. In short, there could be no doubt that this man lived like a hermitin fact he had all the appearance of one, but for one circumstance, and that is, that he had the power to take a walk in the country every now and\n",
      "llama_print_timings:        load time =    5199.87 ms\n",
      "llama_print_timings:      sample time =     163.59 ms /  1024 runs   (    0.16 ms per token,  6259.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     658.59 ms /   494 tokens (    1.33 ms per token,   750.09 tokens per second)\n",
      "llama_print_timings:        eval time =   46605.50 ms /  1023 runs   (   45.56 ms per token,    21.95 tokens per second)\n",
      "llama_print_timings:       total time =   47631.04 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975614\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.71 MiB\n",
      "llm_load_tensors: mem required  = 24826.71 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.72 MiB, (24827.34 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, (25227.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, (25302.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian Pastor, she emerged from the state of nature into that of civilization; became an industrial nation, began to compete with England for the carry-trade, and entered on her career of conquest and colonization. The barriers of the Channel and the Pyrenees ceased to be obstacles. She sent her fleets and armies into the unknown south, there was no question made but she should become the mistress of the world. \n",
      "  15301871\n",
      "CHAPTER I  PARIS BEFORE THE FRENCH REVOLUTION\n",
      "##### ****THE CITY OF PARIS*****\n",
      "THAT Paris is a very old city, all must know who are acquainted with the history of France. It was called Lutetia by the Romans; and it has always been accounted the capital of the Gauls, though this is a matter on which antiquarians have disputed without end. There can be no doubt that when Julius Caesar arrived in the year 51 before Christ to establish his power there, he found an important population, for at least two thousand years had elapsed since it was first founded by those mysterious tribes, the Ligurians and Celts, who formerly lived upon this part of Europe. But Paristhe city with a soul and its streets still in existencewas built under Charlemagne and Louis the Pious. In the seventh century, when Charles Martel defended France against the Moors, he had no sooner defeated them at Tours than he rebuilt Lutetia on the site of the ancient Gallo-Roman city, and laid out its streets with their right angles in an Anglo-Saxon fashion. From that period dates the history of Paris, which is really very little more than two thousand years old; for the most part it is modern and young.\n",
      "Paris is built upon both banks of the Seine. On one side rise the heights of Sainte Genevieve (which have been called by some the hill of Saint Germain), on the other, Montmartre, or Mount Martyrs. The left bank is the lower; that to the right has been raised artificially for more than two thousand yearsso much so that when Caesar arrived he saw no houses upon it and only found the huts of fishermen on its shore. Thus the Celtic Lutetia stood upon a hill, whose base was higher than its top; the Roman city occupied the same site but the ground has since been filled in considerably. The most ancient portion of Paris, the Ile Saint Louis and the quay opposite it, still consists of sand, mud, and gravel which the Seine bears along with it from time to time and leaves there when its level falls during a drought. At certain seasons of flood the bed of the river has been seen at this point a good fifteen feet below its ordinary surface.\n",
      "The left bank is low-lying and flat. It was formerly overflowed every spring by the waters which rose higher than it does now; consequently all the houses on it stood upon wooden or stone pilings driven deep into the ground, for if the foundations had been solid they would have been submerged each year and have decayed at last. Hence, these dwellings were always built lowmere huts of one story. The streets are still narrow and winding, because it was difficult to get across from one side to another without going outside the houses, which stood very close together.\n",
      "On the right bank everything is different. It is a hill, sometimes rising above the water level in such a way as to expose its bed to view; at other times dipping so low that the buildings almost touch the river's edge. In all weathers the stream runs swiftly between its stone banks or leaps into foam against the quays which have been cut out of the hillside like terraces.\n",
      "Here and there, on both banks, a few old houses remain standing whose roofs are hidden by tall trees; their windows overlook the Seine with views so fine that many of them, even at this day, command enormous rents. Here we see only little cottages and hovels, with balconies projecting in front of them, such as the poorest laborer or workman might inhabit if he had but a few sous to spare; here the houses have broad cornices with stone balustrades, like those in the city itself. But these are not the abodes of rich folk. These streets, which have so long been neglected that the grass has grown on them, where the air is so fresh and pure and all seems so quiet and peaceful, still retain their old names which prove how lowly were its earliest inhabitantsthe Rue du Vieux Pont des Meuni\n",
      "llama_print_timings:        load time =    4911.15 ms\n",
      "llama_print_timings:      sample time =      98.64 ms /  1024 runs   (    0.10 ms per token, 10380.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     659.35 ms /   494 tokens (    1.33 ms per token,   749.23 tokens per second)\n",
      "llama_print_timings:        eval time =   46150.98 ms /  1023 runs   (   45.11 ms per token,    22.17 tokens per second)\n",
      "llama_print_timings:       total time =   47048.64 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6537f",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16c67651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975667\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.09 MiB\n",
      "llm_load_tensors: mem required  = 17505.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   166.63 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.09 MiB, (17505.72 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (18285.77 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (18382.78 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there was store of rude implements, and ruder machines, not yet delicately adjusted into form and use, but ready for the hand of Fate, when it should be put forth, to fit them for their destined purposes. \n",
      " But that Woodman and that Farmer, who should know so much, and tell so much, and do so much, and be so much, had never come together in their lives. They were to meet in the element of fire on the last ensuing fifth of November; and that Woodman was to yield him, prisoner. \n",
      " Book II\n",
      "Chapter I\n",
      "The Wine-shop\n",
      "In a large room, with a high, bare, monotonous ceiling, upon which the smoke from a thick hanging chimney-cloth had deposited a substance much resembling the coat of flat-fish, Mr. Charles Darnay had been wrongfully imprisoned for eighteen months.\n",
      "The accusation against him was that he was not distinctly French. Yet he was born in France, of a French family, and though he had lived ten years in London, had never naturalised himself in England. In consequence of his neglect in this particular, it had been judged necessary to imprison him (although he was no worse than his brother-subjects who were free), because it was not clear that his existence in the country was not prejudicial to the interests of France.\n",
      "In this conclusion, however, the jury appeared to have gone a step (or twelve steps) further on their own account: for, after deliberating about half an hour, they brought in the additional verdict \"Guilty of Treason.\" treason being a crime with which Mr. Darnay was not charged, the word was erroneously inserted in the sentence by the judge, when he recorded it.\n",
      "It is worthy of remark that nobody concernedexcept the defendant and the chief judges who tried himseemed to consider this accidental insertion of a word into an accusation important. Everybody seemed to consider it natural and inevitable that an accused person, if he showed any mark or sign of being conscious of his guilt (as undoubtedly Mr. Darnay did when he pleaded Not Guilty), should be found guilty by the jury; and that there was nothing surprising in his being afterwards sentenced to be imprisoned in the Bastille for life:\n",
      "\"This, however,\" said the Doctor, \"is not the general opinion; far from it. Suppose, without going any further than history which might or might not be exactly true (I don't vouch for it), that a certain English King took his pleasure one night in disguising himself and slipping into the House of Commons? That he went there to ascertain whether the members were doing their duty? That he saw them all fast asleep and snoring, with their chairs tilted back, except three or four wakeful ones at the upper end? That he slipped out againbeing a merciful monarchwithout disturbing them? And that next morning he issued his proclamation, declaring those members unworthy sinners who would despoil a defenceless monarch of his repose and endanger a nation? That this proclamation gave great offence? That the legislative bodies of England (if I may so call an assembly of laymen) remonstrated with him? That he was deaf to their suggestions, however much they might be founded in justice? That, finally, the members were dismissed from their seats? And that another parliament was instantly called, which did what it was desired to doin short, did everything that its predecessor had not done?\"\n",
      "\"All this I believe,\" said Charles Darnay, \"to be strictly true, but I ought to remark that I am rather singular in my good opinion of the House of Commons.\"\n",
      "\"Well! It is a thing of present concern enough\n",
      "llama_print_timings:        load time =    4189.50 ms\n",
      "llama_print_timings:      sample time =      84.18 ms /  1024 runs   (    0.08 ms per token, 12164.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1811.37 ms /   494 tokens (    3.67 ms per token,   272.72 tokens per second)\n",
      "llama_print_timings:        eval time =   38588.31 ms /  1023 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
      "llama_print_timings:       total time =   40630.61 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "148d30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975712\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.09 MiB\n",
      "llm_load_tensors: mem required  = 17505.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   166.63 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.09 MiB, (17505.72 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (18285.77 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (18382.78 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his forehead  because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there was sheltering for the night when the snow fell, a Cartwright  who little thought he was a Cartwright  who should one day become famous among men. \n",
      "# Chapter I\n",
      "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      "# Chapter II\n",
      "These two classes of people were like the two main currents in the ocean; they were ever at war, and ever inclined to fraternise. Both were so assured of their being in the right, that the thought of accommodation never occurred to them. Mr Lorry, having led his dangerous charge safely through the dark place, halted him on a bench beside Miss Pross's door, while he softly tried the latch and looked in. It opened noiselessly, and there sat Esther, singing at her work. \n",
      "# Chapter III\n",
      "Mr Lorry nodded his head with as much satisfaction as if he had been in a garden of flowers, instead of being in a narrow courtyard, with a high grim wall at the back, and an empty barrel-organ making a melancholy groaning in the resigned neighbourhood on the other side of the way. 'Esther dear, I hope you are well?' 'I am quite well,' said Miss Pross, proceeding in her work. 'There is no objection to your being seen here?' He asked the question while he looked at her, and his look had meaning in it. 'None at all,' said Mr Lorry. 'I know where you have been, and how you have come back. I have not been sorry to wait for you.' Esther looked up, embracing the kind old man with both her hands. The manner of her embrace was such that Miss Pross, standing stiffly behind her mistress's chair, newly planted in that home, thought: 'I should have done better. And yet how could I doubt him! Even so!' \n",
      "# Chapter IV\n",
      "A slight frown and a wondering smile arose simultaneously in Mr Lorry's mind. He looked at Darnay with renewed attention; and it suddenly grew clear to him that this good-natured, easy young man was a person of some capacity, who had grown intelligent in the things he had gone through. 'Yes,' said Charles Darnay. 'I have been to Paris.' 'On business?' asked Mr Lorry, in his abrupt way. 'On private business,' said Darnay. He glanced at Carton with an unsettled look, and Carton settled him by saying: 'Mr Lorry, she knows all about it, and has known it for years. That is why we say you may confide in her safely. The past is the past; nothing matters in that but our confidence and good faith to one another.' He spoke with a heedfulness, half of solicitude, half of caution: as though he were on his guard lest this confidence should be abused. 'And the present and the future may be discussed without restraint,' Mr Lorry added. 'Granting that I have that confidence in you, do you expect me to be very eloquent? Am I to entertain you with my communications?' asked Darnay. The last question he put with a smile\n",
      "llama_print_timings:        load time =    3570.33 ms\n",
      "llama_print_timings:      sample time =      90.56 ms /  1024 runs   (    0.09 ms per token, 11307.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1818.69 ms /   494 tokens (    3.68 ms per token,   271.62 tokens per second)\n",
      "llama_print_timings:        eval time =   38564.54 ms /  1023 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
      "llama_print_timings:       total time =   40625.54 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975757\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.09 MiB\n",
      "llm_load_tensors: mem required  = 17505.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   166.63 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.09 MiB, (17505.72 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (18285.77 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (18382.78 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the protection of his brother monarchs, George the Third, of England, and Louis the Sixteenth, of France, though the policies of both sovereigns were necessarily to a certain extent identical and inter-blended, the father of waters rolled uninterruptedly seaward, to be traversed in return only by British ships of war, conveying to the coast of France, the proprietors of estates in America. No intercourse was permitted between France and her transatlantic colonies, but such as suited their common lord. The spirit of liberty had been utterly extinguished in France; more than a million of her male population were slain: yet still she made paper money, and was so far from being exhausted thereby, that she continued to supply the world with gold and silver. \n",
      " England too was greatly excited, and by causes the very same. She had sustained a shock in France, which had long trembled to its centre. That shock, unexampled in history, which we call the French Revolution, staggered credulity itself. The altar fires of perjured kings; the crimes, follies, vices, and humiliations of thrones; the insolences of dynasties; the false show of strength by military parades and sham battles; the exposed nerves that quivered when the lash of power was laid upon them; the cowardice that faltered, and the meanness that traded, meanly cringing to liberty; the wild cries of suffering nations, long unheeded by the rulers whom they served; these were among the chief of the reflections which then started from their dread abodes. Long had the warning voice gone forth in England too, and long had it been unregarded. Revolution principles had been working underground.\n",
      "CHAPTER V  \n",
      "A Great Man Descended From an Angel\n",
      "WHATEVER happened to him later, when he was older, much older, Scrooge could never forget his fifth-grade teacher, Mrs. Dilber. She was a tall thin woman with jet black hair drawn tightly into a bun, held in place with sharp steel pins that protruded from her head like knitting needles.\n",
      "Her face was long and narrow and seemed to end abruptly in the tip of a sharp nose. Her eyes were small and beady and set deep in their sockets under heavy black brows. She had high cheekbones, but no lips or chin that Scrooge could ever recall seeing. He imagined those facial features existed somewhere below her neckline, perhaps tucked into the folds of her long black dress.\n",
      "Mrs. Dilber was known by every child who attended the school because of a singular incident that occurred in her fifth grade classroom. During an assembly one morning, Scrooge remembered hearing the headmistress say Mrs. Dilber had been teaching there for twenty years. At that time she taught the entire fifth-grade curriculum to a dozen students or more every yearreading, writing and arithmetic.\n",
      "The incident Scrooge spoke of occurred on his first day in her classroom. The students sat quietly at their desks waiting for Mrs. Dilber to call roll and take attendance. When the bell rang to end recess, each child had been directed by their fourth-grade teachers to line up outside the classroom door until summoned to enter by their new teacher.\n",
      "Soon after they lined up outside her classroom, Mrs. Dilber opened the door and welcomed them inside with a cheerful smile. \"I want all of you to look at each other,\" she said. \"Every student here will become your friend because we are going to spend many happy hours together this year.\"\n",
      "She closed the door behind her and walked among them, introducing herself individually to each child as they sat down. Then she went over to a desk in front of the classroom blackboard that had an inkwell attached to it by a wooden arm on one end of its top. \"Now we will begin with attendance,\" she said.\n",
      "As Mrs. Dilber called out their names, each student responded loudly and proudly with either 'present' or 'absent.' When she had finished calling roll, she turned back to the blackboard behind her desk and picked up a small container of chalk. \"Today is September 17th,\" she said, writing on the board with her left hand as she spoke to them.\n",
      "\"In five days we will celebrate Constitution Day together at school because it falls on Saturday this year. The day before that, which is Friday, our country will have celebrated its 129th birthday.\" Mrs. Dilber turned to face\n",
      "llama_print_timings:        load time =    3555.63 ms\n",
      "llama_print_timings:      sample time =      84.78 ms /  1024 runs   (    0.08 ms per token, 12078.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1815.42 ms /   494 tokens (    3.67 ms per token,   272.11 tokens per second)\n",
      "llama_print_timings:        eval time =   38811.29 ms /  1023 runs   (   37.94 ms per token,    26.36 tokens per second)\n",
      "llama_print_timings:       total time =   40855.76 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb0e7",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975802\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.77 MiB\n",
      "llm_load_tensors: mem required  = 62045.77 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   406.25 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.78 MiB, (62046.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (62826.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (62923.47 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. \n",
      " But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous. \n",
      " In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, \"in consequence of the failure of his ammunition:\"; after which the mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despatched his treasure with a pistol-shot, because he had no time to argue the point with a second highwayman, who came up for that purpose, and was content with his (the first highwayman's) thanks; churches were stripped of their lead, which was melted down in furnaces, and cast into bullets to shoot the King's subjects; flesh was scarcely ever to be had in London for love or money; and skilful tradesmen who exhibited signs on their houses, of cats or dogs, or other deceiving animals, escaped the ruin that was deservedly meted out to the more romantically inclined, who enticed the public into their melancholy shops by announcements of birds, and rabbits, and whales, and other fanciful representations of petrified monsters, allied to no creature ever beheld by human eye. \n",
      " II \n",
      "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way--in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth bl\n",
      "llama_print_timings:        load time =   14135.32 ms\n",
      "llama_print_timings:      sample time =     161.89 ms /  1024 runs   (    0.16 ms per token,  6325.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1608.20 ms /   494 tokens (    3.26 ms per token,   307.18 tokens per second)\n",
      "llama_print_timings:        eval time =  105482.45 ms /  1023 runs   (  103.11 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =  107459.32 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701975926\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.77 MiB\n",
      "llm_load_tensors: mem required  = 62045.77 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   406.25 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.78 MiB, (62046.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (62826.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (62923.47 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, \"in consequence of the failure of his ammunition\"; after which the mail was robbed in peace; that magnificent potentate, the Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in London gaols fought battles with their turnkeys, and the majesty of the law fired blunderbusses in among them, loaded with rounds of shot and ball; thieves snipped off diamond crosses from the necks of noble lords at Court drawing-rooms; musketeers went into St. Giles's, to search for contraband goods, and the mob fired on the musketeers, and the musketeers fired on the mob, and nobody thought any of these occurrences much out of the common way. In the midst of them, the hangman, ever busy and ever worse than useless, was in constant requisition; now, stringing up long rows of miscellaneous criminals; now, hanging a housebreaker on Saturday who had been taken on Tuesday; now, burning people in the hand at Newgate by the dozen, and now burning pamphlets at the door of Westminster Hall; to-day, taking the life of an atrocious murderer, and to-morrow of a wretched pilferer who had robbed a farmer's boy of sixpence.\n",
      "All these dreadful things, and a thousand like them, came to pass in and close upon the dear old year one thousand seven hundred and seventy-five. Environed by them, while the Woodman and the Farmer worked unheeded, those two of the large jaws, and those other two of the plain and the fair faces, poured forth their flood of talk. It was permitted to be scarcely intelligible, how it came about that they were all four talking at once, but it was not permitted to be otherwise than talk it was.\n",
      "As to characters so very odd and peculiar, the reader may be prepared to hear before long (although this is a Digression) that oddity was very far from being confined to them; for, in the very first year of the nineteenth century, all England seemed to be running a race against Time, to compass the greatest number of changes in the least space of hours.\n",
      "Within six hours of sunrise, on the first morning of this very year one thousand seven hundred and seventy-five, Mr\n",
      "llama_print_timings:        load time =   13243.27 ms\n",
      "llama_print_timings:      sample time =     191.29 ms /  1024 runs   (    0.19 ms per token,  5353.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1607.11 ms /   494 tokens (    3.25 ms per token,   307.38 tokens per second)\n",
      "llama_print_timings:        eval time =  105544.81 ms /  1023 runs   (  103.17 ms per token,     9.69 tokens per second)\n",
      "llama_print_timings:       total time =  107567.57 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71c4a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976049\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.77 MiB\n",
      "llm_load_tensors: mem required  = 62045.77 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   406.25 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.78 MiB, (62046.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (62826.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (62923.47 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there was shelter for the Cart before it became the Tumbril which Conquered Paris. But that Woodman and that Carman have not yet come into view, and so the course of the French stream is onwards still, notwithstanding a strong eddy that has recently been caused by the enforced connexion of the British fleet. \n",
      " It was the Dusk of the Evening; and Stryver, having shown his friend to a seat and placed his papers in his hands, leaned back in his own chair, and resigned himself to be filled with admiration at: _'Marvellous!' 'Superb!_ 'Though not altogether new to the respected profession in which he practised for he occasionally saw Mr. Stryver at that bar where forensic fame is made or unmade it was new to him to see a barrister do the dry as from an amphitheatre of spectators instead of out of the dusty little well in Westminster Hall: which, being a parched place, no judge's garden was ever more shorn and blighted than that same court of Chancery. In this pitiable plight it was Mr. Stryver's fortune to present himself on the twenty-eighth of June in the present year one thousand seven hundred and eighty.'\n",
      "'Say, rather, on this day _fortnight_ , my dear fellow,' interrupted Carton, breaking off as suddenly as he had begun, and flinging down the papers he had been looking over.  'No man who was not a barrister could follow the thing now. And even _I_ don't know what it's about! In the first place I don't understand law; in the second I am too honest to be sure I should entirely approve of it, if I did.'\n",
      "It again passed through Sydney's mind, that it was likely to prove an uncomfortable employment to assist a man with such notions as those in any professional business; he therefore showed no great alarm at the sudden change he detected in his friend. After all, there was no new revelation here: Carton had told him from the first, that he hated all his occupations alike, and detested none so much as he detested this.\n",
      "'Though it's not law,' said Carton, looking full at his friend with more meaning than he had ever designed to express before; 'I wish you would tell me I might get free of it, and of the other.'  (For the life of him, poor Darnay could think of no other way of describing the vague but not unreal chain that he perceived to bind him.)\n",
      "'My dear Carton,' said Sydney, shaking his head; 'I am afraid you have a hard bargain of it.'\n",
      "'Don't tell me so. You know I don't like the way in which I have been living here; you must have seen that yourself without my telling you. Don't add to my difficulty by insisting on its being hopeless to get free from it, because it is not a very easy thing for one in your own condition to do.'\n",
      "Sydney seemed deep in his own thoughts for a few minutes; and then he said:\n",
      "'I have never seen you at Stryver's so often, without seeing that there was a marked contrasta contrast pushful on your side, not on his. What I have observed, is confirmed by this.'  He showed Darnay a book which lay on the table. \"These are your notes of all your cases; and here,\" turning to another book, 'these are Stryver's of the same kinds of cases. Are you conscious of no contrast between them?'\n",
      "Sydney Carton looked full in his friend's face.\n",
      "'Here is a passage as to what damages would be reasonable, taken from\n",
      "llama_print_timings:        load time =   13354.96 ms\n",
      "llama_print_timings:      sample time =     171.69 ms /  1024 runs   (    0.17 ms per token,  5964.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1605.82 ms /   494 tokens (    3.25 ms per token,   307.63 tokens per second)\n",
      "llama_print_timings:        eval time =  104999.91 ms /  1023 runs   (  102.64 ms per token,     9.74 tokens per second)\n",
      "llama_print_timings:       total time =  106986.24 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0bef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976170\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.00 MiB\n",
      "llm_load_tensors: mem required  = 35091.00 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35091.00 MiB, (35091.62 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (36371.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (36490.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be lashed into tumbrils of Public Execution when the time should come. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of the Captain, habited anatomically like himself, excused himself by saying that he had only been making a little draft of flour, and then put his gun in his pocket, and went home to Mrs. Turpin. Occasionally, some lonely traveller on the roads stopped at a lonely public-house over night, and was robbed. In the ensuing action he might recognise the waiter or the landlord. English justice was very sharp after highwaymen, as after other criminals. It had hanged them in such numbers that their old haunts were quite deserted, and peaceable travellers could have no chance of being stopped in a lonely place, for the plain reason that there was no man there to stop them.\n",
      "In that sort of material, too, London was so rich, that although the best authorities and the best intentions could not prevent one in every twenty houses being a rookery, and five per cent of its whole people being in the midst of poverty, ignorance, and brutalisation from their cradles to their graves, it could afford to spare a yearly contingent of felons that would have manned a respectable Colony. How bad they were, those felonious Fathers of the people? It is very well worth your while, my darling, to look at this matter, in connection with any account you may read of these same Fathers taking to the polls some day in favour of an extension of the elective franchise.\n",
      "Among them were a quantity of petty pilferers who had been found out in stealing petty articles from their employers, and were far more pitiful than culpableso much wiser now in their punishment than in their sin. Among them were a quantity of young girlsnot a few, like the one poor little fluttered prisoner in the Condemned Hold at Newgate last Monday night, as innocent as Dollswho had made away with themselves, or made believe to make away with themselves, for the selfish love of being talked about and kept in gaol. Among them were a quantity of shameless women, who had been convicted on the least creditable charges. Among them were a few (such cases will occur now and then) who were represented by their counsel as having done nothing at all to merit the distinction that had been conferred upon themas having been the victims of false swearing conspiracy, or mistake. Among them were three wretched men from Dorsetshire, not one of whom could read, and two of whom were imbeciles. Their names were George Loveless, John Loveless (his brother), James Brine (brother-in-law\n",
      "llama_print_timings:        load time =    9400.11 ms\n",
      "llama_print_timings:      sample time =      81.82 ms /  1024 runs   (    0.08 ms per token, 12514.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3571.49 ms /   494 tokens (    7.23 ms per token,   138.32 tokens per second)\n",
      "llama_print_timings:        eval time =   68054.61 ms /  1023 runs   (   66.52 ms per token,    15.03 tokens per second)\n",
      "llama_print_timings:       total time =   71863.06 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976252\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.00 MiB\n",
      "llm_load_tensors: mem required  = 35091.00 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35091.00 MiB, (35091.62 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (36371.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (36490.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, are very slow and leisurely workers. Let them alone, and they will examine the very threads of the parasitical clothes that embrace France around her chest and back, count them over as they count the leaves on a summers day, find the place of that thread and pluck it out.\n",
      "The Dauphiness Becomes Marie Antoinette, Queen Of France - Chapter 2 from Carlyle's The French Revolution\n",
      "From A Tale of Two Cities. Book the FirstRecalled to Life; Chapter 1.The Period. By Charles Dickens (1859)\n",
      "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other wayin short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
      "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever.\n",
      "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the carnal ears of men, pregnant with much importance.\n",
      "A wonderful fact to reflect upon, that every human creature is constituted as it were to be a microcosm of the whole world. There is no place on earth so high but may be overlooked from a higher still; nor any place in the lowest deep but may be fathomed from a deeper still. Even in our most secret recesses, we, like Vulcan, are not only surrounded and besieged by the gods or forces that contribute to our existence, but wedded and married to them in a conjugal tie. Mind has mountains; Pride has precipices; cautious cowardice has chasms to fall into; impetuous valor has armies to encounter; and Philip soberly sitting in his parlour has debts to pay and difficulties with the Excise or Inland Revenue when he thinks of nothing else. Do what we may, what we are engaged in is but a fraction of what we are concerned in, our successes and our failures have\n",
      "llama_print_timings:        load time =    7224.42 ms\n",
      "llama_print_timings:      sample time =      83.34 ms /  1024 runs   (    0.08 ms per token, 12287.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3554.90 ms /   494 tokens (    7.20 ms per token,   138.96 tokens per second)\n",
      "llama_print_timings:        eval time =   68074.32 ms /  1023 runs   (   66.54 ms per token,    15.03 tokens per second)\n",
      "llama_print_timings:       total time =   71864.53 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976332\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.00 MiB\n",
      "llm_load_tensors: mem required  = 35091.00 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35091.00 MiB, (35091.62 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (36371.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (36490.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the road agency system was in full career; and mail-coaches were in constant danger on many routes. A certain amount of complacency was to be observed among the Police Functionaries -- never excessive, for they were, every man of them, subsequently done for -- but national vanity was perhaps most strongly expressed by the sheriffs of London and Middlesex, who competed with one another at twelve guineas a-side as to which should show the best sport first; and who made the echoes of Hampstead resound with pistols and shouting until one of them turned up his toes, and the other was brought with some difficulty from a state of very similar temporary inaction.\n",
      "We had completed our ten miles or so for the first stage, which was performed -- according to an ancient regulation -- on hack post-horses, ridden by postilions, when Mr Pickwick emerged from beneath the wrapper in which he had been packed, by his careful valet, and inquired how I felt.\n",
      "'Most particularly well,' said I; 'and I have hardly turned a hair.'\n",
      "'I am delighted to hear it,' said Mr Pickwick.\n",
      "And well might Mr Pickwick be delighted to hear it, or anything else: being one of those truly amiable men in whom a keen appreciation of their fellow-creatures and their little peculiarities is the most prominent trait -- amiability developing itself in a dozen diverting forms when they get together -- all tending toward that great end and object, the embedding on earth (if but in clay) of the name of Pickwick. Say what you will about me (and I think it does me good to be told the truth sometimes), if you knew half as much about me as he knows about every man, woman, and child who falls in his way, and the many more who get into his books -- in short, if you only knew me as well as he might have known me, at this time yesterday morning (I write on Wednesday, August 26th) -- you would never trust my judgment in any matter, on any consideration. I doubt, myself, whether I am fit to be entrusted with the task I have undertaken.\n",
      "My present difficulty is occasioned by Mr Pickwick's unwarrantable assumption of the garb of old age and infirmity -- a course of conduct which must certainly lay him open to the heaviest strictures on the part of his biographer, but which will be quite enough accounted for, if I show (as in my duty bound I will) how shining traits of youthfulness were constantly breaking out afresh in Mr Pickwick.\n",
      "To this unsettled state of mind we must attribute the weakness of purpose into which he fell on our reaching a convenient inn within ten miles of Eatanswill, where he proposed to halt for refreshment. He was worn out by the fatigues of the\n",
      "llama_print_timings:        load time =    7282.48 ms\n",
      "llama_print_timings:      sample time =      81.96 ms /  1024 runs   (    0.08 ms per token, 12493.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3557.91 ms /   494 tokens (    7.20 ms per token,   138.85 tokens per second)\n",
      "llama_print_timings:        eval time =   68063.29 ms /  1023 runs   (   66.53 ms per token,    15.03 tokens per second)\n",
      "llama_print_timings:       total time =   71856.48 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce3e8",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5122b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976412\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.30 MiB\n",
      "llm_load_tensors: mem required  = 124525.30 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   500.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MiB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.31 MiB, offs = 115439812608, (125025.94 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (126306.00 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (126425.02 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of the Captain, gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, in consequence of the failure of his ammunition: after which the mail was robbed in peace. Banking-houses were invaded and plundered; wells were poisoned by malevolent persons who brooded over family quarrels; individuals built Gallows in their back gardens, and led forth captives handcuffed, to hang them up at pleasure on their own account. The press groaned with accounts of atrocious crimes, contrasted with some touching and occasional cases of the exercise of almost superhuman virtue: such as that of the woman who had walked four hundred miles in her pattens to see the King; of another woman, found frozen on her door-step, who had given birth to twins an hour after death; of the parish clerk and sexton of St. Andrews Holborn, who, under pretence of selling his wife, obtained fifty guineas from bachelors who wanted a wife; and thereupon hanged himself on the same gallows he had erected in his garden to hang other people with.\n",
      "Notwithstanding the enormous extent of these unthinkable horrors, however, it is pleasant to know that Parliament found time to attend to other matters too: such as a petition praying for mercy toward the poor insolvent debtors imprisoned in the Kings Bench (who had increased from one hundred and twenty-three in 1782 to eight hundred in 1795); a bill for the better preservation of the lives of journeymen bakers; a petition against stagecoaches, as an intolerable nuisance; and another for the discharge of John Robinson from Newgate Prison, on his humble petition setting forth that he had been confined two years without being able to prove himself guilty. It is gratifying also to know that Parliament amused itself with some jokes, at intervals; as when a certain Mr. Wilkes said (and said in Parliament), in reference to the Prince of Wales and Mrs. Fitzherbert: I believe the prince has acted conscientiously. I believe he married her because he was afraid she would be damned if they did not.\n",
      "At this time, the Bank of England was so exceedingly careful, that it would not even issue notes for small amounts: though I think I must have seen a twenty-shilling note in my childhood; at any rate I am pretty sure of having seen one. The following extract from\n",
      "llama_print_timings:        load time =   64531.34 ms\n",
      "llama_print_timings:      sample time =      93.04 ms /  1024 runs   (    0.09 ms per token, 11006.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3139.49 ms /   494 tokens (    6.36 ms per token,   157.35 tokens per second)\n",
      "llama_print_timings:        eval time =  204490.36 ms /  1023 runs   (  199.89 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =  207893.94 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86588417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976686\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.30 MiB\n",
      "llm_load_tensors: mem required  = 124525.30 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   500.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MiB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.31 MiB, offs = 115439812608, (125025.94 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (126306.00 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (126425.02 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers warehouses for security; the road-agents, who plundered all passenger-coaches in the neighbourhood of London, took their watches and wallets so much as a matter of course, that they were satisfied if passengers offered no resistance in delivering them up, and even returned thanks to frightened ladies and gentlemen under such circumstances. So little power had the law, that these depredators could rove about masked, head-quarters, and baffle pursuit repeatedly; nay, that a noted malefactor ascertained by description to lodge in St Georges Fields, could not be taken without great difficulty, even when his house was known. A case occurred at Manchester where an unfortunate heiress who had real property came to her own, and was found strangled through the bulls-eye of an otherwise fastened bedroom window, through which handsome housebreakers for plunder, handsome assassins for revenge, had got in and out to kill her. The town of Birmingham was in insurrection, and the manufacturing towns of the North of England were in a perpetual alarm that riots would break out among them; which highly probable event could hardly have been prevented, or even suppressed, by any reasonable amount of military strength. Yet at the time when Mr Pecksniff and his charming daughters, accompanied by a pupil, went so far towards London on their errand of mercy and affection, all these things had either happened in the same week, or were immediately impending.\n",
      "The coach was empty when they entered it; for though the distance was very short (the vehicle was not supposed to go more than four miles an hour at that time), it had long been known as THE slow Tantrum. But they soon began to be enlivened by meeting people who were walking towards Salisbury, and stopping coaches on their way.\n",
      "'Where you from?' said the first passenger who got up. A stout old lady in a large bonnet, with two flat baskets at her feet, one of which was carefully covered with whitey-brown paper. 'Liverpool.' 'Going there?' 'N  no!' The last word appeared to be an afterthought on the part of Mr Pecksniff.\n",
      "'Oh, indeed; where then?' inquired the old lady.\n",
      "The gentleman looked at Mr Pecksniff, and smiled; not a very genteel smile though. He was an elderly man, bald-headed, with thick bushy eyebrows, and a large double chin. His clothes were of good make and material but ill assorted, and rather slovenly. He wore his hat on one side, had rather a high nose, and rather a wide mouth, and he carried a stick.\n",
      "'What place is this we have just passed? Chiselbury?' asked Mr Pecksniff, of the coachman. 'It was a good stage last\n",
      "llama_print_timings:        load time =   52900.81 ms\n",
      "llama_print_timings:      sample time =      79.37 ms /  1024 runs   (    0.08 ms per token, 12901.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3137.76 ms /   494 tokens (    6.35 ms per token,   157.44 tokens per second)\n",
      "llama_print_timings:        eval time =  204492.73 ms /  1023 runs   (  199.90 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =  207865.37 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e1e0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1701976948\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.30 MiB\n",
      "llm_load_tensors: mem required  = 124525.30 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   500.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MiB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.31 MiB, offs = 115439812608, (125025.94 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (126306.00 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (126425.02 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of the Captain, gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, in consequence of the failure of his ammunition: after which the mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in London gaols fought battles with their turnkeys, and the majesty of the law fired blunderbusses in among them, loaded with rounds of shot and ball; thieves snipped off diamond crosses from the necks of noble lords at Court drawing-rooms; musketeers went into St. Giless, to search for contraband goods, and the mob fired on the musketeers, and the musketeers fired on the mob, and nobody thought any of these occurrences much out of the common way. In the midst of them, the hangman, ever busy and ever worse than useless, was in constant requisition; now, stringing up long rows of house-breakers; now, burning people in the hand at Newgate by the dozen, and now, hanging fourteen-year-old boys before scores of thousands of spectators. Often, the lads he hanged were only nominal criminals, and had been put up to petty theft by designing people. But what was that to him? If the law said kill, he killed: if it said spare, he spared; he knew no more about it than his dog.\n",
      "In this reign there was a fashion of prosecuting witches and sorceresses in different parts of England, which, next to a general plague, was perhaps as afflictive as any calamity that could have befallen the nation. The judges made considerable profits out of these trials; hence there seldom wanted accusers, and it is not surprising that this hunt after witches, pursued with all the ardour that popular prejudice and ignorance (not without their tool the law) are capable of exciting, should have been carried to such excess as to destroy even many innocent persons who had no commerce whatever with the Devil, either by invocation or contract.\n",
      "It must be acknowledged that some disgraceful frauds were practised by witches; but the test applied to their detection was quite as bad. The suspected witch was thrown into a pond: if she sunk and was drowned, her innocence was proved; if she\n",
      "llama_print_timings:        load time =   50022.83 ms\n",
      "llama_print_timings:      sample time =      81.87 ms /  1024 runs   (    0.08 ms per token, 12507.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3129.86 ms /   494 tokens (    6.34 ms per token,   157.83 tokens per second)\n",
      "llama_print_timings:        eval time =  203752.53 ms /  1023 runs   (  199.17 ms per token,     5.02 tokens per second)\n",
      "llama_print_timings:       total time =  207101.88 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959fbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
