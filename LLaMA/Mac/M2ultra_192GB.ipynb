{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9583f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jack\n",
      "/Users/jack/llama.cpp\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f651103-a6c8-4b7d-9f0f-be0553b22acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[30m\u001b[43m13B\u001b[m\u001b[m                              ggml-vocab-falcon.gguf\n",
      "\u001b[1m\u001b[36m13B-v2\u001b[m\u001b[m                           ggml-vocab-gpt-neox.gguf\n",
      "\u001b[30m\u001b[43m30B\u001b[m\u001b[m                              ggml-vocab-llama.gguf\n",
      "\u001b[30m\u001b[43m65B\u001b[m\u001b[m                              ggml-vocab-mpt.gguf\n",
      "\u001b[1m\u001b[36m70B-v2\u001b[m\u001b[m                           ggml-vocab-refact.gguf\n",
      "\u001b[30m\u001b[43m7B\u001b[m\u001b[m                               ggml-vocab-stablelm-3b-4e1t.gguf\n",
      "\u001b[1m\u001b[36m7B-v2\u001b[m\u001b[m                            ggml-vocab-starcoder.gguf\n",
      "ggml-vocab-aquila.gguf           \u001b[31mtokenizer.model\u001b[m\u001b[m\n",
      "ggml-vocab-baichuan.gguf         \u001b[31mtokenizer_checklist.chk\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# obtain the original LLaMA model weights and place them in ./models\n",
    "!ls ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6699f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the GGUF read/write example\n",
    "# !make gguf\n",
    "\n",
    "# # write a dummy GGUF model to test.gguf\n",
    "# !./gguf test.gguf w\n",
    "\n",
    "# # read the dummy GGUF model\n",
    "# !./gguf test.gguf r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca0e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install Python dependencies\n",
    "# !python3 -m pip install -r requirements.txt\n",
    "\n",
    "# # convert the model to ggml FP16 format (edit your params.json file if the \"vocab_size\" mismatch)\n",
    "# !python3 convert.py models/7B/\n",
    "# !python3 convert.py models/13B/\n",
    "# !python3 convert.py models/30B/\n",
    "# !python3 convert.py models/65B/\n",
    "\n",
    "# # quantize the model to 4-bits (using q4_0 method)\n",
    "# !./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/13B/ggml-model-f16.gguf ./models/13B/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/30B/ggml-model-f16.gguf ./models/30B/ggml-model-q4_0.gguf q4_0\n",
    "# !./quantize ./models/65B/ggml-model-f16.gguf ./models/65B/ggml-model-q4_0.gguf q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8184a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Darwin\n",
      "I UNAME_P:   arm\n",
      "I UNAME_M:   arm64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \n",
      "I NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \"\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "I CC:        Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "I CXX:       Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope\n",
      "build-info.o\n",
      "common.o\n",
      "console.o\n",
      "ggml-alloc.o\n",
      "ggml-backend.o\n",
      "ggml-metal.o\n",
      "ggml-quants.o\n",
      "ggml.o\n",
      "grammar-parser.o\n",
      "llama.o\n",
      "sampling.o\n",
      "train.o\n",
      "tests/test-c.o\n",
      "benchmark-matmult\n",
      "common/build-info.cpp\n",
      "main\n",
      "quantize\n",
      "quantize-stats\n",
      "perplexity\n",
      "embedding\n",
      "vdot\n",
      "q8dot\n",
      "train-text-from-scratch\n",
      "convert-llama2c-to-ggml\n",
      "simple\n",
      "batched\n",
      "batched-bench\n",
      "save-load-state\n",
      "server\n",
      "gguf\n",
      "llama-bench\n",
      "libllava.a\n",
      "llava-cli\n",
      "baby-llama\n",
      "beam-search\n",
      "speculative\n",
      "infill\n",
      "tokenize\n",
      "parallel\n",
      "finetune\n",
      "export-lora\n",
      "lookahead\n",
      "metal\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Darwin\n",
      "I UNAME_P:   arm\n",
      "I UNAME_M:   arm64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \n",
      "I NVCCFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread    -Wno-pedantic -Xcompiler \"-Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi \"\n",
      "I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "I CC:        Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "I CXX:       Apple clang version 15.0.0 (clang-1500.0.40.1)\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread    -c ggml.c -o ggml.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c llama.cpp -o llama.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/common.cpp -o common.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/sampling.cpp -o sampling.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/console.cpp -o console.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread  -c ggml-metal.m -o ggml-metal.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread     -c ggml-quants.c -o ggml-quants.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/train.cpp -o train.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion -pthread  -c tests/test-c.c -o tests/test-c.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -c common/build-info.cpp -o build-info.o\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  pocs/vdot/vdot.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  pocs/vdot/q8dot.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/export-lora/export-lora.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/metal/metal.cpp ggml.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o metal -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -framework Accelerate -framework Foundation -framework Metal -framework MetalKit   -Wno-cast-qual\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/gguf/gguf.cpp ggml.o llama.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -framework Accelerate -framework Foundation -framework Metal -framework MetalKit  -Wno-cast-qual\n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "c++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DHAVE_BUGGY_APPLE_LINKER -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_METAL  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi  examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -framework Accelerate -framework Foundation -framework Metal -framework MetalKit \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metal build\n",
    "!make clean && LLAMA_METAL=1 make -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21608",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6aee37",
   "metadata": {},
   "source": [
    "### 7B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c68bb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273276\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.97 MiB\n",
      "llm_load_tensors: mem required  = 3647.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.98 MiB, ( 3648.61 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 3904.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 3975.16 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m An official body, called the Financiers of France, sat just below Paris, and decided what that capital should spend in a day; afterwards they settled how the spent part should be paid for. The whole nation seemed to be employed as clerks in copying out this decision. \n",
      " Notwithstanding the above general character of French affairs, however, it is recorded of them, by themselves, that the year 1792 was one of terror and agitation; that there were bloody scenes at Versailles; that the King (Louis XVI) was threatened with death; and that he was dethroned and executed in 1793. There is reason to believe, from the French themselves, that all these things were very much exaggerated in the newspapers of the day; but there are no newspapers extant to confirm this allegation.\n",
      "CHAPTER I  \n",
      " The Shadow\n",
      "Table of Contents\n",
      "A young gentleman  his name was Lionel Dobbs  lay at full length on the ground, looking up into a dark, starless sky. It was very late in the evening: he had been lying where he was for nearly half-an-hour, and was still going to lie there.\n",
      "'You may go home now, Miss Wirt,' said Lionel Dobbs. 'I have settled my business.'\n",
      "Miss Wirt stood with her back against a tree, looking down on him with an air of gentle commiseration: the starlight seemed to soften and refine everything  even Miss Wirt's face. The effect it produced upon the young man himself was very different; and as soon as he had seen how it affected his attendant, he rose from the ground and said 'No, you don't!' in a tone which would have been audacious in him if he had not known her character  but now he only felt mean.\n",
      "The young gentleman was walking away when Miss Wirt stopped him. 'Oh, Mr Lionel!' she said. He stopped.\n",
      "'Mr Lionel,' she continued, 'I _am_ sorry for you.'\n",
      "She went on without waiting for an answer: 'I am afraid that what I have seen and heard is quite enough to make me wish not to have seen it all at last; but really it is too dreadful. And yet you are so brave!'\n",
      "'Mrs Lilly would think she had been very ill-judged if she knew of my being here,' said Lionel. 'I am a coward, Miss W., I am afraid.'\n",
      "'You are no more a coward than I am,' said Miss Wiltshire; 'and yet you are not such a coward as I am.'\n",
      "It was so odd that he had never seen it before in his life! It really was a new thing to him: he could hardly believe in it. She looked up at the sky again, and sighed. Lionel said something which she did not hear; and then, after thinking it over for some moments, he spoke again. 'Miss W.,' he asked, 'what would be your advice  if you _could_ give me any?'\n",
      "Miss Wiltshire considered. She could have given him her advice in a moment: she had never wavered as to what she ought to do; but now it was more difficult to express the idea in words. The young gentleman, noticing that he did not get an answer at once, repeated his question with emphasis. 'Miss W.,' he said, 'what would be your advice?'\n",
      "She sighed again: she had been trying hard to frame her words for some minutes past  but there was so much that she _could_ say! She could have told him to go home and cry as a child or a baby; or, better still, she might have reminded him of what his mother would be thinking at this moment, and how she must be suffering. But this was not the time for saying such things: he had spoken very well just now (though he did seem rather to mistrust himself), and it was best to encourage him to speak more like that in future. So Miss Wiltshire replied that if he had no advice of his own, she could give him none  except to try and bear the trial as long as possible with a firm mind.\n",
      "'I will try,' said Lionel; 'and now, pray, tell me all about it.' He sat down on one of the benches; Miss Wiltshire took another. They were close to the water-side; so that when they looked across the river at the lights twinkling in the distance beyond Putney bridge, they could fancy themselves nearing home.\n",
      "'There is nothing to tell you,' replied Miss W., 'that I do not feel myself.' And she proceeded at once to describe her feelings  which were just the same as those he\n",
      "llama_print_timings:        load time =     688.86 ms\n",
      "llama_print_timings:      sample time =      83.45 ms /  1024 runs   (    0.08 ms per token, 12271.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.60 ms /   494 tokens (    0.83 ms per token,  1211.99 tokens per second)\n",
      "llama_print_timings:        eval time =   11074.35 ms /  1023 runs   (   10.83 ms per token,    92.38 tokens per second)\n",
      "llama_print_timings:       total time =   11703.60 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2325fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273289\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.97 MiB\n",
      "llm_load_tensors: mem required  = 3647.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.98 MiB, ( 3648.61 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 3904.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 3975.16 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was flung into the opposite extreme by the spirit of sordid gain; not that either people were wholly destitute of public virtue: only as both races are addicted to superstition ( England in her tee-totums, France in hers), it is but natural they should be equally impressed with the efficacy of charms and remedies. \n",
      " For example, the British Household of George the Third were not a little exercised on the score of the late Kings health: which, however, though shaken at one time by the French Revolution, had now sufficiently recovered to enable His Majesty to walk in his closet (which he did with great caution). On the other hand, the health of the late King of France was flourishing; as appeared by a message just then received from him, requesting a loan. \n",
      " This being granted on terms very convenient for both parties, it might have gone on to any length: only that an accidental circumstance occurred which interrupted and diverted the negotiation, namely: the Kings sudden death. His brother, however, continued in office, until another king was chosen by the French nation; at which period, being now relieved from a royal debtor, England resumed her claim. \n",
      " At this instant the two nations were on the eve of war, and a fleet preparing for the purpose, under the command of Admiral Rodney: when suddenly an event occurred so unaccountable (considering the distance and hazard) as to check the ardour of both parties. \n",
      " I allude to that extraordinary phenomenon which at this moment may be observed in every part of the British dominions; namely, that of a dead man rising from his grave, and coming upon deck: for this is what has happened now to our friend Captain Aubrey (and indeed to almost all the officers) when on the quarterdeck of the Venerable (a fine new ship mounting ninety-eight carriage guns): but for the particulars, as it were a resurrection scene, you must refer to his journal. \n",
      " CHAPTER IX \n",
      "Aubrey was right in supposing that this would have been an unpleasant thing for a captain on deck of the Venerable. Even though they had only one officer present on deck at the moment  Captain Harvey  it was not so very pleasant: certainly not for the ships company.\n",
      "To see their beloved captain suddenly springing up, as if from nowhere, to confront them in that position, which was the place of their natural confidence; and, having done this, to sink back again into his chair as he had risen from it  as though no more was necessary to be done, nothing further wanted. To see him do these things day after day would have been disquieting enough. But that he should also, in addition, not only be constantly there but actually join them for some of their watches was a thing which would surely have driven them mad; and it was this which they all thought to be so dreadfully odd  the strange behaviour of him whom they had been taught from childhood to consider as so important.\n",
      "In short, the captains presence on deck (and he did this most days) was quite inadmissible: though the reason why it became necessary for him to do this is one which should certainly be considered with due care; and that is  that the officers must have had their duties to attend to.\n",
      "So it has been proved, that this appearance of the captains from his chair to join them on deck was a thing most inadmissible. But if there had been no other reason for it than this (which, of course, was not so), still the officers would have felt it very necessary to consider this; and the sooner they had done so, the better: which they did, by thinking it over every day on deck when they were at quarters. They concluded that his wishing to be with them might arise from some anxiety he might have about something or other  though what this thing could be they could not imagine; since nothing had happened during their cruise which could give rise to any anxiety whatever.\n",
      "On the third day, therefore (as I said) Captain Aubrey, on his way downstairs with a very heavy heart and an empty stomach, came face to face with one of those persons in whom his mind took some comfort; for he was obliged to come upon this encounter as it might be called: which, indeed, would have been the case even if he had gone out by himself and found someone else there. The person that presented herself at his side, however, was the very person he wanted most to see  Dr Maturin.\n",
      "Oh, said she, I didnt know it was you; I thought\n",
      "llama_print_timings:        load time =     473.97 ms\n",
      "llama_print_timings:      sample time =      73.05 ms /  1024 runs   (    0.07 ms per token, 14018.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.37 ms /   494 tokens (    0.82 ms per token,  1212.66 tokens per second)\n",
      "llama_print_timings:        eval time =   11075.58 ms /  1023 runs   (   10.83 ms per token,    92.37 tokens per second)\n",
      "llama_print_timings:       total time =   11660.44 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae626ed6-adc9-429b-9f04-c1fb2cfd8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273301\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from ./models/7B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 3647.97 MiB\n",
      "llm_load_tensors: mem required  = 3647.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3647.98 MiB, ( 3648.61 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 3904.64 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 3975.16 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m An army, which had done the queen and king, and their consorts respectively, the honour of fighting four or five battles successively in their behalf, was allowed to perish by starvation in Bretagne; whereupon the three parties unitedly made war upon each other for a few months. \n",
      " The French revolution began: whereupon, a little later in the year, the allied sovereigns of Britain and Ireland, after having conquered the French, allowed them to have another turn with their old friends. At length, peace being concluded (or made) by both sides, an English army, consisting entirely of the flower of the land, set off from Dover to join their brethren in Holland. \n",
      " These events were connected in a manner which it is far too early to disclose; but the reader may safely take upon him to suppose that they were not wholly fortuitous. In the meantime, the French Revolution, beginning as usual with one head (Louis XVI.), ended as usual with two: the other belonging to Madame Roland.\n",
      "CHAPTER THE SECOND\n",
      " A WOMAN'S REVOLUTION\n",
      "The revolutions which take place in the private opinions of Englishmen are conducted on so limited a scale that they do not come into this category; and yet we have witnessed one since our last meeting  in which a large portion of the community was engaged, and which may be called a woman's revolution.\n",
      "What is a revolution? It is an overthrow of government  either of some particular form of it, or of the whole:  but with this difference, that whereas a rebellion supplants an existing authority by another, and so maintains the existence of power in general; a revolution sweeps away all powers whatever.\n",
      "The French revolution was carried on under the name and form of the _National Assembly_. Now, what is a National Assembly? It is a body consisting of representatives of various parts of society chosen to consider, discuss, and decide upon the state of affairs which requires their attention. These men are called in general the Legislative Body  that is to say, the power which makes laws: but as there was another Legislature before them; it would be an insult to the English Parliament to compare any one part of our Constitution with the National Assembly of France. The French Constitution is called a Republican Form of Government; which means that there is no king or queen at all, and that every body votes according to their own judgment and will: whereas we have a limited monarchy, under which we are governed by kings and queens who take the opinion of some men whom they choose to be their ministers  in our case the House of Commons; but with this difference, that the French have a Legislative Body chosen out of all their country.\n",
      "Now what do you say? Why I should think that there was nothing like it on earth, unless it is one of your revolutionary meetings at Manchester! But if you will look back into history, you will find that something very like our own system existed in Rome before the time of the Republic  when the people voted themselves laws. And yet though we have a House of Commons which was created by a king and made law-givers (whole armies obeyed its decrees), they are not at all called a Legislative Body; and yet that body is chosen out of _every_ part of society! Now I ask, what can be more like a Republican Assembly than the House of Commons?  and if it is only the name which makes so great a difference to us, why does our Constitution not bear the same title as the French one? Why should we call them 'Lords,' when they are in fact our legislators?\n",
      "You will have observed that I say nothing about Peers: for though they were once Lords, they are now very different creatures from the lords of the feudal times  so much so that I think it a great shame to speak of them by that title; and that even if they had a right to sit in Parliament as peers, yet their right to be called such would not be more justifiable than the old nobility's. For there is no difference between a man who has inherited a peerage  an earl for instance  from his father or grandfather; and the earls of the feudal times were called 'Lords' because they _owned_ lands which gave them power to govern other men like slaves, as well as landowners do now. But these Peers are neither lords of people nor owners of lands  for we have got rid of the one: and as to the lands, though they are a great deal richer than anybody else, still their fortunes depend on shares, or money matters; so that they have no property in the land like the landowning nobility of former times\n",
      "llama_print_timings:        load time =     484.18 ms\n",
      "llama_print_timings:      sample time =      72.23 ms /  1024 runs   (    0.07 ms per token, 14176.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     407.72 ms /   494 tokens (    0.83 ms per token,  1211.63 tokens per second)\n",
      "llama_print_timings:        eval time =   11092.99 ms /  1023 runs   (   10.84 ms per token,    92.22 tokens per second)\n",
      "llama_print_timings:       total time =   11680.04 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07d6cd",
   "metadata": {},
   "source": [
    "### 7B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273313\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.12 MiB\n",
      "llm_load_tensors: mem required  = 12853.12 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.12 MiB, (12853.75 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, (13109.78 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, (13180.30 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, who had done the same in other days, stood more cautiously upon the top of the hill, eyeing carefully the rapidity and manner of departure of her neighbours, as if to avoid being left behind in the general whirl. France was carrying everything before her, even the warlike passions of England: with which, in truth, a very natural sympathy existed. The French were carrying all their wars before them too; for though they made great show at home and abroad of being always getting beat, they never lost any real battles  as become a nation of brave men. \n",
      " Notwithstanding the universal persuasion that our armies are full of heroes, and our navies of demi-gods, and our people the best in the world  which cheering faith is the groundwork of English prosperity, peace, contentment, and happiness, we have been getting beat all the time; for twenty years back. And yet our armies are full of heroes, our navies of demi- gods, and our people the best in the world. \n",
      " At this auspicious moment a certain large tract of country between England and France was invaded by an army from both those nations together: who appeared before each other on opposite banks of a river called the Rhine  that being no doubt as good a river for attacking as any other, since there is scarcely such another. \n",
      " A number of battles ensued; in which many valiant things were done on both sides: particularly by an officer on ours who was known to all the army by his having been left behind at Boulogne last year  because of a very extraordinary pimple which had grown over one eye since he left England. This gentleman (who was as gallant and undaunted a soldier as any in our service) charged repeatedly with great resolution and success: always fighting near a cannon that was wheeled out to the front by some gunners belonging to his own corps. He behaved so heroically at the battle of A-H-N, for instance, that a number of officers from foreign nations  who were on the spot during the engagement  complimented him as one of the best officers they had ever seen in action. And his regiment, on their return to England, voted him two thousand guineas; which, however, was never paid him, because it appeared that he could not prove himself to be an Englishman: so he went into opposition and wrote pamphlets about the subject  but with little or no effect; though he was elected a Member of Parliament for Mudfog in consequence. \n",
      " Another battle took place at P-S-P-R; and, as our army marched forward towards Paris (which they did as fast as they could) several other battles were fought at N-O-N-N-, S-S-L-, B-U-D-S-Y-D-, V-V-V-W-, L-U-T-E-M-, E-C-H-E-R-, C-E-F-E-L-, T-T-O-T-A-, M-I-X-M-, S-M-B-, G-N-G-N-, and J-V-J-. At all these places the French were conquered; and it is only justice to say that our officers, in general, behaved with great gallantry throughout the campaign. \n",
      " The army advanced to Paris, which they reached on St Crispin's Day. The first night after their arrival was passed very quietly: but on the following morning  which chanced to be Friday the thirteenth of November  a terrible accident happened. A regiment had been left at C-E-F-E-L-, to prevent any of the French soldiers from escaping in that direction; and the officers, finding some of them absent when they marched off in the morning, thought they might have gone back to get their wives and sweethearts (who happened to be living there). A search was therefore made: but all in vain. \n",
      " It is believed that one regiment went back for its sweetheart on Friday the thirteenth; another on Saturday; and a third on Sunday. Whether these regiments, or any others, had sweethearts at C-E-F-E-L-, we do not know: but, as you will understand, when we say that the French women were generally very handsome, we merely mean that they were, in comparison with Englishwomen. \n",
      " The regiment left at C-E-F-E-L- did not come back on Friday evening; and a search was accordingly made for it, during which several of them (the soldiers) were discovered. They had marched off, to go after their sweethearts  who were Frenchwomen, or half\n",
      "llama_print_timings:        load time =    2143.30 ms\n",
      "llama_print_timings:      sample time =      73.48 ms /  1024 runs   (    0.07 ms per token, 13935.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     358.28 ms /   494 tokens (    0.73 ms per token,  1378.83 tokens per second)\n",
      "llama_print_timings:        eval time =   24844.69 ms /  1023 runs   (   24.29 ms per token,    41.18 tokens per second)\n",
      "llama_print_timings:       total time =   25385.30 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaf89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273341\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.12 MiB\n",
      "llm_load_tensors: mem required  = 12853.12 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.12 MiB, (12853.75 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, (13109.78 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, (13180.30 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England was rotten to the core: but she had only to put herself to rights by borrowing a hundred millions from France; which sum being lent, England was obliged in return for such temporary aid, to acknowledge the French Republic as an independent power. \n",
      " But though France should go on rolling smoothly downhill, with her paper money circulating and spending under the protection of her army; still there are certain indications, at this moment, that the wheels have begun to come off her machine. The papers of Paris have lately been full of a very strange story, about a great personage there who has been made a French citizen. His name is John Adams. \n",
      " That he had rendered some service to his adopted country, is certain: for it was owing to him that the National Assembly (though with very reluctance) agreed last year to send an embassy and mission to America; which now by this event has been made up of three gentlementwo of whom are of distinguished character. \n",
      " This John Adams is a man who does not much affect a court, but retires into the country, when his services are dispensed with in public business. His house at Quincy, in New England, was lately visited by another personage who had served in an official manner under government; whose name I may mention as soon as it has been made public. He is a man of fine abilities and character; but for some cause or other not so much liked on the whole continent as himself seems to be aware he ought to be; at least, as appears from his conducting affairs in such a way that many people who do like him are disposed to suspect he is acting under some secret motive of ambition. \n",
      " The other personage whose name I have mentioned as visiting John Adams, has also been distinguished for official services: but these were more recently given, and perhaps less useful; yet it appears he still keeps up the character he bore in them. This is Thomas Jeffersonanother gentleman who retires into the country, when his services are dispensed with publicly; being at present not far from Monticello, where he resides. \n",
      " John Adams says that Thomas Jefferson has taken great pains to collect a good library at Monticello; and has collected a very great number of books, as it is believed, on the subject of America. That collection seems now in some degree disgusted with him: for I learn from sources which I can scarcely doubt to be true, that he has lately sent several hundred of those volumes into France. I think therefore that there must have been an understanding between John Adams and Thomas Jefferson beforehand about sending books away. Whether the latter was willing to act as the carrier in order that the former might carry on a little correspondence with people in Europe, or whether he was instructed by others to do it at the desire of John Adams himself, is uncertain: but it seems not improbable that both may have been true. \n",
      " In his way Thomas Jefferson stopped a short time with Patrick Henry, where the former had the advantage over him of hearing more than once from the latter's own lips, something of what the people at large said of America, in which I can believe more than others do. It is certain that he must have heard many things from Mr Henry, as it appears by a letter written to him while on his journey. But what the people said generally to Thomas Jefferson at Richmond about America, was not only much less to his own taste, but was expressed with very little reserve; so that it does not appear how he could ever have written or said afterwards any thing respecting American affairs, which would be credited by people in Europe who knew nothing of the country. \n",
      " These are the two principal persons whom I know to be connected with the political history of America, whose characters will be described at full length after having given some account of my own private intercourse with them; a few words may however now suffice as preliminary.\n",
      "I think it is not improbable that when Thomas Jefferson was in London, he might have had some acquaintance also there with one of the members of this Society, whose name I remember to be Benjamin Franklin, at least so far as to say, that from the time that the Society was formed, they met several times at the rooms of the principal members; but I know nothing more. This is all which I have learnt about them from others: it seems however probable to me, that having heard something from each other in America respecting American affairs, they might thereafter sometimes mention those matters to us here. In that case, if one or both of them had any opinion different from our own, we should not at this time have known anything about it: but if such a conversation as I am describing above happened to take place between the two persons when in London; then one of these two men must be supposed\n",
      "llama_print_timings:        load time =    1405.56 ms\n",
      "llama_print_timings:      sample time =      74.03 ms /  1024 runs   (    0.07 ms per token, 13831.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     358.67 ms /   494 tokens (    0.73 ms per token,  1377.31 tokens per second)\n",
      "llama_print_timings:        eval time =   24736.26 ms /  1023 runs   (   24.18 ms per token,    41.36 tokens per second)\n",
      "llama_print_timings:       total time =   25276.01 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a3b477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273368\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 291 tensors from ./models/7B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 12853.12 MiB\n",
      "llm_load_tensors: mem required  = 12853.12 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.57 MiB\n",
      "llama_new_context_with_model: max tensor size =   250.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 12853.12 MiB, (12853.75 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, (13109.78 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, (13180.30 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m England, after various zigzags in her course, followed suit; the same thing was attempted of a sudden by America; but she broke the trail again, by the unexpected result of her last experiment. In fact, the old world seems now to be flying into one consolidated whirlpool of paper, misery, and revolution. The French have their national bank: we shall soon have ours. \n",
      " Chapter I \n",
      " _Paris. A man dressed in black opens the door of a house at midnight. Another man with his face wrapped in a handkerchief stands waiting for him in the dark passage_.\n",
      "'So it is true, then?' asks this second man.\n",
      "'You may take my word for that.'\n",
      "'Are you quite sure?'\n",
      "'As sure as if I had seen them with my own eyes.'\n",
      "'Then where are they? Are they gone already?'\n",
      "'Not yet, but they will be in half an hour at the latest; and then, I am sorry to say, we must let go of each other.'\n",
      "'You mean you know that? That is very bad news. Why did you not tell me this before?'\n",
      "'It was because it would have been impossible for me to see them clearly if I had known. Now the time has come; and there will be no mistake. Do you understand?'\n",
      "The first man took a few steps down the dark passage, and then stopped. A few moments elapsed. The second man went on a step or two himself, but as soon as he felt his companion was not following him he halted again. This passed twice; at last both of them stood still together. It was almost noiselessly that they had done these things, and with the most complete self-command: the first man was a woman, dressed entirely in black; her hair was concealed under a plain close-fitting hood which shaded her face from sight, but revealed all the delicacy of the features beneath. The second man wore the same sort of loose mantle that is worn by foreigners about Paris: but it did not hide his features: he had removed his handkerchief, and showed a white face as calmly beautiful in its mournfulness as was hers that of a young saint; for if she were not more than thirty years old, her beauty seemed almost the greater because so much suffering had stamped its own impressure upon it.\n",
      "'You are sure?' asked the man who was standing near to him; his voice shook just enough to betray a certain agitation. 'Are you quite certain  as positive as that you cannot be mistaken?'\n",
      "She gave him an almost imperceptible sign of her head, in reply to which he went on:  'Yes, yes! I know what you mean!' He paused again for another minute; then putting one arm round the young woman's waist with the most delicate respectful tenderness that had not in it the smallest suggestion of passionate feeling, and speaking very low, he said to her: 'I can never tell you what this last interview has cost me  or how gladly I would have been spared even the necessity for meeting you here.'\n",
      "The woman made no answer but a gentle inclination of the head. He went on:  'You must have observed that I followed you; and what I saw at your father's house determined my plan, so far as I was able to adopt one. You must allow me now to put it into execution without further delay.'\n",
      "The young woman did not make any reply except a low sad sigh. After another pause of a few seconds the man went on again: 'I have said what I came here for  and I will say no more! At all events, at present we may not speak together, or to each other, without risking exposure  and it would be fatal! You understand?'\n",
      "Again she bowed her head in silence. The man proceeded in the same low tone: 'My time is short, but I have one word more to say to you before we part. You may take me for a very foolish or selfish person; but you must believe that my sole object in coming here was to obtain some tidings of your brother  of him whom I love and reverence above everything on earth! It was from no other motive, as you are well aware, that I have been acting all this time. Do not judge me too harshly if you hear ill of him.'\n",
      "The young woman gave a sigh again, but still made no reply; the man went on: 'I ask your pardon for troubling you so much with my visits and my letters! But I must be in haste  we have already stayed long enough!' The young woman rose. 'No,' he said  'you are not to go yet! It is very necessary that we should\n",
      "llama_print_timings:        load time =    1411.29 ms\n",
      "llama_print_timings:      sample time =      73.58 ms /  1024 runs   (    0.07 ms per token, 13917.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     357.88 ms /   494 tokens (    0.72 ms per token,  1380.34 tokens per second)\n",
      "llama_print_timings:        eval time =   24706.38 ms /  1023 runs   (   24.15 ms per token,    41.41 tokens per second)\n",
      "llama_print_timings:       total time =   25244.05 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/7B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84502",
   "metadata": {},
   "source": [
    "### 13B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43038979-9395-4119-94c8-5a8b90198064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273395\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: mem required  = 7024.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.03 MiB, ( 7024.66 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, ( 7424.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, ( 7499.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for life, who had never killed any one, nor stolen anything, nor known how to write, but could read the Lives of the Saints. A columnist in The Times newspaper, which is the only paper published by common consent in that country and which does not avow its own authority, haughtily denies the whole charges of the White-Book (that is, the report presented to parliament by Mr. Canning) in regard to French ferocity. This same columnist has asserted that no less than four hundred thousand men have been kidnapped for ever from their homes; and that the total amount of property destroyed and wasted, whether by fire or rapine, has been five hundred millions sterling: which is a very wide difference in his favour. Mr. OConnells Repealers (notwithstanding their dislike to France) have a monster meeting on St. Patricks day; the object of which seems to be to make the repeal of the Union outrageous by the Union Jack and other patriotic decorations, as if it had been annexed but yesterday. England is at this moment in the throes of one of her worst epidemics of choleraand we believe the very last one that she ever will have the misfortune to suffer from; because, having now made the experiment once for all with this pestilence, she will never again allow a single infected foreigner to set his foot upon her shores. She is in sore need of heroism and courageousness in the face of disease; but those who can be found ready to make up her deficiencies in these qualities are now all employed in hunting down and hanging poor unfortunate wretches for stealing food, while thousands are starving for want of it.\n",
      "It has been asserted by a French writer (M. Emile Souvestre) that the English people are not so well informed as the Continental nations about what is going on in their own country. This may be true to some extentthough the British workman will read his newspaper with more care than most other men read the same paper, which is not very creditable either for the British workman or for his newspaper: but it is certainly the case that Englishmen know nothing whatever about what is going on in other countries, and are particularly ignorant of what is going on in France. M. Souvestre says that when an Englishwoman was shown some old manuscripts belonging to the famous Bibliothque Nationale at Paris (that library which contains more books than any other library excepting one other), she asked whether or not all the volumes in this national library were original and authentic; whether they had been transcribed from the copies of the great classical authors. When we come to the Continental journals, we find that their chief contents are descriptions of French manners and customs, French political events, French celebrities, and French works of art: so that a foreigner might suppose that all France did was eat, drink, talk about politics, and go to the theatre. The French journalists also have an almost insatiable curiosity regarding English doingswhich is very praiseworthy in them; and we find that in the great French journals there are weekly reports of the British Parliamentary proceedingsa thing unknown among ourselves, where it would be regarded as a gross impertinence.\n",
      "A gentleman who has written one or two good books about France, and who seems to understand the nation very thoroughly, observes that there are three peculiarities in French nature:\"The first is the absence of any fixed standard. . . . The second is the almost universal presence of an intense feeling of what they call their 'patrie,' a kind of patriotism which, on certain occasions, becomes very fervid indeed, and leads them to act in a way which appears to us absurdly inconsistent with their own professions.\" And here is his third peculiarity:\"The third is the almost total absence from French nature of any sense of justice, as we understand the word. . . .\" This last-named characteristic is indeed a curious one; it leads the French to think that they are right and everybody else wrong; and they seem to imagine that all things will always go well for themselves because they wish them to do so; while their adversaries will certainly lose because they wish to lose. And yet it is no new peculiarityit is as old as the history of mankind, which has been made up chiefly of quarrels between neighbours and wars of revenge and reprisal, the whole ending in a state of mutual hatred and universal destruction! The French\n",
      "llama_print_timings:        load time =    1228.96 ms\n",
      "llama_print_timings:      sample time =      72.79 ms /  1024 runs   (    0.07 ms per token, 14067.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.52 ms /   494 tokens (    1.51 ms per token,   660.85 tokens per second)\n",
      "llama_print_timings:        eval time =   18314.89 ms /  1023 runs   (   17.90 ms per token,    55.86 tokens per second)\n",
      "llama_print_timings:       total time =   19240.84 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de46ea1-0750-477e-9183-d1e6e0cc9e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273416\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: mem required  = 7024.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.03 MiB, ( 7024.66 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, ( 7424.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, ( 7499.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to hard labour for some loaves of bread which his starving family had tried to save from being eaten up by soldiers. A set of plagues, as fatal if less showy than the last, succeeded each other with unwelcome regularity; and the country rose on its eroded feet only to fall down again, this time for ever. \n",
      "# Chapter Six  \n",
      "The Cock and Anchor\n",
      "Mr. Pickwick took up his hat, and thrusting his hands into his pockets, slowly paced the room from end to end in deep meditation. In one corner there was a sofa: on that he threw himself. After long and patient cogitation of the subject, Mr. Pickwick rose briskly from this resting-place, pulled off his black stockings, and put on a pair of very thin slippers; threw round him an old dressing-gown with a great coarse hood; set his nightcap jauntily on one side; shook his head to be certain that nothing was out of order there; looked once more in the mirror; gave himself a most emphatic slap, and shouted in stentorian tones:\n",
      "'I have made up my mind!'\n",
      "'Yes, yes,' said Mr. Pickwick eagerly, turning round with great alacrity on hearing this last remark, 'and I'll tell you what it is; if he's innocent, let him go.'\n",
      "Mr. Weller stared at his master for a minute or two without speaking. Then he looked up in his face and said:\n",
      "'And the girlwhat about her?'\n",
      "'Hang her,' said Mr. Pickwick.\n",
      "'I don't see as how you can,' rejoined Sam, with great simplicity; 'for she's done nothin', except as you told me to say.'\n",
      "'Pooh! nonsense!' replied Mr. Pickwick: 'don't be absurd. Do you think I want the girl hanged? You know very well that I don't.'\n",
      "Sam nodded his approval of this proposition, and then ventured on another.\n",
      "'She's a beautiful girl,' said Sam; 'and youngnot above eighteen.'\n",
      "'No matter, Sam,' replied Mr. Pickwick, 'we have no business with the girl; she is only an accomplice in crime.'\n",
      "'So I thought as that man was,' rejoined Weller: 'but as I see no marks of violence about him, and he looks like a gentlemanwell, if you think he didn't do it, that settles it. Of course you know best; but the idea of his being guilty is what I call \"downright absurdity\"!'\n",
      "Mr. Pickwick was too much occupied with his own meditations to answer this remark at once; and Sam, after waiting a few moments without receiving any reply, observed in an offended tone: 'I say! you're rather an old-fashioned sort of governor arter all.'\n",
      "'Is he not?' exclaimed Mr. Pickwick. 'Sit down again; I want to ask you some more questions about your evidence.'\n",
      "Mr. Weller obeyed with alacrity. He was evidently quite proud and satisfied with his recent achievements, as he looked upon the prisoner with a complacent smile, and felt his waistcoat pocket with infinite satisfaction, as if there were something very precious locked up within its confines.\n",
      "'Now, Sam,' said Mr. Pickwick, 'you have told us your storyvery briefly.'\n",
      "'I couldn't well tell it otherwise,' replied Sam; 'because it's only what you might call a \"skeleton\" of the story.'\n",
      "Mr. Pickwick looked puzzled for a moment or two, and then said:\n",
      "'Now, Sam, we are alone here. If the prisoner should attempt to escape from custody before we leave this place, how should you advise me to act?'\n",
      "'To get up and run after him, of course,' replied Mr. Weller. 'If I had had a long stick, or a carrot for instance, or a stool to stand onnot that he would have stood much chance then.'\n",
      "Mr. Pickwick smiled at this picture of an athletic Mr. Samuel Weller, brandishing a stool in the shape of a cudgel; and his eyes sparkled with a gaiety that was most infectious, as he said:\n",
      "'I suppose you could give me no description of the prisoner.'\n",
      "Sam scratched behind his ear. 'Not exactly,' he replied; 'because he wore such very much like what I\n",
      "llama_print_timings:        load time =     846.65 ms\n",
      "llama_print_timings:      sample time =      73.88 ms /  1024 runs   (    0.07 ms per token, 13860.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.45 ms /   494 tokens (    1.51 ms per token,   660.91 tokens per second)\n",
      "llama_print_timings:        eval time =   18337.59 ms /  1023 runs   (   17.93 ms per token,    55.79 tokens per second)\n",
      "llama_print_timings:       total time =   19265.82 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763256c8-af9e-493e-b52d-4c161814bd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273436\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 363 tensors from ./models/13B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 7024.03 MiB\n",
      "llm_load_tensors: mem required  = 7024.03 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   128.17 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7024.03 MiB, ( 7024.66 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, ( 7424.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, ( 7499.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane accomplishments as stringing up suspected persons by the thumbs, burning women alive who held religious opinions of their own, and boiling alive divers other unhappy souls; whose guilt did not consist in having once said 'Lord, have mercy on us,' or in ever so slight a change of religious sentiment since.\n",
      "The lot of Spain was made still harder by these particulars; for there were added to them the cruelty and avarice of her priesthood, and the ignorance and bigotry of her king. That any but the worst men possible should be the subjects of so bad a government is, indeed, one of those anomalies which can hardly fail to strike the most careless observer. In some respects, the nation might even be considered as unfortunate. It was under such an administration as this, that its inhabitants were visited with a pestilence, by which a considerable portion of mankind was swept away, and a still more considerable portion terrified into a compliance with the demands of those who pretended to have discovered the only means of safety.\n",
      "These unhappy people had no resource but in submission; for the whole country was covered with armies of that religion which had been so strangely introduced on the continent, and the fanatics were encouraged to proceed to extremities by a formal promise from their priests, 'that all who should perish by the sword in this holy cause' should be regarded as martyrs; while they who escaped should receive an abundant reward here, and all that was good hereafter.\n",
      "The pestilence was believed to have been sent upon them for refusal to comply with these demands. It is true there were some dissenters; but their numbers were so small, that no attention could be paid to them.\n",
      "Whether it was the miseries of war and plague in England or the general discontent of the people at home, or whether it arose from the very natural impression which the account of such a pestilence might produce on the minds of all who were acquainted with its effects in former times; certain it is, that a general cry was raised by the populace against Sir Richard Whittington and his friends. The king's ministers declared at once that this cry should be attended to.\n",
      "There was but one thing to do. The pestilence was stopped. It could not last much longer; there were few enough of its victims left: it was time the people were cured of their folly and restored to reason, by taking away Sir Richard Whittington and his friends! So, they were taken away; and as the cry had been so loud, the populace became as joyful as if the plague were over.\n",
      "It must be confessed that this sudden change produced a great sensation throughout England when it was known. The King of Barebone's party expressed their fears that the government would take advantage of the universal content to seize on the remainder of the prisoners, and get rid of them for ever; and therefore they exhorted those who had been released to go home quietly and be thankful for a respite which might prove but short.\n",
      "The King's party, on the other hand, looked at the event in very different colours. They were delighted by it: they regarded it as an omen of their coming success; and therefore they exhorted all the people to persevere with them, and be confident that a change was near; so, for these reasons, there arose among the multitude a strong inclination to make a party in every county. The King's friends took courage from this disposition, and resolved to try whether they could not make some counter-move with success; accordingly, a few gentlemen who had been released, were recommended to take up their quarters at several of the principal towns throughout the country: their presence was very successful, for it soon produced a general meeting in every place.\n",
      "In consequence of this, when Christmas came on, instead of keeping it secretly as usual, the King's party resolved to hold a great festival, with all manner of feasting and merry-making: the idea being that people should not be angry at each other; for the friends of the King had been very wicked indeed. The enemies of the King, on the other hand, made their minds up to give them something very unpleasant for Christmas, if it could possibly be done.\n",
      "There were some of Sir Richard Whittington's party, who had been in prison all this time, and would not have come out if they could have helped it; but when Christmas drew near, they began to think that the King's party must hold a feast somewhere or other on account of\n",
      "llama_print_timings:        load time =     842.43 ms\n",
      "llama_print_timings:      sample time =      72.56 ms /  1024 runs   (    0.07 ms per token, 14111.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.75 ms /   494 tokens (    1.51 ms per token,   660.65 tokens per second)\n",
      "llama_print_timings:        eval time =   18349.54 ms /  1023 runs   (   17.94 ms per token,    55.75 tokens per second)\n",
      "llama_print_timings:       total time =   19275.79 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac988db",
   "metadata": {},
   "source": [
    "### 13B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acfc9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273456\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.71 MiB\n",
      "llm_load_tensors: mem required  = 24826.71 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.72 MiB, (24827.34 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, (25227.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, (25302.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his nose torn off, and his body burned alive, for the crime of having plucked a few stones from a church; of heaping up the fresh provisions for the knives and forkspoons of the Jacobins, till these lay unsold in heaps by wholesale round about the streets; of nailing two priest's ears to the pillory because they had heard mass; of cutting off the heads of a king, queen, cardinal, bishop, and a hundred other innocent persons,  for no crimes but those of having had birth, wealth, rank, or offices under the old regime. \n",
      "In the year of grace one thousand seven hundred and eighty-nine, the reigning family of France became partakers in some degree, in the general convulsion. The queen, having a weakness in her head, was seized with frequent epileptic fits; and an unfortunate vagary on the part of the king, rendered him ridiculous to European courts. \n",
      "It may be easily supposed, that at such a time, no common penetration would be necessary to discover the symptoms of secret discontent, which were every where visible; even without raking into private correspondence, or listening at key-holes. It is not improbable, that some persons who knew their neighbours really well, and could read in their countenances what they dared not put on paper, were enabled to prophesy with considerable correctness, concerning the fate of some favourites of fortune, who, by a change in public opinion, or a revolution in government, found themselves in a lower sphere of life than that in which they had been born. \n",
      "As these reflections naturally occurred to Miss Cunliffe's mind while she sat alone at tea with her papa; so it is possible, that if Miss Cunliffe had not been reading _The Antiquary_ , the conversation of herself and Mr Jenkinson might have gone on without any mention of Miss Cunliffe's favourite author. But Miss Cunliffe, in the present case, was fortunate enough to have read a book which was in everybody's mouth at that time, and her papa did not fail to take advantage of it; for he had some reason to believe from his daughter's face, when she had been reading it (a circumstance of which she was rather ashamed), that the novel pleased Miss Cunliffe better than she would like it to be known. 'I have often thought,' said Mr Cunliffe, 'that one or two of Sir Walter Scott's novels were a little too violent for young people to read; I don't mean as to the violence of passion in love and war: that is natural enough; but they are so full of murders, and other horrible crimes '\n",
      "Mr Jenkinson was not unwilling to show himself acquainted with novels. 'Surely,' said he, 'we must suppose it to be natural for the young men and women in a story of this kind, to get married before their fortunes are made.'\n",
      "'My dear Mr Jenkinson,' answered Miss Cunliffe's papa, 'it is not the novels we read ourselves, that I object to; but the stories which are told to our children. Why should you give them up as soon as they can go through a volume? But in some of these modern works there seems little or no difference between virtue and vice.'\n",
      "'It has been observed,' said Mr Jenkinson, 'that when a work has a large sale, it cannot be very improper.'\n",
      "'If that were the test of morality,' replied Miss Cunliffe's papa, 'we must have nothing but immorality. But I would not have our daughters read those novels for any consideration: and as to our sons '\n",
      "He stopped, because he was not sure how to express his objection that the young gentlemen should read these novels when they grew up; though he had no doubt in his own mind that if a son of his should ever happen to read one by mistake, he would be extremely shocked. Mr Jenkinson looked at him with great attention and curiosity, not being quite sure whether Miss Cunliffe's papa was going to propose, that young ladies should never marry till they were rich, or that gentlemen ought to consider it a point of honour never to become poor. The conclusion, however, was in favour of the young people; Mr Jenkinson having always an idea that the world would go on better if things were so arranged as to prevent a very large proportion of them from ever growing up\n",
      "llama_print_timings:        load time =    4347.71 ms\n",
      "llama_print_timings:      sample time =      73.73 ms /  1024 runs   (    0.07 ms per token, 13889.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     658.96 ms /   494 tokens (    1.33 ms per token,   749.67 tokens per second)\n",
      "llama_print_timings:        eval time =   46214.28 ms /  1023 runs   (   45.18 ms per token,    22.14 tokens per second)\n",
      "llama_print_timings:       total time =   47058.73 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29573f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273508\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.71 MiB\n",
      "llm_load_tensors: mem required  = 24826.71 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.72 MiB, (24827.34 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, (25227.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, (25302.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that, in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrels of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistically guilty of deism. In England there was scarcely an amount of order and protection enough in the law to hang a man who had murdered another man, but no Englishman in his senses would have given one farthing for his life on condition of having it reared by the State.\n",
      "In the year 1784 the Prince of Wales paid two visits to Chelsea College; the first when the building was not quite finished and before the furniture had arrived, the second when everything was in readiness. His Royal Highness spent several days at the College during which he observed all that was going on, met and spoke to all the men who were working there and inspected the premises generally. He visited again on 14 June 1785.\n",
      "The Prince of Waless visits were not the only royal occasion. On 23 August 1786 a ball was held at Chelsea College to celebrate Queen Charlottes birthday. The King and Queen and various members of the royal family, including Prince Frederick and the Duke and Duchess of Gloucester, were present.\n",
      "The opening of Chelsea Hospital in 1692 had provided employment for hundreds of people, mainly women, in making paupers clothes  a task carried out by the charity for more than two hundred years. But by 1807, when Charles Fraser visited the Royal Hospital to make his report, the whole system was beginning to break down and the work was done by men instead of women; one reason for this change was the rising cost of paupers clothes  it now cost over 370 per year for the 451 old soldiers. The clothing department had been set up by Sir Christopher Wren as part of his original building design but in 1809 an Order from the King and Queen stated that  all men under seventy years of age shall be allowed to work at tailoring, shoeing or weaving instead of making pauper clothes. The number of patients who chose to remain and make the old-fashioned clothing fell dramatically and by 1826 it was a dying industry.\n",
      "The first Superintendent appointed to run Chelsea Hospital in 1807 was Lt Colonel Francis Russell, an experienced soldier from Ireland, who had been wounded at the battle of Fontenoy (May 11th, 1745) during the French wars and taken prisoner. He spent eight years as a French prisoner but after his release he returned to England and became Superintendent for the Chelsea Hospital in 1807  his duties were not just confined to the Royal Hospital but also the Military Depot in Pimlico (near Vauxhall Bridge) which housed officers on half pay. In addition to this he had to keep a close watch on those soldiers who were still at large as deserters and in 1816 it was estimated that there were about one thousand of them  most were old and unfit for military service but the law required that they should be apprehended, tried and punished.\n",
      "The year 1824 proved to be a difficult time at the Chelsea Hospital when, after an inspection by Lord Sidmouth (Secretary of State) it was decided\n",
      "llama_print_timings:        load time =    3509.35 ms\n",
      "llama_print_timings:      sample time =      78.64 ms /  1024 runs   (    0.08 ms per token, 13021.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     659.67 ms /   494 tokens (    1.34 ms per token,   748.86 tokens per second)\n",
      "llama_print_timings:        eval time =   46137.79 ms /  1023 runs   (   45.10 ms per token,    22.17 tokens per second)\n",
      "llama_print_timings:       total time =   46995.29 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876f5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273559\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 363 tensors from ./models/13B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  5120,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 13824,  5120,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  5120, 13824,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 24826.71 MiB\n",
      "llm_load_tensors: mem required  = 24826.71 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 844/844\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 78.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   312.50 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 24826.72 MiB, (24827.34 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   400.03 MiB, (25227.38 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    75.02 MiB, (25302.39 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honor to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards: and because, upon this unexampled provocation, he had dared to raise his voice (although a child) against these reverend fathers of the faith in such terms as A plague on you! You villains! and divers other expressions of a like nature, casting at the same time some opprobrious mud which fortunately fell harmless, and upon the heads only of his persecutors. \n",
      "These were the state of Europe when the French Revolution broke out, and these were the state of mens minds. It was no wonder that the Revolution, influenced as it was by the ignorance, the ferocity, the tyranny, the ambition, the licentiousness which prevailed over the Continent, should strike out in its progress such doctrines as these: No church; no king; no nobles; all are equal and brothers; humanity is the soul of religion. It was natural that it should seek to level everything by a strong hand and a single despotic power; and, deeming nothing too sacred to be profaned for political purposes, should dismember, overturn, burn and massacre with as little remorse and shame as it would a church or a king. The first step was to throw off all external restraint: the next to follow out these doctrines of reason to their furthest consequences; and to sweep away every obstruction in its career. \n",
      "It is no wonder, therefore, that this revolutionary spirit, which began in France with such tremendous violence, should spread over Europe like a plague. The people everywhere were anxious for freedomat any rate as the word was used by those who had been their oppressors; and they were willing to make any sacrifice to obtain it. They did not stop to reason, that if what they were told about liberty was true, and if the French revolutionists knew all about it, the same arguments must have applied in France before as after the Revolution, and that they ought to be enjoying the benefits of it then. As for the people of England, I do not know what they thought of the subject; but what did it signify? They had their rights from birththese were never denied them: all they wanted was fair playthey might have as much liberty as other men. \n",
      "Chapter 65\n",
      "A few days after this conversation, Mr F. came home very late at night and in a state of great agitation; but, as he was unable to speak or explain his condition, Mrs F. left him for the night until morning brought some return of self-possession and strength. \n",
      "He rose before day, and immediately went down into the cellar where his daughter kept her beer barrels, intending to fetch a little of this liquor to mix with the milk, as it was a customary drink at this season for the labourers in the country. Having done so, he returned home, poured out some milk and beer in a jug, and drank off nearly half the quantity. He then went up stairs again into the bed-room of his son John, where the latter lay ill: here he reappeared immediately with two or three bottles of ale, and having given these to his son, returned to his own room, where he again mixed a little more milk and beer, and drank off another large draught. He was then seen by one of the servants to fall into an apparent fit of vomiting; but upon inquiry being made at the cellar door, it appeared that the servant had been asleep in his room. \n",
      "The family now became alarmed, for they knew Mr F.'s peculiar habits on these occasions: he would often take such large draughts of this mixture that nothing but a very powerful physician could counteract their effect, and sometimes it was not until a second dose had been given with similar success to the first that any benefit followed. \n",
      "Dr Hayes was therefore sent for; but he found his patient so far gone that he could only confirm Mr F.'s state, by saying that all remedies were in vainnothing but time and care would restore him. The Doctor, however, remained some hours in attendance upon him: it was late at night before he quitted the house. \n",
      "About five o'clock next morning, John, who slept in the same room with\n",
      "llama_print_timings:        load time =    3414.47 ms\n",
      "llama_print_timings:      sample time =      80.52 ms /  1024 runs   (    0.08 ms per token, 12717.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     658.90 ms /   494 tokens (    1.33 ms per token,   749.73 tokens per second)\n",
      "llama_print_timings:        eval time =   46138.11 ms /  1023 runs   (   45.10 ms per token,    22.17 tokens per second)\n",
      "llama_print_timings:       total time =   47046.38 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/13B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6537f",
   "metadata": {},
   "source": [
    "### 30B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c67651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273610\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.09 MiB\n",
      "llm_load_tensors: mem required  = 17505.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   166.63 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.09 MiB, (17505.72 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (18285.77 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (18382.78 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his lips, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered until the spring of the year seventeen hundred and eighty nine, rude carts, composing part of the farming stock of the establishment, which carts, in contact with disturbed France, were destined to become the stage for the most extraordinary of all theatrical performances ever acted upon it. \n",
      " In the Courts of Europe, kings and emperors resorted to each other, exchanging their respective arts, as the bee her pollen or the humming-bird her colour, and carrying on an epic war in the great game of dress. The trend of public opinion is to the natural. It has set so strongly towards all that is frank and easy and unaffected, that a reactionary movement may be looked for, which shall carry us back to the natural. In this revolution, the gait of the individual will take its appropriate share in expressing the character of the race. When this consummation shall be effected, the man of genius must not be expected to submit his style of walking to an Academy of Fashion: he may probably prefer it unshaped by any law but that of grace and elegance. The time is at hand when men will see kings and emperors as they are. Literature has emancipated the mind, and the great process of democratising property is not slow in bringing about its natural sequencethe democratisation of society. Then will come a time for royalty to make itself more than ever regal. The pomp of courts will be felt as necessary, in order that mankind may appreciate better their own advantages, and thus be led to seek higher benefits.\n",
      "It is the common impression with many people who have little or no acquaintance with the subject, that a good walker can also be an admirable dancera supposition which requires no proof so long as dancing retains its popularity in fashionable life, and so long as the man of genius may happen to be born within the precincts of a court. On the other hand there are many dancers, who having a natural turn for gymnastic exercises, acquire by practice a sort of elasticity or springiness in their walk, which is extremely attractive to the sight; and thus, we might almost say that the more agreeable form of walking is developed rather from an aptitude to dance, than from a special proficiency in gymnastics.\n",
      "The natural walker has many difficulties to contend with. The first one that he meets with is that of getting his friends to believe in him. No matter how gracefully he may move across the carpet, no matter what an appearance of ease and lightness he may present; and though a crowd of admirers may gather around him as he passes from room to roomhis self-conceit being fully pardoned for the sake of the pleasure which his performance affords; yet let him once enter upon his special branch of gymnastics, namely that of walking through crowded streets, up steep hills, and along slippery paths: he is looked upon as an anomalya strange being who has no right to be seen out of the Zoological Gardens. It would not do to make too much of this, but we may safely affirm that every one must have encountered many instances where he has been thought mad for walking; and in some cases even dangerous when there was not the slightest foundation for any such idea.\n",
      "There are various forms of the natural walk which it is necessary briefly to notice. It will be understood that we do not allude to those peculiarities, which, as being generally confined to persons of certain pursuits or trades, may almost be regarded as professional; but rather to the ordinary mode of progressionthe difference between a good and an indifferent performance in this respect, being as great as it is in singing or playing. In the first place we would advise our readers carefully to\n",
      "llama_print_timings:        load time =    3047.60 ms\n",
      "llama_print_timings:      sample time =      78.59 ms /  1024 runs   (    0.08 ms per token, 13029.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1805.82 ms /   494 tokens (    3.66 ms per token,   273.56 tokens per second)\n",
      "llama_print_timings:        eval time =   38241.96 ms /  1023 runs   (   37.38 ms per token,    26.75 tokens per second)\n",
      "llama_print_timings:       total time =   40244.48 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "148d30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273653\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.09 MiB\n",
      "llm_load_tensors: mem required  = 17505.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   166.63 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.09 MiB, (17505.72 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (18285.77 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (18382.78 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and fire applied to his lips (to punish him, one would suppose, for not speaking the language), because he had put on a pair of shoes belonging to his wife, which were too large for him. It was accordingly necessary to pause, and take a deep breath. \n",
      " II. A SLIGHT MISHAP\n",
      " I t is but an hour ago,\" said the Dover mail, getting its last fresh horses out of Ashford, \"and we have not gone five mile an' hour since we left the Red Lion; and now I am told that there is no chance of a coach to London tonight. To think that I should live to be abandoned in a ditch!\" \n",
      " _\"_ Well, Schedule,\" said the Dover mail, still holding forth very wisely, \"what would you have? My lamps were blown out last night, and I had four new ones put in this morning  _and all gone, sir!_ \" \n",
      " \"_And all gone, Sir!\"_ echoed the Schedule. \n",
      " \"Yes,\" said the Dover mail, \"and very unpleasant it was for the guard, _I assure you.\"_ \n",
      " By many roadsby fair and foul many roadswith cold winds blowingand wet roads splashingTo London town this coach has borne. \n",
      " Since its old days of Royal greatness, _down to be the mere vehicle of a vulgar Show_, it has known so much that even I could speak but now, and tell some little part of its surprising history! \n",
      " I have seen such sights as would appal any stouter spectre than my own weak one. I have heard such sounds as no living ear could bear. I have witnessed horrors which would make me tremble _could_ I tremble. But the night is gone, and there is broad daylight about me now; so, let me make my mention of the two passengers who went down in this coach last night, and whom I left at Rochester Castle, just beyond the bridge. \n",
      " The first passenger was Mr. Solomon Pell, formerly a dealer in mahogany and rosewood furniture, but now striving to unite the profitable business of an hotel-keeper with the profession of a money-lender. He carried his family in one of these coaches yesterday morninghis wife, his little boy, and his servant maid; for he was moving from Gravesend. He had sold off his goods by auction, and had got rid of most of his old stock; he was taking furnished apartments at the Golden Cross, Charing-Cross; and in a week or so the remainder of his goods and chattels were to follow him. His family would not accompany him until the things came up. \n",
      " I found Mr. Solomon Pell alone, seated on the seat opposite to me. He was smoking a cigar, and reading an evening paper; but he closed it as soon as I came in, laid it on one side, and stared at me for some moments without speaking. \n",
      " I do not like men to stare too closely at me, especially in broad daylight; so I looked at himand at his legs, which were much the longest of the two. But when he saw that this had no effect upon my composure, he spoke. \n",
      " 'Are you going up?' said Mr. Solomon Pell. \n",
      " 'Yes,' I replied; and thinking we might as well begin to be sociable at once, I asked him where he was going? \n",
      " 'LONDON,' said Mr. Solomon Pell, with a strong emphasis on the word London. He had not much confidence in me, or my respectability, apparently; for instead of saying 'Yesto London,' as any other well-conducted person would have done, he thought it necessary to add that important monosyllable. \n",
      " 'A long way,' I remarked. \n",
      " 'Pretty well,' said Mr. Solomon Pell, with a jerk of the head, intended as a nod, but expressive of nothing but itself. I don't know how other people are in such cases; but when a man, for the first time, jerks his head at me, instead of bowing it or nodding it in a natural manner, and I can see that he is doing it on purpose, my dislike to him always becomes more lively than before. I cannot help thinking that I ought to have some influence over his actionsought to be\n",
      "llama_print_timings:        load time =    2596.43 ms\n",
      "llama_print_timings:      sample time =      78.30 ms /  1024 runs   (    0.08 ms per token, 13077.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1810.24 ms /   494 tokens (    3.66 ms per token,   272.89 tokens per second)\n",
      "llama_print_timings:        eval time =   38286.59 ms /  1023 runs   (   37.43 ms per token,    26.72 tokens per second)\n",
      "llama_print_timings:       total time =   40297.04 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d46dfb5b-75c4-4a6b-99bc-08bb0fe6c012",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273696\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 543 tensors from ./models/30B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q4_0:  421 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 17.09 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 17505.09 MiB\n",
      "llm_load_tensors: mem required  = 17505.09 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   166.63 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 17505.09 MiB, (17505.72 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (18285.77 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (18382.78 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the point of view of its population, and the aim and object of its government, the one country was exactly the counterpart of the other. \n",
      " Chapter I \n",
      " The year 1775 formed an epoch in the life of Madame Roland, a lady who has earned her place among those remarkable women whom distress and public danger have elevated from domestic obscurity to political renown. Her husband, M. Roland de la Platiere, was then twenty-six years old; he was born at Thizy on the 14th of March, 1735: she, whose maiden name was Marie Philppon, had not completed her sixteenth year. She came from a small town near Lyons, named Arras, and was daughter to one of the minor members of the magistracy in that district.\n",
      "The family of Madame Roland, though connected with the highest classes by birth, were not distinguished by fortune: she was their only child; they had no money to give her, but they possessed what seemed a much more precious treasure to her ambitious and romantic imagination, namely, high connections in Paris.\n",
      "It would seem that she left Arras, about the year 1765, to come to the capital: she was then between fourteen and fifteen years of age. She received from her parents some general instruction; she could read, write, and calculate well enough for common life; she could even draw a little, and she had been taught music: but with such accomplishments as these she entered the brilliant world of Paris.\n",
      "She was handsome, gay, and lively; but her education, though not neglected, had been very imperfect; her character had grown up wildly from infancy; and she came to a capital which, at that time, surpassed all others in corrupting the youthful mind. Her parents placed her under the care of an aged relation who had for some years held the post of sub-governor of the Duc de Chartres, afterwards known as the Prince de Cond; this lady lived retired from the world at Paris with a small income derived from her pension: she was so fond of Marie Philppon that, when she quitted Arras and came to Paris, the girl accompanied her.\n",
      "M. Roland was one of those who were in attendance on the sub-governor. He had already written some essays on politics; he was then just about to begin his great work entitled \"An Enquiry into the Influence of Religion,\" a work which is still highly prized, and which was thought so important that it has been translated into English: M. Roland's name and writings had procured for him an entrance into society; but his situation was too dependent to allow of his becoming intimate with any but the lower orders of people; and Marie Philppon was a stranger in Paris, who wanted a companion and a friend. She fell in love with M. Roland before she knew that he was famous; perhaps it may be doubted whether she would have been so much pleased with him if his talents had first excited her curiosity about him.\n",
      "Marie Philppon was the daughter of people in business; her father had been a cloth-merchant, and had died rich: they were not noble, but they possessed great wealth. When she quitted the care of Madame de Beauvais, she continued to see M. Roland frequently at the houses where he made his appearance. Marie was clever, well-informed, lively; she spoke well: all these accomplishments in a young girl would have excited envy in some people and admiration in others: they certainly produced no such effects upon M. Roland. He did not make her many compliments, for he seldom paid any to women; but his manner was that of a man who is fondly attached by the ties of affection and gratitude, as well as by inclination.\n",
      "Marie Philppon loved him with all the force of a first love. She had seen none of her sex in the ranks above her who could compare with herself, or who could vie with the object of her affection; she believed that they would be happy if they were married; and she was determined to obtain his hand.\n",
      "When M. Roland became Minister of the Interior he wished to give up his apartments: he had been lodged in a palace belonging to one of the great hotels at Paris. He wished to live with his mother-in-law, Madame Roland, who occupied the same suite of rooms which she had inhabited while her husband lived; and, as Madame de Beauvais had died long before this period, he did not imagine that there would be any difficulty in obtaining her consent to this arrangement.\n",
      "Madame Roland had been the friend and confidant of his wife for several years. She was a\n",
      "llama_print_timings:        load time =    2573.93 ms\n",
      "llama_print_timings:      sample time =      79.21 ms /  1024 runs   (    0.08 ms per token, 12927.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1811.70 ms /   494 tokens (    3.67 ms per token,   272.67 tokens per second)\n",
      "llama_print_timings:        eval time =   38334.47 ms /  1023 runs   (   37.47 ms per token,    26.69 tokens per second)\n",
      "llama_print_timings:       total time =   40342.32 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fb0e7",
   "metadata": {},
   "source": [
    "### 30B F16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81fb0b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273740\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.77 MiB\n",
      "llm_load_tensors: mem required  = 62045.77 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   406.25 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.78 MiB, (62046.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (62826.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (62923.47 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. \n",
      " But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous. \n",
      " In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the coaches of mails were waylaid by seven robbers, and the guard shot three dead, and then got shot dead himself by the other four, \"in consequence of the failure of his ammunition:\" after which the mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in London gaols fought battles with their turnkeys, and the majesty of the law fired blunderbusses in among them, loaded with rounds of shot and ball; thieves snipped off diamond crosses from the necks of noble lords at Court drawing-rooms; musketeers went into St. Giles's, to search for contraband goods, and the mob fired on the musketeers, and the musketeers fired on the mob, and nobody thought any of these occurrences much out of the common way. In the midst of them, the hangman, ever busy and ever worse than useless, was in constant requisition; now, stringing up long rows of miscellaneous criminals; now, hanging a housebreaker on Saturday who had been taken on Tuesday; now, burning people in the hand at Newgate by the dozen, and now burning pamphlets at the same time (as if that would do any good); to- day, taking the life of an atrocious murderer, and to-morrow of a wretched pilferer who had robbed a farmer's boy of sixpence.\n",
      "All these dreadful things, and a thousand like them, came to pass in and close upon the dear old year one thousand seven hundred and seventy-five. Environed by them, while the Woodman and the Farmer worked unheeded, those two of the large jaws, and those other two of the plain and the fair faces, poured from their full hearts into their attentive ears, the last news that Runnymede had heardand with it all their wonder how bad news come to be so stale at Runnymede before they got it!\n",
      "Is not this a bleak lookout? said Mr. Chester, standing up in his stirrups, and turning a complete circle on the spot, as if he were making observations of the surrounding objects, for the first time.\n",
      "Practically speaking, my dear Sir, returned John Harmon, we are not much to boast of.\n",
      "The wind is so cutting here that it's enough to tear one\n",
      "llama_print_timings:        load time =   11959.99 ms\n",
      "llama_print_timings:      sample time =      73.59 ms /  1024 runs   (    0.07 ms per token, 13915.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1592.94 ms /   494 tokens (    3.22 ms per token,   310.12 tokens per second)\n",
      "llama_print_timings:        eval time =  103667.27 ms /  1023 runs   (  101.34 ms per token,     9.87 tokens per second)\n",
      "llama_print_timings:       total time =  105451.50 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd286f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273857\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.77 MiB\n",
      "llm_load_tensors: mem required  = 62045.77 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   406.25 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.78 MiB, (62046.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (62826.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (62923.47 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. \n",
      " But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous. \n",
      " In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-tradesman whom he stopped in his character of 'the Captain,' gallantly shot him through the head and rode away; the coaches went at night out of the city blinds down, and with lanterns extinguished; and pedestrians went in danger of their lives. There were fifteen hundred capital offences, punishable with death; there was such a jumble of offences, not capital, that whoso got himself into Newgate under charge of any one of them, was held without hope of deliverance to be inevitably hanged. Not that it was at all necessary that he should be hanged, according to some scholars on the Plaintext of the Law; but when the nation had been of a martial temper, and the men of military age had been killed, and the women and children captives, there were not men enough left to try them. Hence every private house in the country was a sanctuary, and any house might be attacked by ruffians who wanted money, for its possessor could shoot one of them dead without reproof or penalty, sometimes (on account of his being what was called a house-breaker), oftenest on no grounds but having been accused by somebody. The accused was held incapable of giving evidence in his own defence; the juries not unfrequently found all manner of persons guilty and voted them execution, on no evidence at all. In short, the country was in a sufficiently savage state, and London was the chief town of it. It was well that Dr. Manette was in hiding, for he would certainly have been delivered up to the first strolling soldier who expressed the requisite curiosity about him.\n",
      "But, although Miss Pross had no misgiving of this kind, she had a careful eye for any signs of disturbance or alarm on the premises; and, twice or thrice in every week, made a circuit of the whole outside, to see that things were right. Among the objects of her attention, was a certain bauble of fantastic shape in the front garden, some seven or eight feet high, which had been erected by order of Madame Defarge long ago, and never removed. It bore the following curious inscription in letters of gold on a blue ground: -\n",
      "In these times of ours, though concerning the exact nature of\n",
      "which strange things (some people say) are occasionally whispered,\n",
      "there is no such person as the King,\n",
      "this carven effigy is\n",
      "entirely accordant with\n",
      "the views of the authorities.\n",
      "It has seen Three\n",
      "Revolutions directed\n",
      "and controlled by the men\n",
      "of this street.\n",
      "It was not difficult for Miss Pross to believe that, if these grim faces were to become animated, they would look much more natural across a counter, or through\n",
      "llama_print_timings:        load time =   10574.31 ms\n",
      "llama_print_timings:      sample time =      80.62 ms /  1024 runs   (    0.08 ms per token, 12700.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1595.29 ms /   494 tokens (    3.23 ms per token,   309.66 tokens per second)\n",
      "llama_print_timings:        eval time =  104298.00 ms /  1023 runs   (  101.95 ms per token,     9.81 tokens per second)\n",
      "llama_print_timings:       total time =  106104.77 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71c4a20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702273975\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 543 tensors from ./models/30B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  6656, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  6656,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 17920,  6656,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  6656, 17920,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  6656,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 6656\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 60\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 52\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 52\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  422 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 6656\n",
      "llm_load_print_meta: n_head           = 52\n",
      "llm_load_print_meta: n_head_kv        = 52\n",
      "llm_load_print_meta: n_layer          = 60\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 17920\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 30B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 32.53 B\n",
      "llm_load_print_meta: model size       = 60.59 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 62045.77 MiB\n",
      "llm_load_tensors: mem required  = 62045.77 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  780.00 MiB, K (f16):  390.00 MiB, V (f16):  390.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1264/1264\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 100.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   406.25 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 62045.78 MiB, (62046.41 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   780.05 MiB, (62826.45 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    97.02 MiB, (62923.47 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, four-wheeled waggons, ponderous chests, barrels, trunks, and boxes, heavy with the weight of agricultural implements and victuals, wine, oats, and straw, destined to be overflowed by the river at a future day; and timber to frame cottages in the country. But these were only some among the countless multitude of its homely filiations; for, on the day when the heavy rush that had long been gathered and prepared, poured forth from Paris, and rolled as a deluge over the country, the feudal edifice, rooted in the deep soil, and strong with the storms and weather of a thousand years, went down like a green tree at the first breath of the tempest.\n",
      "And now that the wood had been laid under the axe, not a word was spoken against such work; almost, to speak the truth, the popular imagination appeared to be too much occupied with the blow to think of anything else. As if it had been the only possible pillow for the King's insensible head, all Jacobin and anti-Jacobin harangues flowed into one strain: 'So perish all tyrants!' and the death was as often recommended by those who had protested against the old institutions, as by others who had not been used to meddle with politics. Not that there were wanting voices which denounced it as a murder; but they were faint and drowned in a general hurrah of rejoicing, from which even strong Republican throats did not wholly abstain.\n",
      "The Abb Fournier sat among these jubilators. The Revolutionary Tribunal had passed upon the King's life before the sun rose on that fatal morning, and as the day declined towards evening, he was put to death on the spot near the Temple, where the mock trial had been held. The Abb Fournier went out to see him die, and heard a loud shout arise when his head fell into the basket.\n",
      "His head, after it had been paraded in the streets, was exhibited at the bar of the Convention that day; and then for the first time the Abb found himself near the president of the Tribunala man whom he knew by sight very well indeed. He sat in an armchair on the platform, and as he looked upon him attentively while he held his pen before him, it seemed to the good priest that there was some expression in his face that had become familiar to him. It suddenly occurred to him that when he had gone out last night to look at the head of M. Gouze, this man had been near him, and that the very same expression was on his countenance then.\n",
      "The Abb Fournier would have started up from his seat if he had dared; for it seemed as if a revelation of horrible import were dawning upon him. He sat still with his eyes fixed upon the president, until some one who was sitting beside him touched his arm and drew his attention to the scene then enacting in the hallthe reading of the minutes by the public crier.\n",
      "The Abb Fournier's mind was so occupied by what had passed in it that he could hardly comprehend what he heard, or indeed see anything very clearly. His eyes wandered from place to place, and fell upon the members as they satupon Robespierre, Danton, Marat, and all those who were known as leaders of parties, or who held high places in the Convention, and had their names inscribed on the tablets around the hall. But there was something else that he looked for, and for which his eyes diligently sought among them alland it was for a certain face which he found at last upon one of those tablesthe face of a man\n",
      "llama_print_timings:        load time =   10624.17 ms\n",
      "llama_print_timings:      sample time =     142.19 ms /  1024 runs   (    0.14 ms per token,  7201.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1593.84 ms /   494 tokens (    3.23 ms per token,   309.94 tokens per second)\n",
      "llama_print_timings:        eval time =  104078.82 ms /  1023 runs   (  101.74 ms per token,     9.83 tokens per second)\n",
      "llama_print_timings:       total time =  106028.89 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/30B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d34884",
   "metadata": {},
   "source": [
    "### 65B Q4_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702274092\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.00 MiB\n",
      "llm_load_tensors: mem required  = 35091.00 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35091.00 MiB, (35091.62 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (36371.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (36490.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrels of the Revolution. But that Woodman and that Farmer, though they work unceasingly, are appointed to work likewise with many an interval of rest. Those two hideous figures are not always seen in their garments of flame by any man. Most assuredly they were not always seen in this age, which had lightened itself with reason and escaped the burthen of superstition.\n",
      "It was the Dover mail in those days. He changed horses, maybe, at Canterbury; certainly not elsewhere to lose 15 or 20 minutes. The guard would sound his horn at one or two stations afterwards; and once, when he thought he heard a signal answered, would put his head out of window and look back along the train by the light of the fire in the eyes of his leading horses. The wood burning brightly in amongst the coal would show him TOM PIPES, THE HOUSE WITH A BILL ON IT, and a cluster of attentive faces (some close at hand, and over a lamp or two at windows), peering out of upper sleeper berths, wrapped in rugs and looking as snug as you please. The sharp whistle of the engine in front would give him timely notice that he was coming up with the other train, and then his cheerful call, All right behind! would be echoed back, all correct! Away then at a round trot past houses, carts, trees, and everything, as if the mail were a fiery dragon with iron wings, or an earthbound rocket rushing headlong through a shower of sparks.\n",
      "The wintry wind grew shrewder as it was daylight, the rain fell heavier, dismal yellow lights at cottage windows were extinguished, and gurgling watercourses came bubbling forth again. At last, when the night mail with its great sword-shaped strokes of flame had passed like a fiery vision by that wilderness of sleepers, there was a rousing in the dark hours before the dawn. The North was wide awake and up and doing while the South still hung heavy on her pillow.\n",
      "Letters to be registered were made up into separate bags or netted in sacks according to the size of each town to which they would have to be delivered, and in that condition transported by coach or cart as the distance was short or long. At seven in the morning one of the two or three letters received in an English provincial post-town on any given day would find its way into such a sack; at eight it would change hands at the cathedral city (perhaps fifty miles away); at nine at the chief manufacturing town (thirty miles beyond that again); and so on through fresh stages until it fell to be delivered up in London at five oclock in the afternoon. At the general post office in Lombard Street it would become a unit in a huge bag of letters addressed within the same district; and as it lay there with hundreds of others all bearing the impress of the local postmark, and waiting until an hour or so before midnight to be sorted for the last time into their several quarters of delivery, how could that letter tell you what it had come through? It couldnt, I reply. There was nothing about it, by look or sign or otherwise, that proclaimed it one whit different from any other drowsy scrap of paper lying near to it.\n",
      "These sacks were sometimes deposited at the offices of provincial newspapers on their way to London. Newspapers in those times paid very little attention indeed to what we now call general news, which\n",
      "llama_print_timings:        load time =    6531.24 ms\n",
      "llama_print_timings:      sample time =      80.62 ms /  1024 runs   (    0.08 ms per token, 12702.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3552.86 ms /   494 tokens (    7.19 ms per token,   139.04 tokens per second)\n",
      "llama_print_timings:        eval time =   68204.40 ms /  1023 runs   (   66.67 ms per token,    15.00 tokens per second)\n",
      "llama_print_timings:       total time =   71959.90 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5ffdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702274171\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.00 MiB\n",
      "llm_load_tensors: mem required  = 35091.00 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35091.00 MiB, (35091.62 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (36371.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (36490.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be laden with corpses. But, that Woodman and that Farmer, though they work unceasingly, are very slow and very silent, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was a crime of high treason against the bristling authority of the National Razor.\n",
      "Towards four o'clock, a terrific sound of footsteps and loud cheering was heard, issuing from an adjacent street; and presently King Traddle emerged with great dignity and composure from the Palace doorwayin company with his brother, Mr. Nubley, who carried himself like an extremely popular and distinguished characterto receive the homage of his devoted subjects. The Mayor was got up for the occasion in a very grand manner; but he looked somewhat fatigued, and perhaps it was that which caused him to appear so pensive as he advanced slowly with Mr. Nubley towards a small platform covered with red baize and guarded by an awning, where he sat down on a gorgeous chair, surrounded by his little court; the rest of whom stood upon the platform, or leaned against its crimson sides in attitudes expressive of admiration of him. He was received with tremendous acclamationswith such loud shouting that it made me quite hoarse to hear himand I noticed Mr. Nubley winking his eyes and putting his handkerchief to his ears, as if he also were slightly affected in that manner. The cries of 'Long life to the Mayor!' were so hearty and earnest that the tears came into Traddles's eyes; and when he stood up with his hat in his hand to return thanks, he could only say:\n",
      "\"My friends!\" with a tremendous blowing of his nose. But they made him say it over and over again, and shouted 'Noble fellow!' and other expressions of that sort so long as he had the hat in his handwhich, of course, was some time; for the Mayor found it difficult to balance himself on one foot while he held it out at arm's length with the other.\n",
      "These preliminary ceremonies disposed of, two bands struck up togethernot without violent animosity, but that passed in the first crashand a grand procession was formed. First, came two trumpeters; then, three marshals on horseback; next, Mr. Nubley and another gentleman (also mounted), who were described as The Right Worshipful Mayor's Attendants; then, SAMUEL PICKWICK, ESQUIRE, AND COMPANY, in a barouche with four horses. A new feature was introduced into the procession this yearan animated tableau, illustrative of the venerable superstition which brings good luck to wedding couples who ride together in an open carriage and pairby means of a bride and bridegroom sitting on two real palfreysvery quiet, sleek, fat poniestastefully ornamented with trappings of copper wire. It was very pretty to see how pleased the old people were; and how the bride's mother wept upon the shoulder of a red-nosed old gentleman who was her father, and he patted her on the back, as much as to say 'Never mind my dear! It'll be your turn next!' A few smart post-chaises, containing visitors, followed in rear; then came the banners of the Pickwick Club,\n",
      "llama_print_timings:        load time =    5898.73 ms\n",
      "llama_print_timings:      sample time =     341.70 ms /  1024 runs   (    0.33 ms per token,  2996.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3552.48 ms /   494 tokens (    7.19 ms per token,   139.06 tokens per second)\n",
      "llama_print_timings:        eval time =   70115.60 ms /  1023 runs   (   68.54 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =   74344.66 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd5c3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702274251\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 723 tensors from ./models/65B/ggml-model-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight q6_K     [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight q4_0     [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight q4_0     [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight q4_0     [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  561 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 34.27 GiB (4.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 35091.00 MiB\n",
      "llm_load_tensors: mem required  = 35091.00 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   205.08 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 35091.00 MiB, (35091.62 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (36371.69 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (36490.70 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, are very slow and very silent, and he was left to grow, and his daughter to bear and rear children, who, in their turn, were not neglectful of their duty in that respect.\n",
      "The baby throve as well as if its clothes had been fashioned by Fairy Godmothers; but Madam Defarge's taste was too ardent and intense to be satisfied with the slowness of Nature. Through the livelong summer at ten o'clock every morning, did she come across the street to look up at the registers that were duly made out and posted there by the authority of the Republic--the name of a male child being inscribed in golden letters on one list, and of a female in silver letters on another. She never came too early, for she could read as well upside down as with her head on; and so keen was her sight, as strong as it was under the long strain of looking close at fine sewing--though here, too, the same impatience showed itself, for she invariably took off her work to hold the book nearer to her eyes--that she could make out the entries with equal facility. The interest excited in her by these occupations, agitated Madame Defarge to that degree that she lost much of the extraordinary advantage she derived from her manipulation of the register; but no one knew it at Saint Antoine except her husband, and he said nothing about it.\n",
      "Thus the autumn days began and ended, and Mrs Cruncher's expectation (which she would have been glad to make longer if she could) dwindled to its appointed week. The last day in that week was cold and windy, with skies so leadenly overcast that the candle had been lighted in the window an hour or two earlier than usual: always a little ceremony of preparation at that time.\n",
      "Mr Cruncher was sitting on a wooden stool which he had reverse hitched to the deal table. His wife, whose face assumed an ill-tempered and impatient look on this occasion, as soon as supper was over and her husband began to smoke his pipe, was seated opposite to him in the arm-chair made of old wool-packs. Miss Cruncher was on a low seat by the fire; Flora and her mother sitting apart from them two steps higher, were engaged in putting rubbishy articles of dress into a very rubbishy old reticule much too easy of access.\n",
      "\"But you can't touch your money, dear boy,\" said Mrs Cruncher, flushed and anxious. \"I--not I alone, but Jerry too have had this upon my mind, dreadful--for a long time. I found out, only to-day, that there was but one honourable way of doing of it.\"\n",
      "\"Not by law-work,\" said he; \"though Mr Lorry knows what he says when he holds up his right hand and says, 'I am a notary public and a gentleman.' Ah! I shall never forget seeing him do it on the day when I found out that Jerry's money was gone.\"\n",
      "\"Ay!\" said Mr Cruncher with a gleam of triumph in his one eye. \"But now comes the question whether he shall do it with his head or without it, that's the present question. Whether he shall keep this till death us do part, or whether he shall get rid of this to-morrow morning. And that brings me back to what you was saying a while ago, dear wife--speaking of\n",
      "llama_print_timings:        load time =    5974.71 ms\n",
      "llama_print_timings:      sample time =     343.19 ms /  1024 runs   (    0.34 ms per token,  2983.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3548.35 ms /   494 tokens (    7.18 ms per token,   139.22 tokens per second)\n",
      "llama_print_timings:        eval time =   70043.85 ms /  1023 runs   (   68.47 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =   74273.96 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-q4_0.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce3e8",
   "metadata": {},
   "source": [
    "### 65B f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5122b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702274332\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.30 MiB\n",
      "llm_load_tensors: mem required  = 124525.30 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   500.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MiB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.31 MiB, offs = 115439812608, (125025.94 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (126306.00 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (126425.02 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But, that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the road- wardens went about bawling that they really could do nothing for the security of property in the streets; highwayman shot travellers from behind hedges, as a matter of course; footpads were an organised band, who ran a regular patrol in all the main roads, and plundered all passenger; mail-coaches were robbed by the light of links; if you had anything to take out of Middlesex into Hertfordshire, you were advised, when you gave the order at the carriage office, to talk a little about it over your grog that same night, pretty freely, because quot;theyquot; were one-and-twenty on the roads between Barnet and Wrotham, and you didn't know what minute they might take a fancy for your job.\n",
      "The city was an awful maze of unlighted streetsa labyrinth of darkness, with scarcely a lampshine anywherewhere it would no more have occurred to you to walk about at night without a lantern and a life-guard, than in the heart of South Africa. Even now there is only one street in London so bad as to contain these regular haunts of footpads; and although there are dangers of another sort, there are few people who, having money and jewels about them, or the reputation of such upon them, think any precaution needless. But in Coffin's time a whole quarter of London was the district of thieves, where the constables and watchmen were afraid to show themselves; where nightly robberies and assaults took place, one after another, as close together as the houses in the street; and where he who was so unlucky as to be attacked, for the sake of his pocket-money or his silver watch (no great matter either), was kicked and knocked about with no more ceremony than if he had been a dog.\n",
      "Yet Coffin lived through this period in comparative comfort and contentment; having an excellent housethe very same that he died in, fifty years after. And when the thieves did make an attack on him, by way of novelty, he so well requited them with his great fists, as soon to be free from any further visitations.\n",
      "A new and unexpected life was opened to him one day, about six months after the commencement of his business. A young gentleman who had been walking out in a rather lonely part of London, came suddenly upon Coffin's shop. The night had come on foggy; he felt quite lost: but seeing through a window a cheerful fire blazing up, and the figure of Mr. Coffin bending over his work, knocked at the door to ask his way into town. He entered as if it were some place where he had been invited; sat down in an arm-chair by the fireside, without saying a word about\n",
      "llama_print_timings:        load time =   35222.03 ms\n",
      "llama_print_timings:      sample time =      74.52 ms /  1024 runs   (    0.07 ms per token, 13741.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3126.32 ms /   494 tokens (    6.33 ms per token,   158.01 tokens per second)\n",
      "llama_print_timings:        eval time =  204120.14 ms /  1023 runs   (  199.53 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =  207448.25 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86588417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702274575\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.30 MiB\n",
      "llm_load_tensors: mem required  = 124525.30 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   500.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MiB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.31 MiB, offs = 115439812608, (125025.94 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (126306.00 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (126425.02 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. \n",
      " But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England, there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognised and challenged by his fellow-citizen whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot himself by the other four, \"in consequence of the failure of his ammunition:\" after which the Mail was robbed in peace; that magnificent potentate, the Lord Mayor of London, was made to stand and deliver on Turnham Green, by one highwayman, who despoiled the illustrious creature in sight of all his retinue; prisoners in London gaols fought battles with their turnkeys, and the majesty of the law fired blunderbusses in among them, loaded with rounds of shot and ball; thieves snipped off diamond crosses from the necks of noble lords at Court drawing-rooms; musketeers went into St. Giles's, to search for contraband goods, and the mob fired on the musketeers, and the musketeers fired on the mob, and nobody thought any of these occurrences much out of the common way. In the midst of them, the hangman, ever busy and ever worse than useless, was in constant requisition; now, stringing up long rows of house-breakers; now, burning people in the hand at Newgate by the dozen, and now, hanging a housebreaker, and burning four people in the hand.\n",
      "Jerry, left to himself in these altered circumstances, fell into habits of low society and late hours. He had signed certificates of marriage between such improper matches as no parson or clerk in holy orders would have assisted at; he had got acquainted with tradesmen who had gone wrong, horse-dealers who had pretended to be gentlemen, young men who spent more than they earned, and old men who earned less than they spent. He began to smoke cigars--to smoke ceaselessly. He left off one shirt a week, and put on another; he left off two shirts a week, and put on three. As to leaving off meat, it would have been easy if he had not always made up his mind tomorrow.\n",
      "Miss Flite was in no condition to notice these changes. Her bird-like manner of occasionally touching her lips with the tip of her dry little tongue when she saw him, and glancing at him in silence, denoted some uneasiness of conscience respecting him; but all distinctness had fled out of Miss Flite's mind, which was never, for a single moment in unison with itself.\n",
      "The Lord Chancellor of that Court is on his way to Lincolnshire. Escorted by the sheriff and other gentlemen, and proceeding leisurely along\n",
      "llama_print_timings:        load time =   39246.35 ms\n",
      "llama_print_timings:      sample time =      75.61 ms /  1024 runs   (    0.07 ms per token, 13542.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3128.32 ms /   494 tokens (    6.33 ms per token,   157.91 tokens per second)\n",
      "llama_print_timings:        eval time =  204144.63 ms /  1023 runs   (  199.55 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =  207477.14 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e1e0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 1619 (bcc0eb4)\n",
      "main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.1.0\n",
      "main: seed  = 1702274824\n",
      "llama_model_loader: loaded meta data with 15 key-value pairs and 723 tensors from ./models/65B/ggml-model-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:               output_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:                    output.weight f16      [  8192, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:              blk.0.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:         blk.0.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.0.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:              blk.1.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:         blk.1.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:            blk.1.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:              blk.2.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:         blk.2.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:            blk.2.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:              blk.3.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:         blk.3.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:              blk.3.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:              blk.4.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:         blk.4.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:            blk.4.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:              blk.5.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:         blk.5.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:            blk.5.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:              blk.6.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:         blk.6.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:            blk.6.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:              blk.7.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:         blk.7.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:            blk.7.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:              blk.8.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:         blk.8.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:            blk.8.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:              blk.8.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:              blk.9.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:              blk.9.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:         blk.9.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.9.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:              blk.9.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.10.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:             blk.10.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:        blk.10.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.10.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.10.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.11.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:             blk.11.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:        blk.11.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:           blk.11.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.11.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:             blk.12.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:             blk.12.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:        blk.12.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:             blk.12.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:             blk.13.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:        blk.13.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:           blk.13.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.14.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:        blk.14.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:           blk.14.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:             blk.15.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:        blk.15.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.15.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:             blk.16.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.16.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:           blk.16.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:             blk.17.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:        blk.17.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.17.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:             blk.17.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:             blk.18.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.18.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:           blk.18.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:             blk.19.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:        blk.19.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.19.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:             blk.20.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.20.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:           blk.20.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:             blk.21.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:        blk.21.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:             blk.21.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:             blk.22.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:        blk.22.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:           blk.22.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:             blk.23.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:        blk.23.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:           blk.23.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:        blk.24.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:           blk.24.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:        blk.25.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:           blk.25.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:        blk.26.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:           blk.26.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:        blk.27.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:           blk.27.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:        blk.28.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:           blk.28.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:        blk.29.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:           blk.29.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:             blk.30.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:        blk.30.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:           blk.30.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:             blk.31.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:        blk.31.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:           blk.31.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  291:             blk.32.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  292:             blk.32.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  293:             blk.32.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  294:        blk.32.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  296:           blk.32.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  297:             blk.32.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  300:             blk.33.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  301:             blk.33.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  302:             blk.33.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  303:        blk.33.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  305:           blk.33.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  306:             blk.33.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  309:             blk.34.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  310:             blk.34.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  311:             blk.34.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  312:        blk.34.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  314:           blk.34.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  315:             blk.34.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  318:             blk.35.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  319:             blk.35.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  320:             blk.35.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  321:        blk.35.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  323:           blk.35.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  324:             blk.35.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  327:             blk.36.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  328:             blk.36.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  329:             blk.36.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  330:        blk.36.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  332:           blk.36.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  333:             blk.36.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  336:             blk.37.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  337:             blk.37.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  338:             blk.37.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  339:        blk.37.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  341:           blk.37.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  342:             blk.37.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  345:             blk.38.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  346:             blk.38.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  347:             blk.38.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  348:        blk.38.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  350:           blk.38.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  351:             blk.38.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  354:             blk.39.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  355:             blk.39.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  356:             blk.39.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  357:        blk.39.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  359:           blk.39.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  360:             blk.39.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  363:             blk.40.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  364:             blk.40.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  365:             blk.40.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  366:        blk.40.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  367:           blk.40.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  368:           blk.40.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  369:             blk.40.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  370:          blk.40.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  371:           blk.40.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  372:             blk.41.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  373:             blk.41.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  374:             blk.41.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  375:        blk.41.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  376:           blk.41.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  377:           blk.41.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  378:             blk.41.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  379:          blk.41.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  380:           blk.41.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  381:             blk.42.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  382:             blk.42.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  383:             blk.42.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  384:        blk.42.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  385:           blk.42.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  386:           blk.42.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  387:             blk.42.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  388:          blk.42.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  389:           blk.42.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  390:             blk.43.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  391:             blk.43.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  392:             blk.43.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  393:        blk.43.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  394:           blk.43.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  395:           blk.43.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  396:             blk.43.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  397:          blk.43.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  398:           blk.43.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  399:             blk.44.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  400:             blk.44.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  401:             blk.44.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  402:        blk.44.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  403:           blk.44.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  404:           blk.44.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  405:             blk.44.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  406:          blk.44.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  407:           blk.44.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  408:             blk.45.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  409:             blk.45.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  410:             blk.45.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  411:        blk.45.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  412:           blk.45.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  413:           blk.45.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  414:             blk.45.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  415:          blk.45.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  416:           blk.45.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  417:             blk.46.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  418:             blk.46.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  419:             blk.46.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  420:        blk.46.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  421:           blk.46.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  422:           blk.46.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  423:             blk.46.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  424:          blk.46.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  425:           blk.46.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  426:             blk.47.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  427:             blk.47.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  428:             blk.47.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  429:        blk.47.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  430:           blk.47.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  431:           blk.47.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  432:             blk.47.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  433:          blk.47.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  434:           blk.47.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  435:             blk.48.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  436:             blk.48.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  437:             blk.48.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  438:        blk.48.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  439:           blk.48.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  440:           blk.48.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  441:             blk.48.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  442:          blk.48.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  443:           blk.48.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  444:             blk.49.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  445:             blk.49.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  446:             blk.49.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  447:        blk.49.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  448:           blk.49.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  449:           blk.49.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  450:             blk.49.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  451:          blk.49.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  452:           blk.49.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  453:             blk.50.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  454:             blk.50.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  455:             blk.50.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  456:        blk.50.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  457:           blk.50.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  458:           blk.50.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  459:             blk.50.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  460:          blk.50.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  461:           blk.50.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  462:             blk.51.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  463:             blk.51.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  464:             blk.51.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  465:        blk.51.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  466:           blk.51.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  467:           blk.51.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  468:             blk.51.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  469:          blk.51.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  470:           blk.51.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  471:             blk.52.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  472:             blk.52.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  473:             blk.52.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  474:        blk.52.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  475:           blk.52.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  476:           blk.52.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  477:             blk.52.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  478:          blk.52.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  479:           blk.52.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  480:             blk.53.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  481:             blk.53.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  482:             blk.53.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  483:        blk.53.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  484:           blk.53.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  485:           blk.53.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  486:             blk.53.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  487:          blk.53.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  488:           blk.53.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  489:             blk.54.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  490:             blk.54.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  491:             blk.54.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  492:        blk.54.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  493:           blk.54.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  494:           blk.54.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  495:             blk.54.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  496:          blk.54.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  497:           blk.54.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  498:             blk.55.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  499:             blk.55.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  500:             blk.55.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  501:        blk.55.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  502:           blk.55.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  503:           blk.55.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  504:             blk.55.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  505:          blk.55.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  506:           blk.55.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  507:             blk.56.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  508:             blk.56.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  509:             blk.56.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  510:        blk.56.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  511:           blk.56.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  512:           blk.56.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  513:             blk.56.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  514:          blk.56.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  515:           blk.56.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  516:             blk.57.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  517:             blk.57.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  518:             blk.57.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  519:        blk.57.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  520:           blk.57.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  521:           blk.57.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  522:             blk.57.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  523:          blk.57.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  524:           blk.57.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  525:             blk.58.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  526:             blk.58.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  527:             blk.58.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  528:        blk.58.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  529:           blk.58.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  530:           blk.58.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  531:             blk.58.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  532:          blk.58.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  533:           blk.58.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  534:             blk.59.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  535:             blk.59.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  536:             blk.59.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  537:        blk.59.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  538:           blk.59.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  539:           blk.59.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  540:             blk.59.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  541:          blk.59.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  542:           blk.59.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  543:             blk.60.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  544:             blk.60.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  545:             blk.60.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  546:        blk.60.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  547:           blk.60.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  548:           blk.60.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  549:             blk.60.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  550:          blk.60.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  551:           blk.60.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  552:             blk.61.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  553:             blk.61.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  554:             blk.61.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  555:        blk.61.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  556:           blk.61.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  557:           blk.61.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  558:             blk.61.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  559:          blk.61.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  560:           blk.61.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  561:             blk.62.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  562:             blk.62.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  563:             blk.62.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  564:        blk.62.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  565:           blk.62.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  566:           blk.62.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  567:             blk.62.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  568:          blk.62.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  569:           blk.62.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  570:             blk.63.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  571:             blk.63.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  572:             blk.63.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  573:        blk.63.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  574:           blk.63.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  575:           blk.63.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  576:             blk.63.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  577:          blk.63.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  578:           blk.63.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  579:             blk.64.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  580:             blk.64.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  581:             blk.64.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  582:        blk.64.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  583:           blk.64.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  584:           blk.64.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  585:             blk.64.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  586:          blk.64.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  587:           blk.64.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  588:             blk.65.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  589:             blk.65.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  590:             blk.65.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  591:        blk.65.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  592:           blk.65.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  593:           blk.65.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  594:             blk.65.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  595:          blk.65.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  596:           blk.65.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  597:             blk.66.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  598:             blk.66.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  599:             blk.66.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  600:        blk.66.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  601:           blk.66.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  602:           blk.66.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  603:             blk.66.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  604:          blk.66.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  605:           blk.66.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  606:             blk.67.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  607:             blk.67.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  608:             blk.67.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  609:        blk.67.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  610:           blk.67.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  611:           blk.67.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  612:             blk.67.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  613:          blk.67.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  614:           blk.67.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  615:             blk.68.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  616:             blk.68.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  617:             blk.68.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  618:        blk.68.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  619:           blk.68.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  620:           blk.68.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  621:             blk.68.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  622:          blk.68.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  623:           blk.68.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  624:             blk.69.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  625:             blk.69.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  626:             blk.69.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  627:        blk.69.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  628:           blk.69.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  629:           blk.69.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  630:             blk.69.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  631:          blk.69.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  632:           blk.69.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  633:             blk.70.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  634:             blk.70.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  635:             blk.70.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  636:        blk.70.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  637:           blk.70.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  638:           blk.70.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  639:             blk.70.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  640:          blk.70.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  641:           blk.70.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  642:             blk.71.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  643:             blk.71.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  644:             blk.71.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  645:        blk.71.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  646:           blk.71.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  647:           blk.71.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  648:             blk.71.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  649:          blk.71.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  650:           blk.71.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  651:             blk.72.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  652:             blk.72.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  653:             blk.72.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  654:        blk.72.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  655:           blk.72.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  656:           blk.72.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  657:             blk.72.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  658:          blk.72.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  659:           blk.72.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  660:             blk.73.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  661:             blk.73.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  662:             blk.73.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  663:        blk.73.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  664:           blk.73.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  665:           blk.73.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  666:             blk.73.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  667:          blk.73.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  668:           blk.73.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  669:             blk.74.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  670:             blk.74.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  671:             blk.74.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  672:        blk.74.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  673:           blk.74.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  674:           blk.74.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  675:             blk.74.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  676:          blk.74.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  677:           blk.74.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  678:             blk.75.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  679:             blk.75.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  680:             blk.75.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  681:        blk.75.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  682:           blk.75.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  683:           blk.75.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  684:             blk.75.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  685:          blk.75.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  686:           blk.75.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  687:             blk.76.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  688:             blk.76.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  689:             blk.76.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  690:        blk.76.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  691:           blk.76.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  692:           blk.76.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  693:             blk.76.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  694:          blk.76.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  695:           blk.76.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  696:             blk.77.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  697:             blk.77.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  698:             blk.77.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  699:        blk.77.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  700:           blk.77.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  701:           blk.77.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  702:             blk.77.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  703:          blk.77.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  704:           blk.77.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  705:             blk.78.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  706:             blk.78.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  707:             blk.78.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  708:        blk.78.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  709:           blk.78.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  710:           blk.78.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  711:             blk.78.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  712:          blk.78.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  713:           blk.78.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  714:             blk.79.attn_q.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  715:             blk.79.attn_k.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  716:             blk.79.attn_v.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  717:        blk.79.attn_output.weight f16      [  8192,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  718:           blk.79.ffn_gate.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  719:           blk.79.ffn_down.weight f16      [ 22016,  8192,     1,     1 ]\n",
      "llama_model_loader: - tensor  720:             blk.79.ffn_up.weight f16      [  8192, 22016,     1,     1 ]\n",
      "llama_model_loader: - tensor  721:          blk.79.attn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  722:           blk.79.ffn_norm.weight f32      [  8192,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 80\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 64\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type  f16:  562 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 8192\n",
      "llm_load_print_meta: n_head           = 64\n",
      "llm_load_print_meta: n_head_kv        = 64\n",
      "llm_load_print_meta: n_layer          = 80\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 65B\n",
      "llm_load_print_meta: model ftype      = mostly F16\n",
      "llm_load_print_meta: model params     = 65.29 B\n",
      "llm_load_print_meta: model size       = 121.61 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size = 124525.30 MiB\n",
      "llm_load_tensors: mem required  = 124525.30 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 1684/1684\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Ultra\n",
      "ggml_metal_init: picking default device: Apple M2 Ultra\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/jack/llama.cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Ultra\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 147456.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 122.07 MiB\n",
      "llama_new_context_with_model: max tensor size =   500.00 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 110592.00 MiB, offs =            0\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size = 14433.31 MiB, offs = 115439812608, (125025.94 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1280.06 MiB, (126306.00 / 147456.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   119.02 MiB, (126425.02 / 147456.00)\n",
      "\n",
      "system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 1.100\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
      "generate: n_ctx = 512, n_batch = 512, n_predict = 1024, n_keep = 0\n",
      "\n",
      "\n",
      "\u001b[33m It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. \n",
      " There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. \n",
      " It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. \n",
      " France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\u001b[0m Under the guidance of her Christian pastors, she entertained herself, besides, with such humane achievements as sentencing a youth to have his hands cut off, his tongue torn out with pincers, and his body burned alive, because he had not kneeled down in the rain to do honour to a dirty procession of monks which passed within his view, at a distance of some fifty or sixty yards. It is likely enough that, rooted in the woods of France and Norway, there were growing trees, when that sufferer was put to death, already marked by the Woodman, Fate, to come down and be sawn into boards, to make a certain movable framework with a sack and a knife in it, terrible in history. It is likely enough that in the rough outhouses of some tillers of the heavy lands adjacent to Paris, there were sheltered from the weather that very day, rude carts, bespattered with rustic mire, snuffed about by pigs, and roosted in by poultry, which the Farmer, Death, had already set apart to be his tumbrils of the Revolution. But that Woodman and that Farmer, though they work unceasingly, work silently, and no one heard them as they went about with muffled tread: the rather, forasmuch as to entertain any suspicion that they were awake, was to be atheistical and traitorous.\n",
      "In England there was scarcely an amount of order and protection to justify much national boasting. Daring burglaries by armed men, and highway robberies, took place in the capital itself every night; families were publicly cautioned not to go out of town without removing their furniture to upholsterers' warehouses for security; the highwayman in the dark was a City tradesman in the light, and, being recognized and challenged by his fellow-tradesman whom he stopped in his character of \"the Captain,\" gallantly shot him through the head and rode away; the mail was waylaid by seven robbers, and the guard shot three dead, and then got shot himself by the other four, in consequence of the failure of his ammunition: after which the mail was robbed in peace.\n",
      "Lord John Russell, in giving his reasons for finally abandoning the cause of Parliamentary Reform, stated that one great inducement to that step was the reflection that he had been mainly instrumental in causing Charles the First to be beheaded, and he never could forget it.\n",
      "PUNCH. (1846.)\n",
      "Melbourne resigned office in July, 1842; Sir Robert Peel was again invited by the Queen to form a government, which he did, with a cabinet composed of Tories and some of those Whigs who had gone into Opposition since the Corn Law debates. This fact alone was sufficient evidence that the party which, in a season of discontent and distrust, had been strong enough to force its views on the unwilling people by an appeal to their passions rather than their reason, had lost its hold upon public opinion when the popular fury had subsided. Peel, whose only crime was his refusal to yield to the dictation of a furious multitude, now became, as he well deserved to be, the foremost statesman in England; and this result is the best proof that the Corn Laws, as an abstract measure of legislation, had never been popular. They owed their success merely to a conjunction of untoward circumstancesa season of scarcity, and the panic into which the farmers had been thrown by the prospect of cheap foreign grain; but as soon as those feelings had passed away, and as soon as Sir Robert Peel saw his way to carry out those measures of commercial reform with regard to which he was in accordance with Mr. Cobden and his friends, no opposition arose to impede him, although there were not wanting symptoms that the manufacturing districts looked coldly on his Free-Trade policy, and feared lest it should interfere with those monopolies by which their prosperity had been hitherto maintained.\n",
      "But the Free Trade movement was at an end for the moment; Cobden's victory over Peel in the Corn Law debates of 1846 rendered him less formidable as an agitator, because he could no longer claim that his views were those of the majority. He had always been regarded by his followers with a sort of superstitious reverence, and as being little if anything short of infallible; but the feeling which had at first prevailed throughout the countrythe sense that they\n",
      "llama_print_timings:        load time =   38703.17 ms\n",
      "llama_print_timings:      sample time =      75.50 ms /  1024 runs   (    0.07 ms per token, 13563.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3132.61 ms /   494 tokens (    6.34 ms per token,   157.70 tokens per second)\n",
      "llama_print_timings:        eval time =  203971.56 ms /  1023 runs   (  199.39 ms per token,     5.02 tokens per second)\n",
      "llama_print_timings:       total time =  207308.02 ms\n",
      "ggml_metal_free: deallocating\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "!./main --color --no-mmap -ngl 1 --temp 1.1 --repeat_penalty 1.1 -n 1024 --ignore-eos -m ./models/65B/ggml-model-f16.gguf  -p \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way  in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only. <0x0A>\\\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever. <0x0A>\\\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the English Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood. <0x0A>\\\n",
    "France, less favoured on the whole as to matters spiritual than her sister of the shield and trident, rolled with exceeding smoothness down hill, making paper money and spending it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959fbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
